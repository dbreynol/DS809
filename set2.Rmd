# Smoothing, Decomposition, Noise

## Exponential Smoothing

An alternate way to smooth a time series, generally appropriate for a time series with no clear seasonality or trend, is with exponential smoothing. Like the moving average, this method averages over recent observations but differs in that it assigns relatively more weight to observations that are relatively closer. This idea can be expressed as,

\begin{align}
\hat{y}_{t+1 | t} = \alpha y_t + (1-\alpha) \hat{y}_{t | t-1},
\end{align}

where $\hat{y}_{t+1 | t}$ can be interpreted as the smoothed value of $y_{t+1}$ given data up to time $t$. So, for the first few smoothed values, we have:

\begin{align}
\hat{y}_{1|0} &= l_0 \\
\hat{y}_{2|1} &= \alpha y_1 + (1-\alpha) l_0 \\
\hat{y}_{3|2} &= \alpha y_2 + (1-\alpha) \hat{y}_{2|1} \\
&= \alpha y_2 + (1-\alpha) \alpha y_1 + (1-\alpha)^2 l_0
\end{align}

Since we don't have data prior to $y_1$, we denote $\hat{y}_{1 | 0} = l_0$. Therefore, this model depends on two parameters, $(l_0, \alpha)$. If we continue with the sequence above, each predicted value $\hat{y_t}$ can be expressed,

\begin{align}
\hat{y}_{t+1 | t} = (1-\alpha) ^ t l_0  + \sum_{j=0}^{t-1} \alpha (1-\alpha) ^ j y_{t-j}. \\
\end{align}

Let's take a look at how this method depends on the parameters. We will use Albanian export data from `tsibbledata`, which you can grab here:

```{r echo=TRUE}
alb = global_economy %>% filter(Country == "Albania", Year > 1990) 
y = alb$Exports
```

```{r}
es = function(par, ts) { # parameter vector: first parameter is alpha/ second is l0
  yt = c(par[2])
  for(j in 2:length(ts)) {
    yt[j] = par[1] * ts[j-1] + (1-par[1]) * yt[j-1]
  }
  return(yt)
}

#plot_df = data.frame(ys = c(y, es(y, .4, 0), es(y, .1, 1)), alpha = c(rep(0,n), rep(.4,n), rep(.1,n)))
plot(x = 1991:2017, y, type = "l", xlab = "year", ylab = "exports", main = "Albanian Exports (black); alpha = .9 (red); alpha = .5 (blue)")
lines(x = 1991:2017, y = es(c(.9, y[1]), y), col = "red")
lines(x = 1991:2017, y = es(c(.5, y[1]), y), col = "blue")
```

In the plot above, of Albanian exports between 1991 and 2017, we display two smoothed time series (one for $\alpha = 0.5$ and one where $\alpha = 0.9$). For any $\alpha$ between 0 and 1, the weights attached to the observations decrease exponentially as we go back in time, hence the name “exponential smoothing”. If $\alpha$ is small (i.e., close to 0), more weight is given to observations from the more distant past. If $\alpha$ is large (i.e., close to 1), more weight is given to the more recent observations.

### Optimization

The $(\alpha, l_0)$ parameters can be estimated by minimizing the SSE:

\begin{align}
\text{SSE} = \sum_{i=i}^T \bigg(y_t - \hat{y}_{t | t-1}   \bigg)^2.
\end{align}

This is a non-linear optimization problem that you will solve in Lab 2!

```{r eval=FALSE}

es_ss = function(par) {
  yt = es(y, par[1], par[2])
  return( sum( (yt - y)^2))
}

optim(par = c(0,0), es_ss, lower = c(0,0), upper = c(1,10), method = "L-BFGS-B")$par

# how does this compare with ets
ets(y, model = "ANN")
```


## Decomposition

Time series data can exhibit a variety of patterns, and it is often helpful to split a time series into several components, each representing an underlying pattern category. In this section, we will aim to decompose a time series into three parts: a trend component ($T$), a seasonality component ($S$), and a random component ($R$). That is, for each observation $Y_t$, we want to break it down into three parts:

$Y_T = T_t + S_t + R_t$.

To illustrate, we will use a dataset on monthly retail employment in the US. Let's first take a look at this data.


```{r echo=TRUE}
# us_employment data frame from the 'fpp3' package
us_employment$date = mdy ( str_c( month(us_employment$Month), "-1-", year(us_employment$Month)) )

retail = us_employment %>% 
  filter(Title == "Retail Trade", year(date) > 2002)
```


```{r}
ggplot(retail, aes(x = date, y = Employed)) + 
  geom_line() + 
  ggtitle("Total US Retail Employees") + 
  theme_minimal()

```

To decompose this time series, we will follow this basic algorithm:

1. First we will use a moving average of order $m=12$ to get the trend, $T$.
2. Then, we will estimate the seasonal effects, $S$,by fitting a linear regression model to the de-trendended series ($y-T$) in which the month is the explanatory variable.
3. The remainder is the random component, $y-T-S$.




```{r}
retail$trend = ma(retail$Employed, order = 12)
retail = drop_na(retail)
retail$detrend = retail$Employed - retail$trend
retail$season = predict(lm(detrend ~ factor(month(date)), retail))
retail$noise = retail$Employed - retail$trend - retail$season

retail_plot = data.frame(retail) %>% select(date, Employed, trend, season, noise) %>% 
  pivot_longer(-date)
  
ggplot(retail_plot, aes(date, value)) + 
  geom_line() + 
  facet_wrap(~name, scales = "free") + theme_minimal()
```

The trend captures the majority of the change that is observed in this time series, while the relative scale of the monthly seasonality and the random variation is small.



It seems plausible that this is a white noise series. How can we build further evidence of this. One way would be to examine the autocorrelation function. What are we looking for?

```{r}
acf(googdf$z[-1], main = "ACF of differenced series")
```

This is helpful but visual evidence alone is fairly weak. We could alternatively use a hypothesis test (Box-Pierce test) in which:

\begin{align}
H_0 &: \rho_1 = \rho_2 = \ldots = \rho_k = 0 \\
H_A &: \text{at least one autocorrelation is different from 0}
\end{align}

The test statistic (the Ljung-Box Q-Statistic) for this test:

\begin{align}
Q = n(n+2) \sum_{i=1}^k \frac{\hat{\rho}_i^2}{(n-k)}
\end{align}

follows a chi-squared distribution with $k$ degrees of freedom under the null hypothesis.



## Lab 2

1. Change the <code>es</code> function we developed in [Exponential Smoothing] to return the SSE, rather than <code>yt</code>, the smoothed time series. Now, use the <code>optim</code> function to find the values of $(\alpha, l_0)$ that minimize SSE. Plot the exponentially smoothed time series of the Albanian export data using the optimized values for $(\alpha, l_0)$. What are these values?

```{r eval=T, echo=TRUE}
y = alb$Exports
es_ss = function(par) {
  yt = es(par, y)
  return( sum( (yt - y)^2))
}

optim(par = c(0,0), es_ss, lower = c(0,0), upper = c(1,10), method = "L-BFGS-B")$par

# alpha = 1; l0 = 7.484819

# how does this compare with ets (an implementation of exponential smoothing from the forecast pkg)
ets(y, model = "ANN")
```

2. Choose a different type of employee from the <code>us_employment</code> dataset and, first, plot the time series. Now, decompose the time series into trend, seasonal, and random components. Follow the algorithm in [Decomposition]. Plot these and comment on your observations.

```{r echo=TRUE}
type = "Government: Federal"
gov = filter(us_employment, Title == "Government: Federal", year(date) >= 1980)

gov$trend = ma(gov$Employed, order = 12)
gov = gov %>% drop_na()
gov$detrend = gov$Employed - gov$trend
gov$season = predict(lm(detrend ~ factor(month(date)), gov))
gov$noise = gov$Employed - gov$trend - gov$season

gov_plot = data.frame(gov) %>% select(date, Employed, trend, season, noise) %>% 
  pivot_longer(-date)
  
ggplot(gov_plot, aes(date, value)) + 
  geom_line() + 
  facet_wrap(~name, scales = "free") + theme_minimal()

```


3. Simulate a [Random Walk] with 100 time points ($y_t$, $T= 1,\ldots,100$). Repeat this process 50 times. Choose your own $\sigma^2$ and use the same value for each of the 50 iterations. Plot all 50 time series on the same plot. On the title, report the $\sigma^2$ value you used. Second, make a histogram of each of the end points (i.e., the 100th observation of each of the 50 time series). How does this histogram correspond with the theoretical properties of $Y_{100}$ (i.e., the mean, variance, and shape of the distribution)?

```{r echo=TRUE}
rw = cumsum(rnorm(100))
last_el = rw[100]
par(mfrow = c(1,2))
plot(1:100, rw, type = "l", ylim = c(-50,50), main = "Random Walks; Sigma = 1")

for (j in 2:50) {
  rw = cumsum(rnorm(100))
  last_el[j] = rw[100]
  lines(1:100, rw, type = "l", col = rainbow(50)[j])
}

hist(last_el, breaks = 10, main = "Histogram of 100th observations")

```


4. Write a function that takes a time series vector as input and returns the Ljung-Box Q-Statistic for $k=2$, along with the p-value for the hypothesis test in [White Noise].

```{r echo=TRUE}
y = arima.sim(100, model = list(ar=c(.6), ma = c(.1))) # input time series example

boxtest = function(input_ts) {
  n = length(input_ts)
  rho_hat = acf(input_ts, plot = F)$acf[2:3] # rho_hat for k=1,2
  Q = n * (n+2) * (rho_hat[1]^2/(n-1) + rho_hat[2]^2/(n-2))
  pval = pchisq(Q, df = 2, lower.tail = F)
  return(list(Q=Q, pval = pval))
}

boxtest(y)
Box.test(y, lag = 2, type = "Ljung-Box")

```



5. Using the <code>quantmod</code> package, choose your favorite stock and see if the differenced version of its closing prices over the past year can be described as white noise. Include plots and the result of the Box test.

See GOOG example above.

