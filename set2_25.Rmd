# ARMA

## Operators

### Backshift operator

The _backshift shift operator_ ($\mathbf{B}$) is an important function in time series analysis, which is defined as

$$ \mathbf{B} x_t = x_{t-1} $$

or more generally as

$$ \mathbf{B}^k x_t = x_{t-k} $$

<span style="color:red">For example, express a random walk, $x_t = x_{t-1} + w_t$, using $\mathbf{B}$.</span>

### The difference operator

The _difference operator_ ($\nabla$) is another important function in time series analysis, which we define as

$$ \nabla x_t = x_t - x_{t-1} $$

<span style="color:red">For example, what does first-differencing a random walk yield?</span>

The difference operator and the backshift operator are related

$$ \nabla^k = (1 - \mathbf{B})^k $$
Differencing is a simple means for removing a trend

The 1st-difference removes a linear trend

A 2nd-difference will remove a quadratic trend

```{r diff_linear, fig.align="center"}
## create biased RW
rr <- ww <- rnorm(50)
for(t in 2:50) {
  rr[t] <- 0.3 + rr[t-1] + ww[t]
}

par(mfrow = c(1,2), mai = c(0.5,0.8,0.1,0), omi=c(0,0,0,0))

## raw data
plot.ts(rr, las = 1,
        ylab = expression(italic(x[t])))
## first difference
plot.ts(diff(rr), las = 1,
        ylab = expression(nabla~italic(x[t])))
```

## Autoregressive (AR) models

An _autoregressive_ model of order _p_, or AR(_p_), is defined as

$$x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \dots + \phi_p x_{t-p} + w_t$$

where we assume

1. $w_t$ is white noise

2. $\phi_p \neq 0$ for an order-_p_ process

```{r ex_AR_models}
## the 4 AR coefficients
ARp <- c(0.7, 0.2, -0.1, -0.3)
## empty list for storing models
AR_mods <- vector("list", 4L)

par(mfrow = c(2,2), mai = c(0.7,0.7,0.3,0.1), omi = c(0,0,0,0))
## loop over orders of p
for(p in 1:4) {
  ## assume SD=1, so not specified
  AR_mods[[p]] <- arima.sim(n=50, list(ar=ARp[1:p]))
  plot.ts(AR_mods[[p]], las = 1,
          ylab = expression(italic(x[t])))
  mtext(side = 3, paste0("AR(",p,")"),
        line = 0.5, adj = 0)
}
```

### AR(1) Model

Let's start by figuring out some properties of the simplest AR model, the AR(1) model:

$$x_t = \phi_0 + \phi_1 x_{t-1} + w_t$$

We start by assuming that $x_t$ is a stationary time series. Under this assumption, we can show:

\begin{align}
E(x_t) &= \frac{\phi_0}{1-\phi_1} \\
Var(x_t) &= \frac{\sigma^2_w}{1-\phi_1^2} \\
\rho(h) &= \phi_1^h
\end{align}

For this to work, $|\phi_1| < 1$. 

### AR Stationarity

We seek a means for identifying whether our AR(_p_) models are also stationary. We can write out an AR(_p_) model using the backshift operator:

$$
  x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \dots + \phi_p x_{t-p} + w_t \\
  \Downarrow \\
\begin{align}
  x_t - \phi_1 x_{t-1} - \phi_2 x_{t-2} - \dots - \phi_p x_{t-p} &= w_t \\
  (1 - \phi_1 \mathbf{B} - \phi_2 \mathbf{B}^2 - \dots - \phi_p \mathbf{B}^p) x_t &= w_t \\
  \phi_p (\mathbf{B}^p) x_t &= w_t \\
\end{align}
$$

If we treat $\mathbf{B}$ as a number (or numbers), we can out write the _characteristic equation_ as $\phi_p (\mathbf{B}^p)$.

To be stationary, __all roots__ of the characteristic equation __must exceed 1 in absolute value__ As a bonus, when this condition is met, then the model is also *causal.*

<span style="color:red"> Example, for what value of $\phi_1$ is AR(1) model stationary? </span>

Are the following AR processes stationary?

1. $x_t = 0.5 x_{t-1} + w_t$
2. $x_t = -0.2 x_{t-1} + 0.4 x_{t-2} +  w_t$
3. $x_t = x_{t-1} + w_t$



```{r ar_comp_pos_neg, fig.height=4}
## list description for AR(1) model with small coef
AR_pos <- list(order=c(1,0,0), ar=0.7, sd=0.1)
## list description for AR(1) model with large coef
AR_neg <- list(order=c(1,0,0), ar=-0.7, sd=0.1)
## simulate AR(1)
AR1_pos <- arima.sim(n=500, model=AR_pos)
AR1_neg <- arima.sim(n=500, model=AR_neg)

## get y-limits for common plots
ylm1 <- c(min(AR1_pos[1:50],AR1_neg[1:50]), max(AR1_pos[1:50],AR1_neg[1:50]))

## set the margins & text size
par(mfrow=c(1,2), mai=c(0.8,0.8,0.3,0.2), oma=c(0,0,0,0))
## plot the ts
plot.ts(AR1_pos[1:50], ylim=ylm1, las = 1,
        ylab=expression(italic(x)[italic(t)]),
        main = "")
mtext(side = 3, expression(paste(phi[1]," = 0.7")),
      line = 0.4, adj = 0)
plot.ts(AR1_neg[1:50], ylim=ylm1, las = 1,
        ylab=expression(italic(x)[italic(t)]),
        main = "")
mtext(side = 3, expression(paste(phi[1]," = -0.7")),
      line = 0.4, adj = 0)
```

### Autocorrelation

The exponential decay observed in autocorrelation function for the the AR(1) model holds in general for AR(p). This decay may oscillate, as shown below.

```{r ex_acf_AR}
## set the margins & text size
par(mfrow=c(2,2), mai=c(0.8,0.8,0.3,0.2), oma=c(0,0,0,0))
## plot the ts
plot.ts(AR1_pos[1:50], ylim=ylm1, las = 1,
        ylab=expression(italic(x)[italic(t)]),
        main = "")
mtext(side = 3, expression(paste(phi[1]," = 0.7")),
      line = 0.4, adj = 0)
acf(AR1_pos, lag.max = 20, las = 1)
plot.ts(AR1_neg[1:50], ylim=ylm1, las = 1,
        ylab=expression(italic(x)[italic(t)]),
        main = "")
mtext(side = 3, expression(paste(phi[1]," = -0.7")),
      line = 0.4, adj = 0)
acf(AR1_neg, lag.max = 20, las = 1)
```


## Moving Average (MA) models

A moving average model of order _q_, or MA(_q_), is defined as

$$ x_t = w_t + \theta_1 w_{t-1} + \theta_2 w_{t-2} + \dots + \theta_q w_{t-q}$$
where $w_t$ is white noise

Each of the $x_t$ is a sum of the most recent error terms

Thus, _all_ MA processes are stationary because they are finite sums of stationary WN processes


### Examples of MA(_q_) models

```{r ex_acf_MA}
## compare MA(1) & MA(2) with similar structure
MA1 <- arima.sim(n=50, list(ma=c(0.7)))
MA2 <- arima.sim(n=50, list(ma=c(-1, 0.7)))

par(mfrow = c(1,2), mai = c(1,1,0.3,0.1), omi = c(0,0,0,0))
## loop over orders of p
plot.ts(MA1, las = 1,
        ylab = expression(italic(x[t])))
mtext(side = 3,
      expression(MA(1):~italic(x[t])==~italic(w[t])+0.7~italic(w[t-1])),
      line = 0.5, adj = 0)
plot.ts(MA2, las = 1,
        ylab = expression(italic(x[t])))
mtext(side = 3,
      expression(MA(2):~italic(x[t])==~italic(w[t])-~italic(w[t-1])+0.7~italic(w[t-2])),
      line = 0.5, adj = 0)

```

### MA(1) Model

Let's start by figuring out some properties of the simplest MA model, the MA(1) model:

$$
x_t = \theta_0 + \theta_1 w_{t-1} + w_t
$$

We start by assuming that $x_t$ is a stationary time series. Under this assumption, we can show:

\begin{align}
E(x_t) &= \theta_0 \\
Var(x_t) &= \sigma^2_w(1+\theta_1^2) \\
\rho(h) &= \frac{\theta_1}{1+\theta_1^2} \text{ for } h=1 \text{ and 0 otherwise. }
\end{align}

### Invertibility

For MA models, we need invertibility in order to identify model paramters. An MA(_q_) process is *invertible* if it can be written as a stationary autoregressive process of infinite order without an error term

$$
x_t = w_t + \theta_1 w_{t-1} + \theta_2 w_{t-2} + \dots + \theta_q w_{t-q} \\
\Downarrow ? \\
w_t = x_t + \sum_{k=1}^\infty(-\theta)^k x_{t-k}
$$


For example, these MA(1) models are equivalent

$$
x_t = w_t + \frac{1}{5} w_{t-1} ~\text{with} ~w_t \sim ~\text{N}(0,25) \\
\Updownarrow \\
x_t = w_t + 5 w_{t-1} ~\text{with} ~w_t \sim ~\text{N}(0,1)
$$


Rewrite an MA(1) model in terms of $w_t$

$$
  x_t = w_t + \theta w_{t-1} \\
   \Downarrow \\
  w_t = x_t - \theta w_{t-1} \\
$$


If we constrain $\lvert \theta \rvert < 1$, then

$$
\lim_{k \to \infty} (-\theta)^{k+1} w_{t-k-1} = 0
$$

and

$$
\begin{align}
  w_t &= x_t - \theta x_{t-1} - \dots -\theta^k x_{t-k}  -\theta^{k+1} w_{t-k-1} \\
  w_t &= x_t - \theta x_{t-1} - \dots -\theta^k x_{t-k} \\
  w_t &= x_t + \sum_{k=1}^\infty(-\theta)^k x_{t-k}
\end{align}
$$


### Autocorrelation

For the MA(q) model, the autocovariance function cuts off for $h >q$. That is,

\begin{align}
\gamma(h) &= \sigma^2 \sum_{j=0}^{q-h} \theta_j \theta_{j+h}~\text{for }h=0,\ldots,q \\
\gamma(h) &= 0~h>q
\end{align}

```{r}
ma_mods <- vector("list", 2L)

## MA(2): phi[1] = 0.7, phi[2] = 0.2, phi[3] = -0.1, theta[1]= 0.5
ma_mods[[1]] <- arima.sim(list(ma=c(0.5, 0.3)), n=5000)
## MA(3): phi[1] = -0.7, phi[2] = 0.2, theta[1] = 0.7, theta[2]= 0.2
ma_mods[[2]] <- arima.sim(list(ma=c(0.7, 0.4, 0.6)), n=5000)

par(mfrow = c(2,2), mai = c(0.7,0.7,0.3,0.1), omi = c(0,0,0,0))

titles <- list(
  expression("MA(2)"),
  expression("MA(3)"))


## loop over orders of p
for(i in 1:2) {
  plot.ts(ma_mods[[i]][1:50], las = 1,
          main = "", ylab = expression(italic(x[t])))
  mtext(side = 3, titles[[i]],
        line = 0.5, adj = 0, cex = 0.8)

  acf(ma_mods[[i]])
  
}

```

Therefore, the sample ACF is useful for model identification when our data comes from a moving average process, but not so useful for data that comes from an AR process. For this, we introduce the partial autocorrelation (PACF).

## PACF

### Definition

The partial autocorrelation of a stationary process, $x_t$, denoted $\phi_{hh}$, for $h=1,2,\ldots$, is 

$$\phi_{11}=\text{cor}(x_{t}, x_{t-1}) = \rho_1$$
and,

$$\phi_{hh}=\text{cor}(x_{t}-\hat{x}_t, x_{t-h} - \hat{x}_{t-h}),~~h \geq 2$$.

The PACF, $\phi_{hh}$, is the correlation between $x_t$ and $x_{t-h}$ with the linear dependence of $\{x_{t-1}, \ldots, x_{t-h+1} \}$ on each, removed.

### AR(1) PACF

Let us calculate $\phi_{22}$ for the AR(1) model, $x_t = \phi x_{t-1} + w_t$. 

1. Consider the regression of $x_t$ on $x_{t-1}$. Choose $\beta$ to minimize,

\begin{align}
E(x_t - \beta x_{t-1})^2 &= \gamma(0) - 2\beta \gamma(1) + \beta^2 \gamma(0)
\end{align}

2. Minimize the expression above to find the estimator of $\beta$.

3. Plug in this estimated quantity to $\text{cor}(x_{t}-\hat{x}_{t-1}, x_{t-2} - \hat{x}_{t-2})$

### PACF for AR(p)

For an AR(p) model, when $h > p$, $\phi_{hh} = 0$. Thus, the PACF is very informative for model identification when data comes from an autoregressive process.

```{r}
ar_mods <- vector("list", 2L)

## MA(2): phi[1] = 0.7, phi[2] = 0.2, phi[3] = -0.1, theta[1]= 0.5
ar_mods[[1]] <- arima.sim(list(ar=c(0.5, 0.3)), n=5000)
## MA(3): phi[1] = -0.7, phi[2] = 0.2, theta[1] = 0.7, theta[2]= 0.2
ar_mods[[2]] <- arima.sim(list(ar=c(0.5, 0.3, 0.1)), n=5000)

par(mfrow = c(2,2), mai = c(0.7,0.7,0.3,0.1), omi = c(0,0,0,0))

titles <- list(
  expression("AR(2)"),
  expression("AR(3)"))


## loop over orders of p
for(i in 1:2) {
  plot.ts(ar_mods[[i]][1:50], las = 1,
          main = "", ylab = expression(italic(x[t])))
  mtext(side = 3, titles[[i]],
        line = 0.5, adj = 0, cex = 0.8)

  pacf(ar_mods[[i]])
  
}
```

## AR Estimation

### Yule-Walker Equations

Consider the AR(p) model, $x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \phi_3 x_{t-3} + w_t$

Now, take the following steps,

1. Multiply both sides by $x_{t-1}$
2. Take an expectation
3. This leads to,

$$
\left[ \begin{array}{c} \gamma(1) \\ \vdots \\ \gamma(p) \end{array} \right] = \begin{pmatrix} \gamma(0) & \ldots & \gamma(p-1) \\ \vdots & \ddots & \vdots \\ \gamma(p-1) & \ldots & \gamma(0) \end{pmatrix} \times \left[ \begin{array}{c} \phi_1 \\ \vdots \\ \phi_p \end{array} \right]
$$
Or, succinctly, 

$$\mathbf{\gamma} = \mathbf{\Gamma} \mathbf{\phi}$$

Which can be solved as,

$$\hat{ \mathbf{\phi} } = \mathbf{\Gamma} ^ {-1}\mathbf{\gamma} $$
The remaining parameter to be estimated is $\sigma^2$, the variance of the white noise term. An estimator can be established by multiplying the model by $x_t$, which leads to

$$\hat{\sigma}^2 = \hat{\gamma}(0) - \sum_{i=1}^{p}\hat{\phi}_i \hat{\gamma}(i)$$

### Example

Suppose you aim to estimate $\mathbf{\phi}$ for a dataset in which $\hat{\mathbf{\gamma}} = (3.8,3.1,3,2.8)$. What is the Yule-Walker estimate for $\mathbf{\phi}$ and also for $\sigma$?

```{r echo = F, eval=FALSE}
set.seed(1)
ar_mods <- arima.sim(list(ar=c(0.5, 0.3, 0.1)), n=5000)
fit = arima(ar_mods, order = c(3, 0, 0))
gamma = array ( (acf( (ar_mods - mean(ar_mods)), type = "covariance"))$acf ) 

Gmat = matrix(data = c(gamma[1:3], gamma[c(2,1,2)], gamma[c(3,2,1)]) , nrow = 3)
gvec = gamma[2:4]

phihat = solve(Gmat) %*% gvec
```


## ARMA models

An autoregressive moving average, or ARMA(_p_,_q_), model is written as

$$
x_t = \phi_1 x_{t-1} + \dots + \phi_p x_{t-p} + w_t + \theta_1 w_{t-1} + \dots + \theta_q w_{t-q} 
$$

We can write an ARMA(_p_,_q_) model using the backshift operator

$$
\phi_p (\mathbf{B}^p) x_t=  \theta_q (\mathbf{B}^q) w_t 
$$

ARMA models are _stationary_ if all roots of $\phi_p (\mathbf{B}) > 1$

ARMA models are _invertible_ if all roots of $\theta_q (\mathbf{B}) > 1$

### $\psi$ representation

For a causal ARMA$(p,q)$ model, $\phi_p (\mathbf{B}^p) x_t=  \theta_q (\mathbf{B}^q) w_t$, we may write

\begin{align}
x_t = \sum_{j=0}^\infty \psi_j w_{t-j}.
\end{align}

Solving for the $\psi$ weights in general is complicated and can be solved using the <code>ARMAtoMA</code> function in R. 

### ARMA(1,1)

<span style="color:red">Solve for the $\psi$ wieghts in the case of a causal, invertible ARMA(1,1) process. </span>

### Forecasting

To forecast with an ARMA model, we simply use the parameter estimates for future periods. For example, consider the following dataset of 500 observations simulated from an ARMA(1,1) process. 

```{r echo=TRUE}
set.seed(1)
ex_ts_arma = arima.sim(n = 500, list(ar = c(.8), ma = c(-.3)))
arma_fit = Arima(ex_ts_arma, order = c(1,0,1), include.mean = F)
```

The fitted model is $x_t = 0.7732447 x_{t-1} -0.2942922 w_{t-1}$. To forecast two observations in the future, we simply use the parameter estimates as follows:

```{r echo=TRUE}
w_vec = abs( fitted(arma_fit) - ex_ts_arma)
h1 = coefficients(arma_fit)[1] * ex_ts_arma[500] + coefficients(arma_fit)[2] * w_vec[500] # 0.2161869
h2 = coefficients(arma_fit)[1] * h1

```

Our one step ahead forecast is `r h1` and the two step ahead forecast is `r h2`. The same values can be computed using <code>forecast(arma_fit, h=2)$mean</code>.

### Forecast Errors

In order to compute forecast errors, we will make use of the following two parameters:

\begin{align}
x_{T+h} &= \sum_{j=0}^\infty \psi_j w_{T+h-j} \\
\tilde{x}_{T+h} &= E( x_{T+h} | x_1, \ldots, x_T )
\end{align}

Since $E(w_t | x_1, \ldots, x_T) = w_t$ for $t \leq T$ and 0 for $t > T$,

\begin{align}
x_{T+h} - \tilde{x}_{T+h}  &= \sum_{j=0}^{h-1} \psi_j w_{T+h-j} \\
\end{align}

Therefore, the variance of this quantity is:

\begin{align}
\sigma^2 \sum_{j=0}^{h-1} \psi_j ^2  \\
\end{align}

With the assumption of normally distributed errors, a 95\% prediction interval for $x_{T+h}$ is,

\begin{align}
\hat{x}_{T+h} \pm 1.96 \sqrt{ \hat{\sigma}^2_w \sum_{j=0}^{h-1} \psi_j^2 }
\end{align}

```{r echo=TRUE, eval = F}
psis = ARMAtoMA(ar = coefficients(arma_fit)[1], ma = coefficients(arma_fit)[2], lag.max = 2)

# confidence interval for one step ahead forecast
c(h1 - qnorm(.975) * sqrt( arma_fit$sigma2 ) , h1 + qnorm(.975) * sqrt ( arma_fit$sigma2 )) # (-1.8, 2.2)

# confidence interval for 2 step ahead forecast
h2_se = sqrt( arma_fit$sigma2 * (1 + psis[1]^2 ) )
c(h2 - qnorm(.975) * h2_se, h2 + qnorm(.975) * h2_se) # (-2.0, 2.4)
```

Or, use <code>forecast(arma_fit, h = 2)</code>

## Lab 2

1. Simulate data from the MA(1) model: $x_t = w_t+\theta_1w_{t-1}$ in which $\theta = (0.7)$. For sample sizes $n=10,10^3,10^5$, plot the difference between the theoretical autocorrelation and the observed (sample) autocorrelation for lags 1 through 5. What do you observe? Do the same thing for an MA(3) model. Include which values you use for $\theta_1, \theta_2, \theta_3$. See page 96 of the textbook for the theoretical AFC function for the MA(3) model. 

```{r echo=TRUE}
set.seed(1)

# MA1
theta = 0.7
sim1 = arima.sim(model = list(ma=c(theta)), n = 10)
sim2 = arima.sim(model = list(ma=c(theta)), n = 10^3)
sim3 = arima.sim(model = list(ma=c(theta)), n = 10^5)

acf_theoretical = c( theta/ (1 + theta^2) , rep(0,4))

simlist = list(sim1, sim2, sim3)
acf_sample = lapply(simlist, function(x) abs ( (acf(x, plot = F))$acf[2:6] - acf_theoretical ) )

plot(x = 1:5, y = acf_sample[[1]], type = "l", col = "red", 
     ylim = c(0,1), ylab = "Empirical-Theoretical ACF", main = "MA1",
     sub = "Small sample in red",
     xlab = "lags")
lines(acf_sample[[2]], col = "green")
lines(acf_sample[[3]], col = "blue")
```

```{r echo=TRUE}
# MA3
theta = c(0.8, 0.7, 0.4)
den = 1 + sum(theta^2)
acf_theoretical = c( (theta[1] + theta[2] * theta[2] + theta[2] * theta[3])/den,
              (theta[2] + theta[1] * theta[3])/den,
              theta[3]/den, rep(0,2))

sim1 = arima.sim(model = list(ma= theta), n = 10)
sim2 = arima.sim(model = list(ma= theta), n = 10^3)
sim3 = arima.sim(model = list(ma= theta), n = 10^5)


simlist = list(sim1, sim2, sim3)
acf_sample = lapply(simlist, function(x) abs ( (acf(x, plot = F))$acf[2:6] - acf_theoretical ) )

plot(x = 1:5, y = acf_sample[[1]], type = "l", col = "red", 
     ylim = c(0,1), ylab = "Empirical-Theoretical ACF", main = "MA3",
     xlab = "lags",
     sub = "Small sample in red")
lines(acf_sample[[2]], col = "green")
lines(acf_sample[[3]], col = "blue")
```

From page 96, for $1 \leq h \leq q$ 

\begin{align}
\rho(h) = \frac{\sigma^2_w \sum_{j=0}^{q-h} \theta_j \theta_{j+h}}{1+\theta_1^2 + \ldots + \theta_q^2},
\end{align}

and $\rho(h) = 0$ for $h > q$.


2. For an AR(1), derive an expression for $\phi_{22}$ (the lag 2 partial ACF). Follow the steps in [AR(1) PACF]. See also page 100 in the textbook. 

3. Using ACF & PACF for model ID. Complete the table with either _cuts off after $\ldots$_ or _tails off slowly_. Back up the 3 entries you fill in with either math or with empirical data (simulate data from the model type and show the relevant estimate quantities).

| Model   | ACF               | PACF                    |
|:-------:|:-----------------:|:-----------------------:|
| AR(_p_) | Tails off slowly  | Cuts off after lag _p_  |
| MA(_q_) | Cuts off after lag _q_  | Tails off slowly  |
| ARMA(_p_,_q_) | Tails off slowly  |  Tails off slowly |

4. Suppose you aim to estimate $\mathbf{\phi}$ for an AR(2) model in which $\hat{\mathbf{\gamma}} = (2.1,1.5,1.3)$ (i.e., $\hat{\gamma}(0)=2.1)$. What is the Yule-Walker estimate for $\mathbf{\phi}$ and also for $\sigma$? See [AR Estimation] for details.

$$
\left[ \begin{array}{c} 1.5 \\ 1.3 \end{array} \right] = \begin{pmatrix} 2.1 & 1.5 \\ 1.5 & 2.1 \end{pmatrix} \times \left[ \begin{array}{c} \phi_1 \\ \phi_2 \end{array} \right]
$$

```{r echo=TRUE}
Gamma_mat = matrix(c(2.1,1.5,1.5,2.1), nrow = 2)
phi_hat = solve(Gamma_mat) %*% c(1.5, 1.3) # 0.556, 0.222
sigma_hat = 2.1 - sum(phi_hat * c(1.5, 1.3)) # 0.98
```


5. Using the Egyptian export data from class, compute the 95\% interval for the one period and 2 period ahead forecasts. Use the $\psi$ representation from [Forecast Errors]. You can find the $\phi$ values using <code>ARMAtoMA</code>. Validate your answers using <code>forecast::forecast</code>.


\begin{align}
\hat{x}_{T+h} \pm 1.96 \sqrt{ \hat{\sigma}^2_w \sum_{j=0}^{h-1} \psi_j^2 }
\end{align}

```{r echo=TRUE}
library(fpp3)
exp = global_economy %>% filter(Code == "EGY") %>% select(Exports)

m0 = auto.arima(exp$Exports)
psi = ARMAtoMA(ar = coefficients(m0)[1:2], ma = coefficients(m0)[3], lag.max = 1 ) 

# forecast(m0, h = 2) # 12.4 - 18.0 - 23.6
c( 18.00745 - 1.96 * sqrt( m0$sigma2 ) , 18.00745 + 1.96 * sqrt( m0$sigma2 ) ) # h = 1
c( 20.04187 - 1.96 * sqrt( m0$sigma2 * (1+psi^2)  ) , 20.04187 + 1.96 * sqrt( m0$sigma2 * (1+psi^2)  ) ) # h = 2

```



