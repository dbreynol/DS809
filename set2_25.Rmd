# ARMA

## Operators

### Backshift operator

The _backshift shift operator_ ($\mathbf{B}$) is an important function in time series analysis, which we define as

$$ \mathbf{B} x_t = x_{t-1} $$

or more generally as

$$ \mathbf{B}^k x_t = x_{t-k} $$

<span style="color:red">For example, express a random walk, $x_t = x_{t-1} + w_t$, using $\mathbf{B}$.</span>

### The difference operator

The _difference operator_ ($\nabla$) is another important function in time series analysis, which we define as

$$ \nabla x_t = x_t - x_{t-1} $$

<span style="color:red">For example, what does first-differencing a random walk yield?</span>

The difference operator and the backshift operator are related

$$ \nabla^k = (1 - \mathbf{B})^k $$
Differencing is a simple means for removing a trend

The 1st-difference removes a linear trend

A 2nd-difference will remove a quadratic trend

```{r diff_linear, fig.align="center"}
## create biased RW
rr <- ww <- rnorm(50)
for(t in 2:50) {
  rr[t] <- 0.3 + rr[t-1] + ww[t]
}

par(mfrow = c(1,2), mai = c(0.5,0.8,0.1,0), omi=c(0,0,0,0))

## raw data
plot.ts(rr, las = 1,
        ylab = expression(italic(x[t])))
## first difference
plot.ts(diff(rr), las = 1,
        ylab = expression(nabla~italic(x[t])))
```

## Autoregressive (AR) models

An _autoregressive_ model of order _p_, or AR(_p_), is defined as

$$x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \dots + \phi_p x_{t-p} + w_t$$

where we assume

1. $w_t$ is white noise

2. $\phi_p \neq 0$ for an order-_p_ process

```{r ex_AR_models}
## the 4 AR coefficients
ARp <- c(0.7, 0.2, -0.1, -0.3)
## empty list for storing models
AR_mods <- vector("list", 4L)

par(mfrow = c(2,2), mai = c(0.7,0.7,0.3,0.1), omi = c(0,0,0,0))
## loop over orders of p
for(p in 1:4) {
  ## assume SD=1, so not specified
  AR_mods[[p]] <- arima.sim(n=50, list(ar=ARp[1:p]))
  plot.ts(AR_mods[[p]], las = 1,
          ylab = expression(italic(x[t])))
  mtext(side = 3, paste0("AR(",p,")"),
        line = 0.5, adj = 0)
}
```

### AR(1) Model

Let's start by figuring out some properties of the simplest AR model, the AR(1) model:

$$x_t = \phi_0 + \phi_1 x_{t-1} + w_t$$

We start by assuming that $x_t$ is a stationary time series. Under this assumption, we can show:

\begin{align}
E(x_t) &= \frac{\phi_0}{1-\phi_1} \\
Var(x_t) &= \frac{\sigma^2_w}{1-\phi_1^2} \\
\rho(h) &= \phi_1^h
\end{align}

For this to work, $|\phi_1| < 1$. 

### Stationarity

We seek a means for identifying whether our AR(_p_) models are also stationary. We can write out an AR(_p_) model using the backshift operator:

$$
  x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \dots + \phi_p x_{t-p} + w_t \\
  \Downarrow \\
\begin{align}
  x_t - \phi_1 x_{t-1} - \phi_2 x_{t-2} - \dots - \phi_p x_{t-p} &= w_t \\
  (1 - \phi_1 \mathbf{B} - \phi_2 \mathbf{B}^2 - \dots - \phi_p \mathbf{B}^p) x_t &= w_t \\
  \phi_p (\mathbf{B}^p) x_t &= w_t \\
\end{align}
$$

If we treat $\mathbf{B}$ as a number (or numbers), we can out write the _characteristic equation_ as $\phi_p (\mathbf{B}^p)$.

To be stationary, __all roots__ of the characteristic equation __must exceed 1 in absolute value__ As a bonus, when this condition is met, then the model is also *causal.*

<span style="color:red"> Example, for what value of $\phi_1$ is AR(1) model stationary? </span>

Are the following AR processes stationary?

1. $x_t = 0.5 x_{t-1} + w_t$
2. $x_t = -0.2 x_{t-1} + 0.4 x_{t-2} +  w_t$
3. $x_t = x_{t-1} + w_t$



```{r ar_comp_pos_neg, fig.height=4}
## list description for AR(1) model with small coef
AR_pos <- list(order=c(1,0,0), ar=0.7, sd=0.1)
## list description for AR(1) model with large coef
AR_neg <- list(order=c(1,0,0), ar=-0.7, sd=0.1)
## simulate AR(1)
AR1_pos <- arima.sim(n=500, model=AR_pos)
AR1_neg <- arima.sim(n=500, model=AR_neg)

## get y-limits for common plots
ylm1 <- c(min(AR1_pos[1:50],AR1_neg[1:50]), max(AR1_pos[1:50],AR1_neg[1:50]))

## set the margins & text size
par(mfrow=c(1,2), mai=c(0.8,0.8,0.3,0.2), oma=c(0,0,0,0))
## plot the ts
plot.ts(AR1_pos[1:50], ylim=ylm1, las = 1,
        ylab=expression(italic(x)[italic(t)]),
        main = "")
mtext(side = 3, expression(paste(phi[1]," = 0.7")),
      line = 0.4, adj = 0)
plot.ts(AR1_neg[1:50], ylim=ylm1, las = 1,
        ylab=expression(italic(x)[italic(t)]),
        main = "")
mtext(side = 3, expression(paste(phi[1]," = -0.7")),
      line = 0.4, adj = 0)
```

### Autocorrelation

The exponential decay observed in autocorrelation function for the the AR(1) model holds in general for AR(p). This decay may oscillate, as shown below.

```{r ex_acf_AR}
## set the margins & text size
par(mfrow=c(2,2), mai=c(0.8,0.8,0.3,0.2), oma=c(0,0,0,0))
## plot the ts
plot.ts(AR1_pos[1:50], ylim=ylm1, las = 1,
        ylab=expression(italic(x)[italic(t)]),
        main = "")
mtext(side = 3, expression(paste(phi[1]," = 0.7")),
      line = 0.4, adj = 0)
acf(AR1_pos, lag.max = 20, las = 1)
plot.ts(AR1_neg[1:50], ylim=ylm1, las = 1,
        ylab=expression(italic(x)[italic(t)]),
        main = "")
mtext(side = 3, expression(paste(phi[1]," = -0.7")),
      line = 0.4, adj = 0)
acf(AR1_neg, lag.max = 20, las = 1)
```


## Moving Average (MA) models

A moving average model of order _q_, or MA(_q_), is defined as

$$ x_t = w_t + \theta_1 w_{t-1} + \theta_2 w_{t-2} + \dots + \theta_q w_{t-q}$$
where $w_t$ is white noise

Each of the $x_t$ is a sum of the most recent error terms

Thus, _all_ MA processes are stationary because they are finite sums of stationary WN processes


### Examples of MA(_q_) models

```{r ex_acf_MA}
## compare MA(1) & MA(2) with similar structure
MA1 <- arima.sim(n=50, list(ma=c(0.7)))
MA2 <- arima.sim(n=50, list(ma=c(-1, 0.7)))

par(mfrow = c(1,2), mai = c(1,1,0.3,0.1), omi = c(0,0,0,0))
## loop over orders of p
plot.ts(MA1, las = 1,
        ylab = expression(italic(x[t])))
mtext(side = 3,
      expression(MA(1):~italic(x[t])==~italic(w[t])+0.7~italic(w[t-1])),
      line = 0.5, adj = 0)
plot.ts(MA2, las = 1,
        ylab = expression(italic(x[t])))
mtext(side = 3,
      expression(MA(2):~italic(x[t])==~italic(w[t])-~italic(w[t-1])+0.7~italic(w[t-2])),
      line = 0.5, adj = 0)

```

### MA(1) Model

Let's start by figuring out some properties of the simplest MA model, the MA(1) model:

$$
x_t = \theta_0 + \theta_1 w_{t-1} + w_t
$$

We start by assuming that $x_t$ is a stationary time series. Under this assumption, we can show:

\begin{align}
E(x_t) &= \theta_0 \\
Var(x_t) &= \sigma^2_w(1+\theta_1^2) \\
\rho(h) &= \frac{\theta_1}{1+\theta_1^2} \text{ for } h=1 \text{ and 0 otherwise. }
\end{align}

### Invertibility

For MA models, we need invertibility in order to identify model paramters. An MA(_q_) process is *invertible* if it can be written as a stationary autoregressive process of infinite order without an error term

$$
x_t = w_t + \theta_1 w_{t-1} + \theta_2 w_{t-2} + \dots + \theta_q w_{t-q} \\
\Downarrow ? \\
w_t = x_t + \sum_{k=1}^\infty(-\theta)^k x_{t-k}
$$


For example, these MA(1) models are equivalent

$$
x_t = w_t + \frac{1}{5} w_{t-1} ~\text{with} ~w_t \sim ~\text{N}(0,25) \\
\Updownarrow \\
x_t = w_t + 5 w_{t-1} ~\text{with} ~w_t \sim ~\text{N}(0,1)
$$


Rewrite an MA(1) model in terms of $w_t$

$$
  x_t = w_t + \theta w_{t-1} \\
   \Downarrow \\
  w_t = x_t - \theta w_{t-1} \\
$$


If we constrain $\lvert \theta \rvert < 1$, then

$$
\lim_{k \to \infty} (-\theta)^{k+1} w_{t-k-1} = 0
$$

and

$$
\begin{align}
  w_t &= x_t - \theta x_{t-1} - \dots -\theta^k x_{t-k}  -\theta^{k+1} w_{t-k-1} \\
  w_t &= x_t - \theta x_{t-1} - \dots -\theta^k x_{t-k} \\
  w_t &= x_t + \sum_{k=1}^\infty(-\theta)^k x_{t-k}
\end{align}
$$


### Autocorrelation

For the MA(q) model, the autocorrelation function cuts off for $h >q$. That is,

\begin{align}
\gamma(h) &= \sigma^2 \sum_{j=0}^{q-h} \theta_j \theta_{j+h}~\text{for }h=0,\ldots,q \\
\gamma(h) &= 0~h>q
\end{align}

```{r}
ma_mods <- vector("list", 2L)

## MA(2): phi[1] = 0.7, phi[2] = 0.2, phi[3] = -0.1, theta[1]= 0.5
ma_mods[[1]] <- arima.sim(list(ma=c(0.5, 0.3)), n=5000)
## MA(3): phi[1] = -0.7, phi[2] = 0.2, theta[1] = 0.7, theta[2]= 0.2
ma_mods[[2]] <- arima.sim(list(ma=c(0.7, 0.4, 0.6)), n=5000)

par(mfrow = c(2,2), mai = c(0.7,0.7,0.3,0.1), omi = c(0,0,0,0))

titles <- list(
  expression("MA(2)"),
  expression("MA(3)"))


## loop over orders of p
for(i in 1:2) {
  plot.ts(ma_mods[[i]][1:50], las = 1,
          main = "", ylab = expression(italic(x[t])))
  mtext(side = 3, titles[[i]],
        line = 0.5, adj = 0, cex = 0.8)

  acf(ma_mods[[i]])
  
}

```

Therefore, the sample ACF is useful for model identification when our data comes from a moving average process, but not so useful for data that comes from an AR process. For this, we introduce the partial autocorrelation (PACF).

## PACF

### Definition

The partial autocorrelation of a stationary process, $x_t$, denoted $\phi_{hh}$, for $h=1,2,\ldots$, is 

$$\phi_{11}=\text{cor}(x_{t}, x_{t-1}) = \rho_1$$
and,

$$\phi_{hh}=\text{cor}(x_{t}-\hat{x}_t, x_{t-h} - \hat{x}_{t-h}),~~h \geq 2$$.

The PACF, $\phi_{hh}$, is the correlation between $x_t$ and $x_{t-h}$ with the linear dependence of $\{x_{t-1}, \ldots, x_{t-h+1} \}$ on each, removed.

### AR(1) PACF

Let us calculate $\phi_{22}$ for the AR(1) model, $x_t = \phi x_{t-1} + w_t$. 

1. Consider the regression of $x_t$ on $x_{t-1}$. Choose $\beta$ to minimize,

\begin{align}
E(x_t - \beta x_{t-1})^2 &= \gamma(0) - 2\beta \gamma(1) + \beta^2 \gamma(0)
\end{align}

2. Minimize the expression above to find the estimator of $\beta$.

3. Plug in this estimated quantity to $\text{cor}(x_{t}-\hat{x}_{t-1}, x_{t-2} - \hat{x}_{t-2})$

### PACF for AR(p)

For an AR(p) model, when $h > p$, $\phi_{hh} = 0$. Thus, the PACF is very informative for model identification when data comes from an autoregressive process.

```{r}
ar_mods <- vector("list", 2L)

## MA(2): phi[1] = 0.7, phi[2] = 0.2, phi[3] = -0.1, theta[1]= 0.5
ar_mods[[1]] <- arima.sim(list(ar=c(0.5, 0.3)), n=5000)
## MA(3): phi[1] = -0.7, phi[2] = 0.2, theta[1] = 0.7, theta[2]= 0.2
ar_mods[[2]] <- arima.sim(list(ar=c(0.5, 0.3, 0.1)), n=5000)

par(mfrow = c(2,2), mai = c(0.7,0.7,0.3,0.1), omi = c(0,0,0,0))

titles <- list(
  expression("AR(2)"),
  expression("AR(3)"))


## loop over orders of p
for(i in 1:2) {
  plot.ts(ar_mods[[i]][1:50], las = 1,
          main = "", ylab = expression(italic(x[t])))
  mtext(side = 3, titles[[i]],
        line = 0.5, adj = 0, cex = 0.8)

  pacf(ar_mods[[i]])
  
}
```

## AR Estimation

## ARMA

## Lab 2

1. Simulate data from an AR(1) process in which $\phi_1 = 0.6$. For sample sizes $n=10,10^3,10^5$, plot the difference between the theoretical autocorrelation and the observed (sample) autocorrelation for lags 1 through 20. What do you observe?

2. Simulate data from the MA(3) model: $x_t = w_t+\theta_1w_{t-1}+\theta_2w_{t-2}+\theta_3w_{t-3}$ in which $\theta = (0.7,0.6,0.2)$. For sample sizes $n=10,10^3,10^5$, plot the difference between the theoretical autocorrelation and the observed (sample) autocorrelation for lags 1 through 20. What do you observe?

3. For an MA(1), $x_t = w_t + \theta w_{t-1}$ with $|\theta|<1$, derive an expression for $\phi_{22}$. Simulate data from this process and plot the difference between the theoretical value of $\phi_{22}$ and the sample quantity (which you can compute using <code>pacf</code>).

