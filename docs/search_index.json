[["index.html", "Artifex - Fall 2023 Chapter 1 Topics in Time Series 1.1 Autocorrelation 1.2 Assignment 3 1.3 Change Points 1.4 Assignment 4 1.5 Moving Average 1.6 Exponential Smoothing 1.7 Assignment 5", " Artifex - Fall 2023 David Reynolds 2023-12-17 Chapter 1 Topics in Time Series 1.1 Autocorrelation A central concept in time series analysis is autocorrelation. This is the correlation between \\(y_t\\) and its lagged value. For a lag of \\(h\\), this is the correlation between \\(y_t\\) and \\(y_{t-h}\\). Let’s first review correlation. For two vectors of data, \\(x\\) and \\(y\\), the correlation between the two is, \\[\\begin{align} \\text{cor}(x,y) &amp;= \\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} \\\\ &amp;= \\frac{ \\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{ \\sqrt{ \\sum_i (x_i - \\bar{x})^2 \\sum_i (y_i - \\bar{y})^2 }} \\end{align}\\] set.seed(1) n = 5 x = rnorm(n) y = rnorm(n) sum( (x - mean(x)) * (y - mean(y))) / ( (n-1) * sd(x) * sd(y) ) cor(x, y) The autocorrelation applies this concept to a single time series. Autocorrelation is the correlation of a time series with a delayed copy of itself as a function of delay. The autocorrelation for a time series \\(y\\) at lag \\(k\\) is: \\[\\begin{equation} r_k = \\frac{ \\sum_{t = k + 1}^{T} (y_t - \\bar{y})(y_{t-k} - \\bar{y}) }{\\sum_{t=1}^T (y_t - \\bar{y})^2 } \\end{equation}\\] Here is a simple example of computing a lag 1 autocorrelation. a = c(1,2,3,4,5) a1 = c(1,2,3,4) a2 = c(2,3,4,5) # lag 1 autocorrelation sum( (a1 - mean(a)) * (a2 - mean(a))) / (sum( (a - mean(a))^2 ) ) # by hand (acf(a)) 1.2 Assignment 3 Write a function in R that takes a vector and a lag value, \\(k\\), as arguments and outputs that autocorrelation of the vector with the lag \\(k\\) copy of itself. Compute the lag 1 through 14 acf for the Kayak visits data (group by date to get total visits per day). What do you notice? Verify the results of your function using acf. Find an economic variable of interest and compute the lag 1 through lag 10 acf of the variable. Do the same thing after applying the diff function to your data. Describe the results of this analysis. What does it tell you, if anything, about the variable you chose? 1.3 Change Points 1.3.1 CUSUM Consider the following time series \\(x_t\\), for \\(t = 1, \\ldots, T\\), where \\(T=100\\). Suppose you are an analyst at an industrial company and this is a data stream from some instrument that you expect to generate random noise around 0. If there is a shift in the mean of this process, that signals trouble. Looking at this data, do you think there is any shift? set.seed(1) x1 = rnorm(75, 0, 0.5) x2 = rnorm(25, 0.25, 0.5) xt = c(x1, x2) plot(xt, type = &quot;l&quot;) It is hard to say. One method to detect change points is CUSUM. This method works by incorporating all of the information in the sequence in the form of a cumulative sum. To implement this method, first standardize the data. We can call the new time series \\(y\\) where each element, \\(y_i\\), can is a standardized version of the corresponding \\(x_i\\): \\[\\begin{align} y_i = \\frac{x_i - \\mu_0}{\\sigma} \\end{align}\\] Then, define two vectors, \\(C^+\\) and \\(C^-\\) whose first element is 0. That is, \\(C_1^+ = C_1^- = 0\\). For \\(i \\in (2,\\ldots,T)\\), \\[\\begin{align} C_i^+ &amp;= \\text{max}(0, y_i - K + C_{i-1}^+) \\\\ C_i^- &amp;= \\text{min}(0, y_i + K + C_{i-1}^-) \\end{align}\\] The parameter \\(K\\) is the shift in means that you hope to detect, \\(\\delta \\sigma\\), measured in standard deviations. For example, if you want to detect a shift of a half standard deviation, use \\(K=0.5\\). This is implemented in the qcc package within the cusum function, where the parameter \\(\\delta\\) is controlled by se.shift. 1.4 Assignment 4 Write a function to generate vectors \\(C^+\\) and \\(C^-\\) for a given time series and \\(\\delta = 1\\). Use the function on simulated data. How does it perform? Compare with output from cusum. Show the plotted data to another team and ask them for an eyeball estimate of the changepoint. Who won: (wo)man or machine? 1.5 Moving Average Another way to detect whether changes are occurring in a process is with a simple moving average. For a time series \\(y_t\\), \\(t = 1, \\ldots, T\\), a moving average of order \\(m\\) can be written, \\[\\begin{align} \\hat{y_t} = \\frac{1}{m} \\sum_{j=-k}^{k} y_{t+j}, \\end{align}\\] where \\(m=2k+1\\). The concept behind this technique is that observations that are close in time are likely to be close in value. Below, I have simulated a noisy time series with a strong trend component and plotted the data long with the order 5 moving average and the order 10 moving average. set.seed(1) xt = 0.5 * 1:100 + rnorm(100, 0, 5) #plot(xt, type = &quot;l&quot;, main = &quot;Simulated Data&quot;) df = data.frame(t = 1:100, data = xt, ma_5 = forecast::ma(xt, 3), ma_10 = forecast::ma(xt,11)) %&gt;% pivot_longer(-t) ggplot(df, aes(x = t, y = value, color = name)) + geom_line() + theme_minimal() + ggtitle(&quot;Higher order MA leads to a smoother series&quot;) 1.6 Exponential Smoothing When you have data that does not have a clear trend, you may want a forecasting method that averages over the past data values but that assigns relatively more weight to observations that are relatively closer. One way to achieve this is with simple exponential smoothing. This idea can be expressed as, \\[\\begin{align} \\hat{y}_{t+1 | t} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t | t-1}. \\end{align}\\] Since we don’t have data prior to \\(y_1\\), let us denote \\(\\hat{y}_{1 | 0} = l_0\\). Therefore, this model depends on two parameters, \\((l_0, \\alpha)\\). More generally, each predicted value \\(\\hat{y_t}\\) can be expressed, \\[\\begin{align} \\hat{y}_{t+1 | t} = (1-\\alpha) ^ t l_0 + \\sum_{j=0}^{t-1} \\alpha (1-\\alpha) ^ j y_{t-j}. \\\\ \\end{align}\\] 1.7 Assignment 5 Write your own moving average function that takes a vector (your time series) and a scalar (order of the moving average, \\(m\\)) as arguments and outputs an order \\(m\\) moving average of the vector input. Make sure that your output has the same length as the vector input, with NA values where appropriate. For \\(\\alpha = 0.3172\\) and \\(l_0 = 1.1199\\), write a function that computes a vector of predictions (i.e., \\(\\hat{y}_t\\) for all \\(t\\)) using exponential smoothing. Use the xt vector from the code block above (using a seed value of 1) and compare your results with the vector generated below. Hint: you might find it helpful to use the sapply function for the summation (but don’t let this restrict your creativity). fitted_vals = forecast::ets(xt, model = &quot;ANN&quot;)$fitted "],["linear-regression-revisited.html", "Chapter 2 Linear Regression Revisited 2.1 Estimation 2.2 Example 2.3 Assignment 2", " Chapter 2 Linear Regression Revisited Let’s consider a scenario in which we have information on a response variable, \\(y\\), and \\(p\\) predictor variables organized in a design matrix \\(X\\). We want to model the conditional mean, \\(E(Y | X)\\) using a linear combination of predictor variables. So, our model is: \\[\\begin{align} y = X \\beta + \\epsilon \\end{align}\\] For now, let us assume that there is a linear relationship between \\(y\\) and \\(X\\) and that the errors (the elements of \\(\\epsilon\\)) are independent with mean of 0 and variance of \\(\\sigma^2\\). 2.1 Estimation To find the estimate for \\(\\beta\\), we can use the criteria of least squares and find the \\(\\beta\\) that minimizes the sum of the squared errors. That is, our \\(\\hat{\\beta}\\) is the vector that minimizes, \\[\\begin{align} (y - X \\beta) ^ T (y - X \\beta) \\end{align}\\] In order to minimize this function, we need some basic results for matrix derivatives. \\[\\begin{align} \\frac{\\partial}{\\partial \\beta} (A \\beta) &amp;= A \\\\ \\frac{\\partial}{\\partial \\beta} (\\beta^T M \\beta) &amp;= 2 M \\beta \\end{align}\\] 2.2 Example Let’s investigate the relationship between home prices and mortgage rates. To start, let’s use data from the quantmod package and look at the relationship between these two variables from 2000 to 2004. # https://fred.stlouisfed.org/ pr = getSymbols(&#39;MORTGAGE30US&#39;,src=&#39;FRED&#39;, warnings = F) # 30-Year Fixed Rate Mortgage Average home = getSymbols(&#39;CSUSHPINSA&#39;, src = &#39;FRED&#39;) # S&amp;P/Case-Shiller U.S. National Home Price Index gdp = getSymbols(&#39;A939RX0Q048SBEA&#39;, src = &#39;FRED&#39;) # clean up dataset mort = tidy(MORTGAGE30US) %&gt;% # tidy is from the broom pkg - coerces xts object to a data frame mutate(date = ymd(index)) %&gt;% # coerce the index to be a date filter(year(date) &gt;= 2010, year(date) &lt;= 2023) %&gt;% # date range mutate(mo_year = ymd ( str_c (year(date), &quot;-&quot;, month(date), &quot;-&quot;, 01 )) ) %&gt;% # make a new date so we can group by month/year group_by(mo_year) %&gt;% summarise(val = mean(value)) %&gt;% filter(mo_year &lt; &quot;2023-08-01&quot;) home_pr = tidy(CSUSHPINSA) %&gt;% mutate(date = ymd(index)) %&gt;% filter(year(date) &gt;= 2010, year(date) &lt;= 2021) gd = tidy(A939RX0Q048SBEA) plot(gd$index, gd$value) comb = inner_join(gd, home_pr, by = c(&quot;index&quot;)) with(comb, plot(value.x, value.y)) m0 = lm(value.y ~ value.x, data = comb) ggplot(comb, aes(x = value.x, y = value.y, color = as.factor(year(index)))) + geom_point() ggplot(comb, aes(x = index, y = residuals(m0))) + geom_point() combined_dat = data.frame(mort = mort$val, pr = home_pr$value, date = home_pr$date) ggplot(combined_dat, aes( mort, pr, color = as.factor( year(date) ) ) ) + geom_point() + theme_minimal() + ggtitle(&quot;Case Shiller Index (y) and Avg. Mortgage Rates (x)&quot;) + xlab(&quot;30-Year Fixed Rate Mortgage Average&quot;) + ylab(&quot;S&amp;P/Case-Shiller U.S. National Home Price Index&quot;) How would you describe this relationship? Based on the relationship, do you think a simple linear regression model is appropriate for these data? 2.3 Assignment 2 Fit a linear regression model to these two variables and write out the model equation. Make a histogram of the residuals from this model and comment on how they correspond with the assumption of normal residuals. Plot the residuals over time. What do you see? Based on the residuals over time (plot from question 3), is there any additional variable you could add to the model to improve fit? If so, add this variable as a covariate to the linear regression and compare the fit of the augmented model with the one you fit in question 1. Currently, the 30-Year Fixed Rate Mortgage Average is around 8%. What does the model predict for the Case-Shiller Index? The current value of the Case-Shiller Index is 305. What is the residual? Now, using a scatterplot, check the relationship between these two variables for a more recent time frame (such as the last 10 years). Do you see the same relationship? Color code observations by year. "],["kayak-data-analysis.html", "Chapter 3 Kayak Data Analysis 3.1 Prompt", " Chapter 3 Kayak Data Analysis We are going to do some analysis on a dataset sent from Kayak. The prompts and data are from their data science recruiting process. To access the data, use the following code: visits = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/visits.csv&quot;) conversions = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/conversions.csv&quot;) channels = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/channel_descriptions.csv&quot;) 3.1 Prompt One of our team’s goals is to accurately evaluate how successful our marketing campaigns are and to predict how successful future campaigns will be. In this assignment, we want you to explore how to evaluate one notion of success: conversion rate. Depending on the context, a conversion can mean different things, including a flight or hotel booking, a click on a specific part of our site, or an account sign up. Conversion rate in the first case could therefore be defined as completed flight bookings per user visit. You were given some datasets that contain a sanitized version of some of our logging data. Explore the datasets and work on the questions below. One of the CSVs contains data on daily user visits broken down by user location and marketing channel, and another contains data on conversions also broken down by user location and channel. You also have been given a file with a brief description of the channels. Do you see any interesting patterns in the data? For example, are there any seasonality trends in user visits or conversion rates (conversions/visits)? Do these seasonality trends differ between countries or channel? Can you think of any potential explanations for any of the patterns/trends/differences you saw? Which channels have the best and worst conversion rates? What reasons can you think of to explain the differences? Do you see any major conversion differences for the same channel in different countries? Predict next month’s (September 2015) aggregate conversion rates for each channel and country combination? How good are your predictions? Last time, we discussed a few ways to estimate the slope coefficient in a linear regression model. This yields a point estimate. The other key ingredient for inference is to determine how much uncertainty there is in our estimate. For the case of \\(\\hat{\\beta_1}\\), we want to know how much this estimate varies from sample to sample for the specified sample size. That is, we want to know the variance of the estimate: \\[\\begin{align} Var(\\hat{\\beta_1}). \\end{align}\\] To find what this is, let’s just try to figure out the variance directly, using the analytic formula for \\(\\hat{\\beta_1}\\) in simple linear regression: \\[\\begin{align} Var \\bigg( \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2 } \\bigg). \\end{align}\\] To carry out this calculation, recall that \\(\\text{var}(cx) = c^2 \\text{var}(x)\\). Also, recall the model assumption of independence across observations. After carrying out this calculation, we can show that, \\[\\begin{equation} Var(\\hat{\\beta}) = \\frac{\\hat{\\sigma}^2}{(n-1) \\hat{var}(x)} \\end{equation}\\] How does variation in \\(\\hat{\\beta}\\) respond to changes in sample size? How about to variation in the predictor? Before moving on, let’s write some code to make sure our calculations correspond with those generated by lm. In this case, we are able to obtain not only the variation of the estimate but also the distribution of the estimate as well as the test statistic, \\[\\begin{align} T = \\frac{\\hat{\\beta_1} - \\beta}{\\sqrt{\\text{var}\\beta}} \\sim t_{n-2} \\end{align}\\] This allows us to compute confidence intervals. For instance, if we want to compute a (1-\\(\\alpha\\)) confidence interval, note that: \\[\\begin{align} P(t_{\\alpha/2} &lt; T &lt; t_{1-\\alpha/2}) = (1-\\alpha) \\end{align}\\] "],["set-1.html", "Chapter 4 Set 1 4.1 Time Series Data 4.2 Time Series EDA 4.3 Multiple Time Series 4.4 Autocorrelation 4.5 Lab 1", " Chapter 4 Set 1 idea- make a lab question about smoothing 4.1 Time Series Data A time series is an ordered sequence of observations, where the ordering is through time. Time series data creates unique problems for statistical modeling and inference. Traditional inference assumes that observations (data) are independent and identically distributed. Adjacent data points in time series data are not necessarily independent (uncorrelated). Most time series models aim to exploit such dependence. For instance, yesterday’s demand of a product may tell us something about today’s demand of a product. There are several different ways to represent time series data in R. We will use the tidyverse family of packages extensively in this class. This package includes the lubridate package, which includes functions to work with date-times. Two of the most common ways to represent time series data are using data frames in which one of the variables is a time object (such as POSIXct or Date) or using a time series object. These two representations are shown below with simulated trading data for a single 8-hour trading day. set.seed(1) # option 1: represent time series data within a data frame hr = seq(mdy_hm(&quot;12-11-2023 09:30&quot;), mdy_hm(&quot;12-11-2023 16:30&quot;), &#39;hour&#39;) # 8 hours pr = rnorm(8) # generate fake trading data trading_dat = data.frame(hr, pr) # option 2: represent time series data using a time series object trading_ts = ts(data = trading_dat$pr, start = 1, frequency = 8) 4.2 Time Series EDA The first thing to do in any data analysis is exploratory data analysis (EDA). Graphs enable many features of the data to be visualized, including patterns, unusual observations, changes over time, and relationships between variables. The features that are seen in plots of the data can then be incorporated into statistical models. R has several systems for making graphs. We will primarily use ggplot2, which is among the set of tidyverse packages and is one of the most versatile systems for plotting. We will use a data set from Kayak to motivate our analysis. conversions = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/conversions.csv&quot;) knitr::kable(head(conversions)) datestamp country_code marketing_channel conversions 2014-11-01 be Display Ads 1174 2014-11-01 be KAYAK Deals Email 57 2014-11-01 be Search Engine Ads 1122 2014-11-01 be Search Engine Results 976 2014-11-01 fi Display Ads 12 2014-11-01 fi Search Engine Results 2 This dataset contains information on the total number of daily conversions by country and marketing channel. Let us focus our analysis on the US and fist visualize the number of conversions by day. This plot contains a lot of useful information. To gain insight into how conversions depend on marketing channel, we can use facets. Facets are subplots that display a time series for each marketing channel. Display ads and search engine ads are the dominant marketing channels. Both have a regular pattern that is likely a function of the day of week, with a higher number of conversions during weekdays as compared with weekends. We can explore this feature by aggregating over each weekday and visualizing how the distribution of conversions changes by day. Clearly, there are significant changes in the mean level of conversions across the week. This is a form of seasonality. It may be useful to see what the data look like when this weekday effect is removed. To do so, we could visualize the residuals from the following linear regression model: \\[\\begin{align} \\hat{\\text{conversions}} = \\hat{\\beta}_0 + \\sum_{j=2}^7 \\bigg( \\hat{\\beta}_j \\times 1(\\text{weekday = j}) \\bigg), \\end{align}\\] where \\(j\\) indexes the day of week. The residuals from this model consist of each observation minus the mean for that particular weekday. This allows us to more clearly see the trend across the date range, removing the effect of the weekly pattern. 4.3 Multiple Time Series Often we will want to develop insight into the relationship between several variables. To illustrate, we will use quarterly data on GDP per capita and the Case Shiller Home Price Index (both from the FRED database). It looks like these two time series track pretty closely to one another. We could fit a linear regression to this data in order to estimate the expected change in the Case Shiller Index for a unit ($1) change in GDP/ capital term estimate std.error statistic p.value (Intercept) -556.2290146 33.8244315 -16.44459 0 gdp 0.0126208 0.0005663 22.28669 0 Further, we could also examine the residuals to gain insight into what is missing from this model. The model severely underestimates the house index starting during the pandemic. There is a clear pattern to these residuals. Is this a problem? 4.4 Autocorrelation One of the assumptions of the linear regression model is that the errors are independent and identically distributed. That is, for the model, \\[\\begin{align} y = X \\beta + \\epsilon, \\end{align}\\] The error vector, \\(\\epsilon \\sim N(0, \\sigma^2)\\). This implies that there is no correlation structure to the residuals. One way to check that this is true is to check for the absence of autocorrelation in the observed residuals. Given a time series, \\(y_t\\), where \\(t=1,\\ldots,T\\), the correlation between \\(y_t\\) and its lagged value, \\(y_{t-k}\\), is defined as autocorrelation. Let’s first review correlation. For two vectors of data, \\(x\\) and \\(y\\), the correlation between the two is, \\[\\begin{align} \\text{cor}(x,y) &amp;= \\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} \\\\ &amp;= \\frac{ \\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{ \\sqrt{ \\sum_i (x_i - \\bar{x})^2 \\sum_i (y_i - \\bar{y})^2 }} \\end{align}\\] set.seed(1) n = 5 x = rnorm(n) y = rnorm(n) sum( (x - mean(x)) * (y - mean(y))) / ( (n-1) * sd(x) * sd(y) ) cor(x, y) Autocorrelation applies this concept to a single time series. Autocorrelation is the correlation of a time series with a delayed copy of itself as a function of delay. The autocorrelation for a time series \\(y\\) at lag \\(k\\) is: \\[\\begin{align} r_k &amp;= \\frac{\\text{Cov}(y_t, y_{t-k})}{\\sigma_{y_t} \\sigma_{y_{t-k}}} \\\\ &amp;= \\frac{ \\sum_{t = k + 1}^{T} (y_t - \\bar{y})(y_{t-k} - \\bar{y}) }{\\sum_{t=1}^T (y_t - \\bar{y})^2 } \\end{align}\\] Here is a simple example of computing a lag 1 autocorrelation. a = c(1,2,3,4,5) a1 = c(1,2,3,4) a2 = c(2,3,4,5) # lag 1 autocorrelation sum( (a1 - mean(a)) * (a2 - mean(a))) / (sum( (a - mean(a))^2 ) ) # by hand (acf(a)) functions in R assignment - write a function 4.5 Lab 1 Starting from the code chunk in Time Series Data, extend the simulated training data to a full week (December 11 through December 15, eight hours each day). Using the data frame representation, plot(trading_dat$hr, trading_dat$pr). Using the time series data, plot(trading_ts). What are the differences between these two plots? We can further hone in on the trend of the residuals at the end of Time Series EDA by computing and plotting a moving average. For a time series \\(y_t\\), \\(t = 1, \\ldots, T\\), a moving average of order \\(m\\) can be written, \\[\\begin{align} \\hat{y_t} = \\frac{1}{m} \\sum_{j=-k}^{k} y_{t+j}, \\end{align}\\] where \\(m=2k+1\\). The concept behind this technique is that observations that are close in time are likely to be close in value. Compute a moving average of order \\(m=7\\) for the residual time series and plot it along with the residuals in a single plot. "],["set-2.html", "Chapter 5 Set 2 5.1 Exponential Smoothing 5.2 Decomposition 5.3 Autoregressive Models", " Chapter 5 Set 2 5.1 Exponential Smoothing 5.2 Decomposition 5.3 Autoregressive Models 5.3.1 Stationarity 5.3.2 ACF Function "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
