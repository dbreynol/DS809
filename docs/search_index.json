[["index.html", "DS809 Set 1 Course Info", " DS809 David Reynolds 2025-02-18 Set 1 Course Info The course is designed to introduce techniques needed in the estimation/analysis of temporal data (time series) in various business disciplines. The first half of the course focuses on traditional stationary time series models. Some examples of business application areas include demand forecasting, financial asset return modeling, stochastic volatility modeling of financial indexes and securities, mortgage default risk assessment, call center arrival modeling, online webpage click-rate modeling, and market share modeling. The second half of the course focuses on state space modelling approaches to time series data and Bayesian techniques for time series data. "],["exploratory-analysis-of-time-series-data.html", "Set 2 Exploratory Analysis of Time Series Data 2.1 Time Series Data 2.2 Time Series EDA 2.3 Classical Regression 2.4 Noise Processes 2.5 Measures of Dependence 2.6 Stationarity 2.7 Estimation 2.8 Lab 1", " Set 2 Exploratory Analysis of Time Series Data 2.1 Time Series Data A time series is an ordered sequence of observations, where the ordering is through time. Time series data creates unique problems for statistical modeling and inference. Traditional inference assumes that observations (data) are independent and identically distributed. Adjacent data points in time series data are not necessarily independent (uncorrelated). Most time series models aim to exploit such dependence. For instance, yesterday’s demand of a product may tell us something about today’s demand of a product. There are several different ways to represent time series data in R. We will use the tidyverse family of packages extensively in this class. This package includes the lubridate package, which includes functions to work with date-times. Two of the most common ways to represent time series data are using data frames in which one of the variables is a time object (such as POSIXct or Date) or using a time series object. 2.2 Time Series EDA The first thing to do in any data analysis is exploratory data analysis (EDA). Graphs enable many features of the data to be visualized, including patterns, unusual observations, changes over time, and relationships between variables. The features that are seen in plots of the data can then be incorporated into statistical models. R has several systems for making graphs. We will primarily use ggplot2, which is among the set of tidyverse packages and is one of the most versatile systems for plotting. We will use a data set from Kayak to motivate our analysis. conversions = read.csv(&quot;data/conversions.csv&quot;) #read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/conversions.csv&quot;) knitr::kable(head(conversions)) datestamp country_code marketing_channel conversions 2014-11-01 be Display Ads 1174 2014-11-01 be KAYAK Deals Email 57 2014-11-01 be Search Engine Ads 1122 2014-11-01 be Search Engine Results 976 2014-11-01 fi Display Ads 12 2014-11-01 fi Search Engine Results 2 This dataset contains information on the total number of daily conversions by country and marketing channel. Let us focus our analysis on the US and first visualize the number of conversions by day. This plot contains a lot of useful information. To gain insight into how conversions depend on marketing channel, we can use facets. Facets are subplots that display a time series for each marketing channel. Display ads and search engine ads are the dominant marketing channels. Both have a regular pattern that is likely a function of the day of week, with a higher number of conversions during weekdays as compared with weekends. We can explore this feature by aggregating over each weekday and visualizing how the distribution of conversions changes by day. Clearly, there are significant changes in the mean level of conversions across the week. This is a form of seasonality. It may be useful to see what the data look like when this weekday effect is removed. To do so, we could visualize the residuals from the following linear regression model: \\[\\begin{align} \\hat{\\text{conversions}} = \\hat{\\beta}_0 + \\sum_{j=2}^7 \\bigg( \\hat{\\beta}_j \\times 1(\\text{weekday = j}) \\bigg), \\end{align}\\] where \\(j\\) indexes the day of week. The residuals from this model consist of each observation minus the mean for that particular weekday. This allows us to more clearly see the trend across the date range, removing the effect of the weekly pattern. 2.3 Classical Regression Given that regression is already in our tool kit, we will use it as our first method to model a time series. Let us assume that we have some output or dependent time series, say, \\(x_t\\) , for \\(t = 1, \\ldots,n\\), that is being influenced by a collection of possible inputs, say, \\(z_{t1}, z_{t2}, \\ldots, z_{tq}\\), where we regard the inputs as fixed and known. We will look at monthly Australian electricity production (the fma::elec dataframe). The data is shown below. Let us fit two models to this data, one where we use time as the independent variable. This estimates a linear trend. In the other, we will try to capture some of the obvious seasonality in the data. \\[\\begin{align} y_t &amp;= \\beta_0 + \\beta_1 t + w_t \\\\ y_t &amp;= \\beta_0 + \\beta_1 t + \\beta_2 \\times ({2\\pi \\cos(t)}) + w_t \\end{align}\\] The fits from these two models are shown below. Clearly, model 2 offers a superior fit as compared with model 1. In fact, this model explains about 95% of the variation in electricity production. To check the validity of the model, let us examine the residuals across the range of time, \\(t\\). One of the assumptions of the linear regression model is that the errors are independent and identically distributed. That is, for the model, \\[\\begin{align} y = X \\beta + \\epsilon, \\end{align}\\] The error vector, \\(\\epsilon \\sim N(0, \\sigma^2)\\), consists of independent and identically distributed random variables. This implies that there is no correlation structure to the residuals. One way to check that this is true is to check for the absence of correlation in the observed residuals. What does the figure above indicate about the validity of this assumption for model 2? 2.4 Noise Processes What if we re-arrange the model we fit above into a deterministic part and a random part: \\[\\begin{align} y_t - x_t \\beta = w_t \\end{align}\\] What can we say about the right hand side? Since it is a random variable that is indexed by time, we can consider this our first and simplest time series model. 2.4.1 White noise Let \\(w_t\\) be a random variable indexed by time, \\(t \\in [1,T]\\). The following properties characterize white noise: \\[\\begin{align} E(w_t) &amp;= 0 \\\\ Var(w_t) &amp;= \\sigma^2 \\\\ Cov(w_t, w_s) &amp;= 0 \\text{ } \\forall t,s \\end{align}\\] Note that Gaussian white noise is a special case where \\(w_t \\sim N(o, \\sigma^2)\\). White noise can come in different flavors. In the right hand plot below, \\(w_t = 2e_t - 1\\), where \\(e_t \\sim \\text{Bernoulli(.5)}\\). In the left plot, \\(w_t \\sim N(0,2)\\). 2.4.2 Random Walk Let us consider another simple model to describe time series data, \\(y_t = y_{t-1}+w_t\\), where \\(w_t \\sim N(0, \\sigma^2)\\) and all elements of the error vector are mutually independent. Let’s derive some important properties of this model: What is the mean, \\(E(y_t)\\)? What is the variance, \\(Var(y_t)\\)? 2.4.3 AR Process A slightly more general model is the autoregressive (AR) model, in which \\(x_t\\) is a linear combination of its past \\(p\\) values plus gaussian white noise, \\(w_t\\). That is, \\[\\begin{align} x_t = \\sum_{i=1}^p \\phi_i x_{t-i} + w_t. \\end{align}\\] 2.4.4 Exercise Consider the AR(1) process with a drift: \\[\\begin{align} x_t &amp;= a + \\phi x_t + w_t \\end{align}\\] Write \\(x_t\\) as a function of past white noise values and find \\(E(x_t)\\) and \\(Var(x_t)\\). 2.5 Measures of Dependence 2.5.1 Autocovariance In all but the simplest models, there is dependence between adjacent values \\(x_s\\) and \\(x_t\\). This can be assessed using the notions of covariance and correlation. The autocovariance function is defined as the second moment product: \\[\\begin{align} \\gamma(s,t) &amp;= \\text{cov}(x_s, x_t) \\\\ &amp;= E((x_s - \\mu_s) (x_t - \\mu_t)) \\end{align}\\] The autocovariance measures the linear dependence between two points on the same series observed at different times. Very smooth series exhibit autocovariance functions that stay large even when the \\(t\\) and \\(s\\) are far apart, whereas choppy series tend to have autocovariance functions that are nearly zero for large separations. 2.5.2 Autocorrelation The autocorrelation function (ACF) is defined as, \\[\\begin{align} \\rho(s,t) &amp;= \\frac{\\gamma(s,t)}{\\sqrt{\\gamma(s,s) \\gamma(t,t)}} \\end{align}\\] The ACF measures the linear predictability of the series at time \\(t\\), say \\(x_t\\) , using only the value \\(x_s\\). 2.5.3 Exercise Compute the autocorrelation of: The white noise process The random walk \\(x_t = a + \\phi x_{t-1} + w_t\\) 2.6 Stationarity A (weakly) stationary time series \\(x_t\\) is a process such that, The mean is constant: \\(E(X_t) = \\mu\\). The autocovariance function, \\(\\gamma(s,t)\\) depends on \\(s\\) and \\(t\\) only through their difference, \\(|s-t|\\). Therefore, we can write the autocovariance function of a stationary time series as, \\[\\begin{align} \\gamma(h) &amp;= \\text{cov}(x_{t+h}, x_{t}) \\\\ &amp;= E((x_{t+h} - \\mu) (x_{t} - \\mu)) \\end{align}\\] And, similarly, the autocorrelation function of a stationary time series can be written, \\[\\begin{align} \\rho(h) &amp;= \\frac{\\gamma(t+h,t)}{\\sqrt{\\gamma(t+h,t+h) \\gamma(t,t)}} \\\\ &amp;= \\frac{\\gamma(h)}{\\gamma(0)} \\end{align}\\] 2.7 Estimation Although the theoretical mean, autocovariance, and autocorrelation functions are useful for describing the properties of certain hypothesized models, we also need to estimate these quantities using sampled data. 2.7.1 Mean If a time series is stationary, the mean function is constant so that we can estimate it by the sample mean, \\[\\begin{align} \\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^T x_t \\end{align}\\] What is the variance of the mean in a stationary time series context? 2.7.2 Autocovariance The sample autocovariance, \\(\\hat{\\gamma}\\), for a time series \\(x\\) at lag \\(k\\) is: \\[\\begin{align} \\hat{\\gamma}_k &amp;= \\text{cov}(x_t, x_{t-k}) \\\\ &amp;= \\frac{1}{T} \\sum_{t = k + 1}^{T} (x_t - \\bar{x})(x_{t-k} - \\bar{x}) \\end{align}\\] 2.7.3 Autocorrelation The sample autocorrelation function for lag \\(k\\), \\(\\hat{\\rho}_k\\), is simply the lag \\(k\\) autocovariance, \\(\\hat{\\gamma_k}\\), scaled by the sample variance. \\[\\begin{align} \\hat{\\rho}_k &amp;= \\frac{ \\hat{\\gamma_k} }{\\hat{\\sigma}_{y_t} \\hat{\\sigma}_{y_{t-k}}} \\\\ &amp;= \\frac{ \\hat{\\gamma_k} }{\\hat{\\gamma_0}}. \\end{align}\\] 2.8 Lab 1 Join the Kayak visits data (see below) to the conversions data from Time Series EDA. Use the inner_join function with the argument, by = c(“datestamp”, “country_code”, “marketing_channel”) and then filter to only US observations. Make a plot of user_visits and conversions by day. Standardize them if it makes sense. Then, fit a linear regression model with conversions as the response variable and user_visits as the explanatory variable. What is the estimated line equation? Finally, make a plot of the residuals from this model. Do the residuals validate standard linear regression model assumptions? visits = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/visits.csv&quot;) visits$datestamp = ymd(visits$datestamp) df = inner_join(visits, conversions, by = c(&quot;datestamp&quot;, &quot;country_code&quot;, &quot;marketing_channel&quot;)) %&gt;% filter(country_code == &quot;us&quot;) %&gt;% drop_na() %&gt;% group_by(datestamp) %&gt;% summarise(visits = sum(user_visits), conv = sum(conversions)) conv_lm = lm(conv ~ visits, data = df) # conversions = 2752.13 + .31 * visits df %&gt;% mutate(std_visits = (visits - mean(visits))/sd(visits), std_conv = (conv - mean(conv))/sd(conv)) %&gt;% select(datestamp, std_visits, std_conv) %&gt;% pivot_longer(-datestamp) %&gt;% ggplot(aes(datestamp, value)) + geom_line(aes(color = name)) + theme_minimal() + ggtitle(&quot;Standardized Visits and Conversions&quot;) Smoothing a time series offers a way of visualizing the underlying trend of a process, which can often be obscured by seasonal as well as random fluctuations. One relatively simple method of smoothing a time series is by computing a moving average. For a time series \\(y_t\\), \\(t = 1, \\ldots, T\\), a moving average of order \\(m\\) can be written, \\[\\begin{align} \\hat{y_t} = \\frac{1}{m} \\sum_{j=-k}^{k} y_{t+j}, \\end{align}\\] where \\(m=2k+1\\). The concept behind this technique is that observations that are close in time are likely to be close in value. Compute a moving average of order \\(m=13\\) for the fma::elec dataframe used in Classical Regression. Plot the smoothed series along with the raw data in a single plot. k = 6 elec_T = stats::filter(fma::elec, filter = rep(1/13, 13), sides = 2) Time series data can exhibit a variety of patterns, and it is often helpful to decompose a time series into components, each representing an underlying pattern category. In this question, you will decompose a time series into three parts: a trend component (\\(T\\)), a seasonality component (\\(S\\)), and a random component (\\(R\\)). That is, for each observation \\(Y_t\\), we want to break it down into three parts: \\[Y_T = T_t + S_t + R_t\\] You will again use the fma::elec dataframe and proceed in 3 steps: Step 1: Compute the trend component using an order 13 moving average (good news: you did exactly this in the last question, so use the output from the prior question as the trend component, \\(\\hat{T}\\)). Step 2: Compute the detrended series: \\(Y-\\hat{T}\\) Step 3: To estimate the seasonal component for each month, simply average the detrended values for each month. For example, the seasonal component for March is the average of all the detrended March values in the data. Then adjust the seasonal component values to ensure that they add to zero. The seasonal component is obtained by stringing together these monthly values, and then replicating the sequence for each year of data. This gives \\(\\hat{S}\\). Step 4: The remainder component is calculated by subtracting the estimated seasonal and trend-cycle components. That is \\(\\hat{R} = Y - \\hat{T} - \\hat{S}\\). Provide your code that is clearly commented for each step as well as a plot that shows each of the three component series in three separate plots (hint: use par(mfrow = c(1,3)) to make three plots in a row). detrend = fma::elec - elec_T monthly_means = tapply(detrend, cycle(detrend), function(x) mean(x, na.rm = T)) elec_S = ts ( rep(monthly_means, length.out = length(fma::elec)), start = c(1956,1), frequency = 12) elec_R = ts ( detrend - elec_S, start = c(1956,1), frequency = 12) par(mfrow = c(1,3)) plot(elec_T, main = &quot;Trend&quot;) plot(elec_S, ylim = c(-1000,1000), main = &quot;Seasonality&quot;) plot(elec_R, ylim = c(-1000,1000), main = &quot;Random&quot;) # tseries::na.remove(ts) to look at acf plots for time series with na values See chapter 1 of TSA (page 30) for a definition of cross correlation. This is a measure of the linear dependence of one time series on another. In other words, this is a multivariate version of the autocorrelation. Compute the cross correlation between GDP per capita and the Case Shiller Home Price Index (both from the FRED database), using the following data: fred = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/fred_dat.csv&quot;)[,-1] fred$date = ymd(fred$date) Provide your code that computes the ccf for lag 0 and 1:10 in both directions (i.e., one where GDP/capita leads and the other in which the Case Shiller leads). Validate your calculations using the ccf function. From the text, we aim to compute: \\[\\begin{align} \\hat{\\gamma}_{xy}(h) = n^{-1} \\sum_{t=1}^{n-h} (x_{t+h}-\\bar{x})(y_t - \\bar{y}) \\end{align}\\] x = fred$gdp y = fred$shiller xbar = mean(x) ybar = mean(y) n = length(x) den = sqrt( sum( (x - xbar) ^ 2 ) * sum( (y - ybar) ^2 )) h = 2 # compute ccf for a given value of h my_ccf = sapply( -10:10 , function(hi) { if(hi &lt; 0) { x = lag(x, abs(hi)) } else if(hi &gt; 0) { y = lag(y, hi) } sum( (x - xbar) * (y - ybar) , na.rm = T ) / den }) names(my_ccf) = -10:10 print(round(my_ccf, 3)) ## -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 ## 0.460 0.502 0.554 0.617 0.668 0.717 0.771 0.821 0.864 0.906 0.951 0.884 0.817 ## 3 4 5 6 7 8 9 10 ## 0.755 0.687 0.609 0.528 0.457 0.393 0.333 0.279 (ccf(x,y, type = &quot;correlation&quot;, lag.max = 10)) ## ## Autocorrelations of series &#39;X&#39;, by lag ## ## -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 ## 0.460 0.502 0.554 0.617 0.668 0.717 0.771 0.821 0.864 0.906 0.951 0.884 0.817 ## 3 4 5 6 7 8 9 10 ## 0.755 0.687 0.609 0.528 0.457 0.393 0.333 0.279 Write a function in R that takes two arguments: a time series (\\(y\\)) vector and a vector (\\(k\\)) specifying a set of lags. The output for this function is the autocorrelation of \\(y\\) for each lag in \\(k\\), using the formula in Autocorrelation. Compare the output of your function to the output from (acf(y)). # simulated data to test function y = arima.sim(n = 10, model = list(ar = c(.1,.2))) acf_fun = function(y, k) { mean_y = mean(y) gamma_0 = sum((y - mean_y) ^ 2) # compute autocovariance for each lag in k sapply(k, function(ki) { v1 = as.numeric(window(y, end = length(y) - ki)) # ends &#39;early&#39; v2 = as.numeric(window(y, start = ki + 1)) # starts &#39;late&#39; gamma_k = sum((v1 - mean_y) * (v2 - mean_y)) return(gamma_k / gamma_0) }) } print ( acf_fun(y,c(1:5)) ) ## [1] -0.02955049 0.07644815 -0.10432208 0.22030633 -0.27448442 print ( (acf(y, plot = F))$acf[2:6] ) ## [1] -0.02955049 0.07644815 -0.10432208 0.22030633 -0.27448442 Simulate data from the following model: \\(x_t = a + \\phi x_{t-1} + w_t\\), in which \\(w_t \\sim N(0,\\sigma^2)\\). You can choose the value for parameters: \\(\\phi, \\sigma^2, a\\). Report which values you chose and make a plot of the theoretical versus observed autocorrelation function for a simulation with length \\(T=100\\) and a simulation with length \\(T=10,000\\). "],["arma.html", "Set 3 ARMA 3.1 Operators 3.2 Autoregressive (AR) models 3.3 Moving Average (MA) models 3.4 PACF 3.5 AR Estimation 3.6 ARMA models 3.7 Lab 2", " Set 3 ARMA 3.1 Operators 3.1.1 Backshift operator The backshift shift operator (\\(\\mathbf{B}\\)) is an important function in time series analysis, which is defined as \\[ \\mathbf{B} x_t = x_{t-1} \\] or more generally as \\[ \\mathbf{B}^k x_t = x_{t-k} \\] For example, express a random walk, \\(x_t = x_{t-1} + w_t\\), using \\(\\mathbf{B}\\). 3.1.2 The difference operator The difference operator (\\(\\nabla\\)) is another important function in time series analysis, which we define as \\[ \\nabla x_t = x_t - x_{t-1} \\] For example, what does first-differencing a random walk yield? The difference operator and the backshift operator are related \\[ \\nabla^k = (1 - \\mathbf{B})^k \\] Differencing is a simple means for removing a trend The 1st-difference removes a linear trend A 2nd-difference will remove a quadratic trend 3.2 Autoregressive (AR) models An autoregressive model of order p, or AR(p), is defined as \\[x_t = \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + \\dots + \\phi_p x_{t-p} + w_t\\] where we assume \\(w_t\\) is white noise \\(\\phi_p \\neq 0\\) for an order-p process 3.2.1 AR(1) Model Let’s start by figuring out some properties of the simplest AR model, the AR(1) model: \\[x_t = \\phi_0 + \\phi_1 x_{t-1} + w_t\\] We start by assuming that \\(x_t\\) is a stationary time series. Under this assumption, we can show: \\[\\begin{align} E(x_t) &amp;= \\frac{\\phi_0}{1-\\phi_1} \\\\ Var(x_t) &amp;= \\frac{\\sigma^2_w}{1-\\phi_1^2} \\\\ \\rho(h) &amp;= \\phi_1^h \\end{align}\\] For this to work, \\(|\\phi_1| &lt; 1\\). 3.2.2 AR Stationarity We seek a means for identifying whether our AR(p) models are also stationary. We can write out an AR(p) model using the backshift operator: \\[ x_t = \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + \\dots + \\phi_p x_{t-p} + w_t \\\\ \\Downarrow \\\\ \\begin{align} x_t - \\phi_1 x_{t-1} - \\phi_2 x_{t-2} - \\dots - \\phi_p x_{t-p} &amp;= w_t \\\\ (1 - \\phi_1 \\mathbf{B} - \\phi_2 \\mathbf{B}^2 - \\dots - \\phi_p \\mathbf{B}^p) x_t &amp;= w_t \\\\ \\phi_p (\\mathbf{B}^p) x_t &amp;= w_t \\\\ \\end{align} \\] If we treat \\(\\mathbf{B}\\) as a number (or numbers), we can out write the characteristic equation as \\(\\phi_p (\\mathbf{B}^p)\\). To be stationary, all roots of the characteristic equation must exceed 1 in absolute value As a bonus, when this condition is met, then the model is also causal. Example, for what value of \\(\\phi_1\\) is AR(1) model stationary? Are the following AR processes stationary? \\(x_t = 0.5 x_{t-1} + w_t\\) \\(x_t = -0.2 x_{t-1} + 0.4 x_{t-2} + w_t\\) \\(x_t = x_{t-1} + w_t\\) 3.2.3 Autocorrelation The exponential decay observed in autocorrelation function for the the AR(1) model holds in general for AR(p). This decay may oscillate, as shown below. 3.3 Moving Average (MA) models A moving average model of order q, or MA(q), is defined as \\[ x_t = w_t + \\theta_1 w_{t-1} + \\theta_2 w_{t-2} + \\dots + \\theta_q w_{t-q}\\] where \\(w_t\\) is white noise Each of the \\(x_t\\) is a sum of the most recent error terms Thus, all MA processes are stationary because they are finite sums of stationary WN processes 3.3.1 Examples of MA(q) models 3.3.2 MA(1) Model Let’s start by figuring out some properties of the simplest MA model, the MA(1) model: \\[ x_t = \\theta_0 + \\theta_1 w_{t-1} + w_t \\] We start by assuming that \\(x_t\\) is a stationary time series. Under this assumption, we can show: \\[\\begin{align} E(x_t) &amp;= \\theta_0 \\\\ Var(x_t) &amp;= \\sigma^2_w(1+\\theta_1^2) \\\\ \\rho(h) &amp;= \\frac{\\theta_1}{1+\\theta_1^2} \\text{ for } h=1 \\text{ and 0 otherwise. } \\end{align}\\] 3.3.3 Invertibility For MA models, we need invertibility in order to identify model paramters. An MA(q) process is invertible if it can be written as a stationary autoregressive process of infinite order without an error term \\[ x_t = w_t + \\theta_1 w_{t-1} + \\theta_2 w_{t-2} + \\dots + \\theta_q w_{t-q} \\\\ \\Downarrow ? \\\\ w_t = x_t + \\sum_{k=1}^\\infty(-\\theta)^k x_{t-k} \\] For example, these MA(1) models are equivalent \\[ x_t = w_t + \\frac{1}{5} w_{t-1} ~\\text{with} ~w_t \\sim ~\\text{N}(0,25) \\\\ \\Updownarrow \\\\ x_t = w_t + 5 w_{t-1} ~\\text{with} ~w_t \\sim ~\\text{N}(0,1) \\] Rewrite an MA(1) model in terms of \\(w_t\\) \\[ x_t = w_t + \\theta w_{t-1} \\\\ \\Downarrow \\\\ w_t = x_t - \\theta w_{t-1} \\\\ \\] If we constrain \\(\\lvert \\theta \\rvert &lt; 1\\), then \\[ \\lim_{k \\to \\infty} (-\\theta)^{k+1} w_{t-k-1} = 0 \\] and \\[ \\begin{align} w_t &amp;= x_t - \\theta x_{t-1} - \\dots -\\theta^k x_{t-k} -\\theta^{k+1} w_{t-k-1} \\\\ w_t &amp;= x_t - \\theta x_{t-1} - \\dots -\\theta^k x_{t-k} \\\\ w_t &amp;= x_t + \\sum_{k=1}^\\infty(-\\theta)^k x_{t-k} \\end{align} \\] 3.3.4 Autocorrelation For the MA(q) model, the autocovariance function cuts off for \\(h &gt;q\\). That is, \\[\\begin{align} \\gamma(h) &amp;= \\sigma^2 \\sum_{j=0}^{q-h} \\theta_j \\theta_{j+h}~\\text{for }h=0,\\ldots,q \\\\ \\gamma(h) &amp;= 0~h&gt;q \\end{align}\\] Therefore, the sample ACF is useful for model identification when our data comes from a moving average process, but not so useful for data that comes from an AR process. For this, we introduce the partial autocorrelation (PACF). 3.4 PACF 3.4.1 Definition The partial autocorrelation of a stationary process, \\(x_t\\), denoted \\(\\phi_{hh}\\), for \\(h=1,2,\\ldots\\), is \\[\\phi_{11}=\\text{cor}(x_{t}, x_{t-1}) = \\rho_1\\] and, \\[\\phi_{hh}=\\text{cor}(x_{t}-\\hat{x}_t, x_{t-h} - \\hat{x}_{t-h}),~~h \\geq 2\\]. The PACF, \\(\\phi_{hh}\\), is the correlation between \\(x_t\\) and \\(x_{t-h}\\) with the linear dependence of \\(\\{x_{t-1}, \\ldots, x_{t-h+1} \\}\\) on each, removed. 3.4.2 AR(1) PACF Let us calculate \\(\\phi_{22}\\) for the AR(1) model, \\(x_t = \\phi x_{t-1} + w_t\\). Consider the regression of \\(x_t\\) on \\(x_{t-1}\\). Choose \\(\\beta\\) to minimize, \\[\\begin{align} E(x_t - \\beta x_{t-1})^2 &amp;= \\gamma(0) - 2\\beta \\gamma(1) + \\beta^2 \\gamma(0) \\end{align}\\] Minimize the expression above to find the estimator of \\(\\beta\\). Plug in this estimated quantity to \\(\\text{cor}(x_{t}-\\hat{x}_{t-1}, x_{t-2} - \\hat{x}_{t-2})\\) 3.4.3 PACF for AR(p) For an AR(p) model, when \\(h &gt; p\\), \\(\\phi_{hh} = 0\\). Thus, the PACF is very informative for model identification when data comes from an autoregressive process. 3.5 AR Estimation 3.5.1 Yule-Walker Equations Consider the AR(p) model, \\(x_t = \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + \\phi_3 x_{t-3} + w_t\\) Now, take the following steps, Multiply both sides by \\(x_{t-1}\\) Take an expectation This leads to, \\[ \\left[ \\begin{array}{c} \\gamma(1) \\\\ \\vdots \\\\ \\gamma(p) \\end{array} \\right] = \\begin{pmatrix} \\gamma(0) &amp; \\ldots &amp; \\gamma(p-1) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\gamma(p-1) &amp; \\ldots &amp; \\gamma(0) \\end{pmatrix} \\times \\left[ \\begin{array}{c} \\phi_1 \\\\ \\vdots \\\\ \\phi_p \\end{array} \\right] \\] Or, succinctly, \\[\\mathbf{\\gamma} = \\mathbf{\\Gamma} \\mathbf{\\phi}\\] Which can be solved as, \\[\\hat{ \\mathbf{\\phi} } = \\mathbf{\\Gamma} ^ {-1}\\mathbf{\\gamma} \\] The remaining parameter to be estimated is \\(\\sigma^2\\), the variance of the white noise term. An estimator can be established by multiplying the model by \\(x_t\\), which leads to \\[\\hat{\\sigma}^2 = \\hat{\\gamma}(0) - \\sum_{i=1}^{p}\\hat{\\phi}_i \\hat{\\gamma}(i)\\] 3.5.2 Example Suppose you aim to estimate \\(\\mathbf{\\phi}\\) for a dataset in which \\(\\hat{\\mathbf{\\gamma}} = (3.8,3.1,3,2.8)\\). What is the Yule-Walker estimate for \\(\\mathbf{\\phi}\\) and also for \\(\\sigma\\)? 3.6 ARMA models An autoregressive moving average, or ARMA(p,q), model is written as \\[ x_t = \\phi_1 x_{t-1} + \\dots + \\phi_p x_{t-p} + w_t + \\theta_1 w_{t-1} + \\dots + \\theta_q w_{t-q} \\] We can write an ARMA(p,q) model using the backshift operator \\[ \\phi_p (\\mathbf{B}^p) x_t= \\theta_q (\\mathbf{B}^q) w_t \\] ARMA models are stationary if all roots of \\(\\phi_p (\\mathbf{B}) &gt; 1\\) ARMA models are invertible if all roots of \\(\\theta_q (\\mathbf{B}) &gt; 1\\) 3.6.1 \\(\\psi\\) representation For a causal ARMA\\((p,q)\\) model, \\(\\phi_p (\\mathbf{B}^p) x_t= \\theta_q (\\mathbf{B}^q) w_t\\), we may write \\[\\begin{align} x_t = \\sum_{j=0}^\\infty \\psi_j w_{t-j}. \\end{align}\\] Solving for the \\(\\psi\\) weights in general is complicated and can be solved using the ARMAtoMA function in R. 3.6.2 ARMA(1,1) Solve for the \\(\\psi\\) wieghts in the case of a causal, invertible ARMA(1,1) process. 3.6.3 Forecasting To forecast with an ARMA model, we simply use the parameter estimates for future periods. For example, consider the following dataset of 500 observations simulated from an ARMA(1,1) process. set.seed(1) ex_ts_arma = arima.sim(n = 500, list(ar = c(.8), ma = c(-.3))) arma_fit = Arima(ex_ts_arma, order = c(1,0,1), include.mean = F) The fitted model is \\(x_t = 0.7732447 x_{t-1} -0.2942922 w_{t-1}\\). To forecast two observations in the future, we simply use the parameter estimates as follows: w_vec = abs( fitted(arma_fit) - ex_ts_arma) h1 = coefficients(arma_fit)[1] * ex_ts_arma[500] + coefficients(arma_fit)[2] * w_vec[500] # 0.2161869 h2 = coefficients(arma_fit)[1] * h1 Our one step ahead forecast is 0.2161869 and the two step ahead forecast is 0.1671654. The same values can be computed using forecast(arma_fit, h=2)$mean. 3.6.4 Forecast Errors In order to compute forecast errors, we will make use of the following two parameters: \\[\\begin{align} x_{T+h} &amp;= \\sum_{j=0}^\\infty \\psi_j w_{T+h-j} \\\\ \\tilde{x}_{T+h} &amp;= E( x_{T+h} | x_1, \\ldots, x_T ) \\end{align}\\] Since \\(E(w_t | x_1, \\ldots, x_T) = w_t\\) for \\(t \\leq T\\) and 0 for \\(t &gt; T\\), \\[\\begin{align} x_{T+h} - \\tilde{x}_{T+h} &amp;= \\sum_{j=0}^{h-1} \\psi_j w_{T+h-j} \\\\ \\end{align}\\] Therefore, the variance of this quantity is: \\[\\begin{align} \\sigma^2 \\sum_{j=0}^{h-1} \\psi_j ^2 \\\\ \\end{align}\\] With the assumption of normally distributed errors, a 95% prediction interval for \\(x_{T+h}\\) is, \\[\\begin{align} \\hat{x}_{T+h} \\pm 1.96 \\sqrt{ \\hat{\\sigma}^2_w \\sum_{j=0}^{h-1} \\psi_j^2 } \\end{align}\\] psis = ARMAtoMA(ar = coefficients(arma_fit)[1], ma = coefficients(arma_fit)[2], lag.max = 2) # confidence interval for one step ahead forecast c(h1 - qnorm(.975) * sqrt( arma_fit$sigma2 ) , h1 + qnorm(.975) * sqrt ( arma_fit$sigma2 )) # (-1.8, 2.2) # confidence interval for 2 step ahead forecast h2_se = sqrt( arma_fit$sigma2 * (1 + psis[1]^2 ) ) c(h2 - qnorm(.975) * h2_se, h2 + qnorm(.975) * h2_se) # (-2.0, 2.4) Or, use forecast(arma_fit, h = 2) 3.7 Lab 2 Simulate data from the MA(1) model: \\(x_t = w_t+\\theta_1w_{t-1}\\) in which \\(\\theta = (0.7)\\). For sample sizes \\(n=10,10^3,10^5\\), plot the difference between the theoretical autocorrelation and the observed (sample) autocorrelation for lags 1 through 5. What do you observe? Do the same thing for an MA(3) model. Include which values you use for \\(\\theta_1, \\theta_2, \\theta_3\\). See page 96 of the textbook for the theoretical AFC function for the MA(3) model. set.seed(1) # MA1 theta = 0.7 sim1 = arima.sim(model = list(ma=c(theta)), n = 10) sim2 = arima.sim(model = list(ma=c(theta)), n = 10^3) sim3 = arima.sim(model = list(ma=c(theta)), n = 10^5) acf_theoretical = c( theta/ (1 + theta^2) , rep(0,4)) simlist = list(sim1, sim2, sim3) acf_sample = lapply(simlist, function(x) abs ( (acf(x, plot = F))$acf[2:6] - acf_theoretical ) ) plot(x = 1:5, y = acf_sample[[1]], type = &quot;l&quot;, col = &quot;red&quot;, ylim = c(0,1), ylab = &quot;Empirical-Theoretical ACF&quot;, main = &quot;MA1&quot;, sub = &quot;Small sample in red&quot;, xlab = &quot;lags&quot;) lines(acf_sample[[2]], col = &quot;green&quot;) lines(acf_sample[[3]], col = &quot;blue&quot;) # MA3 theta = c(0.8, 0.7, 0.4) den = 1 + sum(theta^2) acf_theoretical = c( (theta[1] + theta[2] * theta[2] + theta[2] * theta[3])/den, (theta[2] + theta[1] * theta[3])/den, theta[3]/den, rep(0,2)) sim1 = arima.sim(model = list(ma= theta), n = 10) sim2 = arima.sim(model = list(ma= theta), n = 10^3) sim3 = arima.sim(model = list(ma= theta), n = 10^5) simlist = list(sim1, sim2, sim3) acf_sample = lapply(simlist, function(x) abs ( (acf(x, plot = F))$acf[2:6] - acf_theoretical ) ) plot(x = 1:5, y = acf_sample[[1]], type = &quot;l&quot;, col = &quot;red&quot;, ylim = c(0,1), ylab = &quot;Empirical-Theoretical ACF&quot;, main = &quot;MA3&quot;, xlab = &quot;lags&quot;, sub = &quot;Small sample in red&quot;) lines(acf_sample[[2]], col = &quot;green&quot;) lines(acf_sample[[3]], col = &quot;blue&quot;) From page 96, for \\(1 \\leq h \\leq q\\) \\[\\begin{align} \\rho(h) = \\frac{\\sigma^2_w \\sum_{j=0}^{q-h} \\theta_j \\theta_{j+h}}{1+\\theta_1^2 + \\ldots + \\theta_q^2}, \\end{align}\\] and \\(\\rho(h) = 0\\) for \\(h &gt; q\\). For an AR(1), derive an expression for \\(\\phi_{22}\\) (the lag 2 partial ACF). Follow the steps in AR(1) PACF. See also page 100 in the textbook. Using ACF &amp; PACF for model ID. Complete the table with either cuts off after \\(\\ldots\\) or tails off slowly. Back up the 3 entries you fill in with either math or with empirical data (simulate data from the model type and show the relevant estimate quantities). Model ACF PACF AR(p) Tails off slowly Cuts off after lag p MA(q) Cuts off after lag q Tails off slowly ARMA(p,q) Tails off slowly Tails off slowly Suppose you aim to estimate \\(\\mathbf{\\phi}\\) for an AR(2) model in which \\(\\hat{\\mathbf{\\gamma}} = (2.1,1.5,1.3)\\) (i.e., \\(\\hat{\\gamma}(0)=2.1)\\). What is the Yule-Walker estimate for \\(\\mathbf{\\phi}\\) and also for \\(\\sigma\\)? See AR Estimation for details. \\[ \\left[ \\begin{array}{c} 1.5 \\\\ 1.3 \\end{array} \\right] = \\begin{pmatrix} 2.1 &amp; 1.5 \\\\ 1.5 &amp; 2.1 \\end{pmatrix} \\times \\left[ \\begin{array}{c} \\phi_1 \\\\ \\phi_2 \\end{array} \\right] \\] Gamma_mat = matrix(c(2.1,1.5,1.5,2.1), nrow = 2) phi_hat = solve(Gamma_mat) %*% c(1.5, 1.3) # 0.556, 0.222 sigma_hat = 2.1 - sum(phi_hat * c(1.5, 1.3)) # 0.98 Using the Egyptian export data from class, compute the 95% interval for the one period and 2 period ahead forecasts. Use the \\(\\psi\\) representation from Forecast Errors. You can find the \\(\\phi\\) values using ARMAtoMA. Validate your answers using forecast::forecast. \\[\\begin{align} \\hat{x}_{T+h} \\pm 1.96 \\sqrt{ \\hat{\\sigma}^2_w \\sum_{j=0}^{h-1} \\psi_j^2 } \\end{align}\\] library(fpp3) exp = global_economy %&gt;% filter(Code == &quot;EGY&quot;) %&gt;% select(Exports) m0 = auto.arima(exp$Exports) psi = ARMAtoMA(ar = coefficients(m0)[1:2], ma = coefficients(m0)[3], lag.max = 1 ) # forecast(m0, h = 2) # 12.4 - 18.0 - 23.6 c( 18.00745 - 1.96 * sqrt( m0$sigma2 ) , 18.00745 + 1.96 * sqrt( m0$sigma2 ) ) # h = 1 ## [1] 12.44787 23.56703 c( 20.04187 - 1.96 * sqrt( m0$sigma2 * (1+psi^2) ) , 20.04187 + 1.96 * sqrt( m0$sigma2 * (1+psi^2) ) ) # h = 2 ## [1] 12.23114 27.85260 "],["arima.html", "Set 4 ARIMA 4.1 Stationarity 4.2 Differencing \\(\\nabla\\) 4.3 ARIMA Models 4.4 SARIMA 4.5 Lab 3", " Set 4 ARIMA 4.1 Stationarity The ARMA model we have seen assumes stationary data. Stationarity means ‘not changing in time’ in the context of time-series models. In a typical data analysis, however, we will not be assured that our data is stationary. Therefore, we need some methods to evaluate stationarity and also to deal with nonstationary data. We will discuss 2 common approaches to evaluating stationarity: Visual test Unit Root Test (ADF) 4.1.1 Visual Test The visual test is simply looking at a plot of the data versus time. Look for: Change in the level over time. Is the time series increasing or decreasing? Does it appear to cycle? Change in the variance over time. Do deviations away from the mean change over time, increase or decrease? 4.1.2 Unit Root Tests One of the most common forms of non-stationarity that is tested for is that the process has a random walk component, such as \\(x_t = x_{t-1} + e_t\\). A random walk is called a ‘unit root’ process in the time series literature since it occurs when one of the roots of the AR polynomial is 1. A test for an underlying random walk is called a ‘unit root’ test. 4.1.3 Dickey Fuller Test (DF) The DF tests for a unit root in the context of an AR(1) model, which can be written, \\[\\begin{align} x_t &amp;= \\phi_1 x_{t-1} + w_t \\\\ \\nabla x_t &amp;= (\\phi_1 - 1) x_{t-1} + w_t \\\\ &amp;= \\delta x_{t-1} + w_t \\end{align}\\] This model can be estimated, and testing for a unit root is equivalent to testing: \\[\\begin{align} H_0&amp;: \\delta = 0 \\\\ H_A&amp;: \\delta \\neq 0 \\end{align}\\] The test statistic has a specific distribution simply known as the Dickey–Fuller table, which is used to find the p-value. We want to reject the null hypothesis (small p-value) of non-stationarity. 4.1.4 Augmented DF Test The Augmented Dickey-Fuller Test (ADF) looks for evidence that an AR(p) process has a unit root (an underlying random walk process). The null hypothesis is that the time series has a unit root, that is, it has a random walk component. The alternative hypothesis is some variation of stationarity. The first difference, \\(\\nabla\\), of the general AR(p) model can be re-written as, \\[\\begin{align} \\nabla x_t = \\phi_1^\\prime x_{t-1} + \\sum_{i=1}^{p-1} \\phi_{i+1}^\\prime \\nabla x_{t-1} + w_t, \\end{align}\\] where \\[\\begin{align} \\phi_1^\\prime &amp;= \\sum_{j=1}^{p} \\phi_{j}-1 \\\\ \\phi_j^\\prime &amp;= \\sum_{j=1}^{p} \\phi_{j}~\\text{ for} j \\geq 2 \\end{align}\\] Our interest is on \\(\\phi_1^\\prime\\) since it is equal to 0 exactly when 1 is a root of the AR(p) polynomial. Therefore, the hypothesis test is, \\[\\begin{align} H_0&amp;: \\phi_1^\\prime = 0 \\\\ H_A&amp;: \\phi_1^\\prime \\neq 0 \\end{align}\\] 4.1.5 ADF: tseries::adf.test() adf.test() in the tseries package will apply the Augmented Dickey-Fuller with a constant and trend and report the p-value. We want to reject the Dickey=Fuller null hypothesis of non-stationarity. When k=0, the Dickey-Fuller test asseses AR(1) stationarity. The Augmented Dickey-Fuller tests for more general lag-p stationarity. adf.test(x, alternative = c(&quot;stationary&quot;, &quot;explosive&quot;), k = trunc((length(x)-1)^(1/3))) In this lecture we will use differencing, the I in ARIMA model refers to differencing. 4.2 Differencing \\(\\nabla\\) Differencing means to create a new time series \\(z_t = x_t - x_{t-1}\\). The I in ARIMA model refers to differencing. First order differencing means you do this once (so \\(z_t\\)) and second order differencing means you do this twice (so \\(z_t - z_{t-1}\\)). The diff() function takes the first difference: x &lt;- diff(c(1,2,4,7,11)) x ## [1] 1 2 3 4 The second difference is the first difference of the first difference. diff(x) ## [1] 1 1 1 4.3 ARIMA Models Our data is not always stationary. If the data do not appear stationary, differencing can help. This leads to the class of autoregressive integrated moving average (ARIMA) models. ARIMA models are indexed with orders (p,d,q) where d indicates the order of differencing. \\(\\{x_t\\}\\) follows an ARIMA(p,d,q) process if \\((1-\\mathbf{B})^d x_t\\) is an ARMA(p,q) process. In this context, \\(p\\) refers to the order of the autoregressive part of the model, \\(d\\) refers to the degree of differencing required, and \\(q\\) refers to the order of the moving average component. 4.3.1 ARIMA(1,1,1) For example, consider an ARIMA(1,1,1). What is the response variable of the model? Write out the model equation. What would you expect the ACF and PACF plots to look like? 4.3.2 Real Data Example Let us look at some real data - exports of the Central African Republic from 1960-2016. In this time series, we see a clear evolution in the mean of the time series, indicating that the time series is not stationary. caf = tsibbledata::global_economy %&gt;% filter(Code == &quot;CAF&quot;) %&gt;% mutate(first_diff = c(NA, diff(Exports))) To address the non-stationarity, we will take a first difference of the data. The differenced data are shown below. The DF test corresponds with a visual assessment - the first difference appears to be stationary. 4.3.3 Model Selection/ Fitting To assess what kind of model to fit to the data, we can look at the ACF and PACF plots of the differenced data and diagnose some candidate models. For example: The PACF above is suggestive of an AR(2) model; so an initial candidate model is an ARIMA(2,1,0). The ACF suggests an MA(3) model; so an alternative candidate is an ARIMA(0,1,3). We can fit both of these models and find which one has a better AIC score. Arima(caf$Exports, order = c(2,1,0)) # -134.27 / 274.54 Arima(caf$Exports, order = c(0,1,3)) # - 133.12 / 274.25 Since the AIC is smaller for the ARIMA(0,1,3) model, this is a good candidate. We could also search a larger space of models with the help of the auto.arima function. 4.3.4 Model Selection - automated The general sequence of steps involved in fitting an ARIMA model to a given time series are: Evaluate whether the time series is stationary If not, make it stationary - select the differencing level (d) Select the AR level (p) and the MA level (q) that optimize the AIC Steps two and three are automated with the function forecast::auto.arima function. For instance, m_auto = auto.arima(caf$Exports) The model selected by this stepwise procedure is an ARIMA(2,1,2), whose AIC is 274.2, a small improvement over the AIC value of the ARIMA(0,1,3), which was selected by visual inspection of the ACF/ PACF plots. 4.3.5 Model Checking 4.3.5.1 Check the residuals Residuals = difference between the observations (data), \\(y\\), and expected (fitted) values, \\(\\hat{y}\\). Thus, the i’th residual is: \\(y_i-\\hat{y}_i\\). In R, we can obtain the vector of residuals using the residuals function. What do we aim to see in these plots? Do we? We could alternatively use a hypothesis test (Box-Pierce test) in which: \\[\\begin{align} H_0 &amp;: \\rho_1 = \\rho_2 = \\ldots = \\rho_k = 0 \\\\ H_A &amp;: \\text{at least one autocorrelation is different from 0} \\end{align}\\] The test statistic (the Ljung-Box Q-Statistic) for this test: \\[\\begin{align} Q = n(n+2) \\sum_{i=1}^k \\frac{\\hat{\\rho}_i^2}{(n-k)} \\end{align}\\] follows a chi-squared distribution with \\(k\\) degrees of freedom under the null hypothesis. 4.3.6 Forecasting 4.3.6.1 Point Estimates The basic idea of forecasting with an ARIMA model is to estimate the parameters and forecast forward. For example, let’s say we want to forecast Central African Exports with the ARIMA(2,1,0) model with drift: \\[z_t = \\mu + \\phi_1 z_{t-1} + \\phi_2 z_{t-2} + w_t\\] where \\(z_t = x_t - x_{t-1}\\), the first difference. Arima() would write this model: \\[(z_t-m) = \\phi_1 (z_{t-1}-m) + \\phi_2 (z_{t-2}-m) + e_t\\] The relationship between \\(\\mu\\) and \\(m\\) is \\(\\mu = m(1 - \\phi_1 - \\phi_2)\\). Let’s estimate the \\(\\phi\\)’s for this model from the Central African export data. ## ar1 ar2 drift ## -0.5230284 -0.3065268 -0.2119722 mu &lt;- coef(fit)[3]*(1-coef(fit)[1]-coef(fit)[2]) So, \\(\\mu=\\) -0.39 So we can forecast with this model: \\(z_t =\\) -0.39 + -0.52 \\(z_{t-1}\\) -0.31\\(z_{t-2}\\) + \\(w_t\\) To obtain the \\(T+1\\) forecast value: zt_1 = 12.51809 - 12.72904 zt_2 = 12.72904 - 12.61192 12.51809 + (-0.3878149 -0.5230284 * zt_1 - 0.3065268 * zt_2) ## [1] 12.20471 You can also use the forecast function to obtain point estimates and prediction intervals. For example, fr = forecast(fit, h = 5) plot(fr) 4.4 SARIMA So far, we have restricted our attention to non-seasonal data and non-seasonal ARIMA models. However, ARIMA models are also capable of modelling a wide range of seasonal data. A seasonal ARIMA model is formed by including additional seasonal terms in the ARIMA models we have seen so far. It is written as follows: \\[ARIMA(p,d,q)~(P,D,Q)_m\\] where \\(m\\) is the seasonal period (e.g., number of observations per seasonal period). We use uppercase notation for the seasonal parts of the model, and lowercase notation for the non-seasonal parts of the model. The seasonal part of the model consists of terms that are similar to the non-seasonal components of the model, but involve backshifts of the seasonal period. The modelling procedure is almost the same as for non-seasonal data, except that we need to select seasonal AR and MA terms as well as the non-seasonal components of the model. Let us start by unpacking some simple SARIMA models. 4.4.1 Unpacking Notation Consider the SARIMA model, ARIMA\\((0,0,0)(0,0,1)_{12}\\) Write out the model equation What is the theoretical variance of the model? What is the theoretical autocorrelation function? What would you expect to see in the seasonal lags of the PACF? Consider the SARIMA model, ARIMA\\((0,0,0)(1,0,0)_{12}\\) Write out the model equation What is the theoretical variance of the model? What is the theoretical autocorrelation function? What would you expect to see in the seasonal lags of the PACF? 4.4.2 Data Example To see how this looks with real data, we will try to forecast monthly corticosteroid drug sales in Australia. These are known as H02 drugs under the Anatomical Therapeutic Chemical classification scheme. Their primary application is the treatment of allergic reactions, and are therefore highly seasonal. The dataset is tsibbledata::PBS. h02 &lt;- PBS %&gt;% filter(ATC2 == &quot;H02&quot;) %&gt;% summarise(Cost = sum(Cost)/1e6) %&gt;% mutate(lcost = log(Cost)) %&gt;% mutate(dlcost = c( rep(0,12),diff(lcost, 12))) The data are strongly seasonal and obviously non-stationary, so seasonal differencing will be used. The seasonally differenced data are shown below. The differenced data appear somewhat stationary and the null hypothesis is rejected in the ADF test, so our response variable will be: \\(z_t = x_t - x_{t-12}\\). Below we examine the ACF and PACF of \\(z_t\\). In the plots of the seasonally differenced data, there are spikes in the PACF at lags 12 and 24, but nothing at seasonal lags in the ACF. This may be suggestive of a seasonal AR(2) term. In the non-seasonal lags, there are three significant spikes in the PACF, suggesting a possible AR(3) term. The pattern in the ACF is not indicative of any simple model. This initial analysis suggests that a possible model for these data is an ARIMA\\((3,0,0)(2,1,0)_{12}\\). We fit this model below: Cost = ts(h02$Cost, frequency = 12, start = c(1991, 7)) m0 = arima(Cost, order = c(3,0,0), seasonal = c(2,1,0)) m0 %&gt;% broom::tidy() %&gt;% knitr::kable() term estimate std.error ar1 0.0985710 0.0701709 ar2 0.3980094 0.0622226 ar3 0.3897839 0.0724192 sar1 -0.4378022 0.0787779 sar2 -0.3047419 0.0782169 Let’s quickly examine the residuals from the model, And then we can produce forecasts for the next 12 months. 4.5 Lab 3 Consider fpp3::aus_airpassengers, the total number of passengers (in millions) from Australian air carriers for the period 1970-2011. Plot the data. Are the data stationary? If not, find an appropriate transformation which yields stationary data. Plot the ACF and PACF of the transformed time series. What are one or two candidate models based on these plots? Use forecast::auto.arima() to find an optimized ARIMA model. Use the original (i.e., nonstationary) time series as the input. What model was selected? Write the fit model equation. Check that the residuals look like white noise. forecast::checkresiduals Plot forecasts for the next 10 periods. forecast::forecast Plot forecasts from an ARIMA(0,1,0) model with drift and compare these to the automatically selected model. Choose an employment type from fpp3::us_employment, the total employment in different industries in the United States. Are the data stationary? If not, find an appropriate transformation which yields stationary data. Examine ACF and PACF plots of the transformed data (if this was necessary to attain stationarity) to identify a couple of ARIMA models that might be useful in describing the time series. Which of your models is the best according to their AIC values? Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better. Forecast the next 3 years of data. Get the latest figures from https://fred.stlouisfed.org/categories/11 to check the accuracy of your forecasts. Eventually, the prediction intervals are so wide that the forecasts are not particularly useful. How many years of forecasts do you think are sufficiently accurate to be usable? "],["review-mild-extensions.html", "Set 5 Review/ Mild Extensions 5.1 Linear Filtering 5.2 AR Parameter Constraints 5.3 MA Parameter Constraints 5.4 SARIMA models 5.5 Residuals check", " Set 5 Review/ Mild Extensions 5.1 Linear Filtering We saw a case of a linear filter with the moving average in Lab 1. A linear filter is a mathematical operation that transforms a time series into a new time series by applying a weighted sum of past and present values of the original series. Another popular linear filter, generally appropriate for a time series with no clear seasonality or trend, is with exponential smoothing. Like the moving average, this method averages over recent observations but differs in that it assigns relatively more weight to observations that are relatively closer. This idea can be expressed as, \\[\\begin{align} \\hat{y}_{t+1 | t} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t | t-1}, \\end{align}\\] where \\(\\hat{y}_{t+1 | t}\\) can be interpreted as the smoothed value of \\(y_{t+1}\\) given data up to time \\(t\\). So, for the first few smoothed values, we have: \\[\\begin{align} \\hat{y}_{1|0} &amp;= l_0 \\\\ \\hat{y}_{2|1} &amp;= \\alpha y_1 + (1-\\alpha) l_0 \\\\ \\hat{y}_{3|2} &amp;= \\alpha y_2 + (1-\\alpha) \\hat{y}_{2|1} \\\\ &amp;= \\alpha y_2 + (1-\\alpha) \\alpha y_1 + (1-\\alpha)^2 l_0 \\end{align}\\] Since we don’t have data prior to \\(y_1\\), we denote \\(\\hat{y}_{1 | 0} = l_0\\). Therefore, this model depends on two parameters, \\((l_0, \\alpha)\\). If we continue with the sequence above, each predicted value \\(\\hat{y_t}\\) can be expressed, \\[\\begin{align} \\hat{y}_{t+1 | t} = (1-\\alpha) ^ t l_0 + \\sum_{j=0}^{t-1} \\alpha (1-\\alpha) ^ j y_{t-j}. \\\\ \\end{align}\\] Let’s take a look at how this method depends on the parameters. We will use Algerian export data from tsibbledata, which you can grab here: alg = global_economy %&gt;% filter(Country == &quot;Algeria&quot;, Year &gt; 1990) y = alg$Exports es = function(par, ts) { # parameter vector: first parameter is alpha/ second is l0 yt = c(par[2]) for(j in 2:length(ts)) { yt[j] = par[1] * ts[j-1] + (1-par[1]) * yt[j-1] } return(yt) } Below, we compute the exponentially smoothed series using es(c(0.9, y[1]), y) as well as \\(\\alpha = 0.1\\). For any \\(\\alpha\\) between 0 and 1, the weights attached to the observations decrease exponentially as we go back in time, hence the name “exponential smoothing”. If \\(\\alpha\\) is small (i.e., close to 0), more weight is given to observations from the more distant past. If \\(\\alpha\\) is large (i.e., close to 1), more weight is given to the more recent observations. The \\((\\alpha, l_0)\\) parameters can be estimated by minimizing the SSE: \\[\\begin{align} \\text{SSE} = \\sum_{i=i}^T \\bigg(y_t - \\hat{y}_{t | t-1} \\bigg)^2. \\end{align}\\] Change the es function we developed in [Exponential Smoothing] to return the SSE, rather than yt, the smoothed time series. Now, use the optim function to find the values of \\((\\alpha, l_0)\\) that minimize SSE. Plot the exponentially smoothed time series of the Albanian export data using the optimized values for \\((\\alpha, l_0)\\). What are these values? 5.2 AR Parameter Constraints This relates to the AR Stationarity section from last week’s material. Prove that the following AR process either is or is not stationary: \\[x_t = -0.2 x_{t-1} + 0.4 x_{t-2} + w_t\\] 5.3 MA Parameter Constraints MA models do not have issues with stationarity (since they are finite sums of white noise), but there can be issues with parameter identifiability. That is, there can be multiple ways to write a single MA model. This leads to the parameter constraint on the \\(\\theta\\) parameters in ARMA Models. To show concretely how this can be a problem, show that the following two MA(1) models are the same (that is, they have the same mean function, variance function, and autocorrelation function). \\[ x_t = w_t + \\frac{1}{5} w_{t-1} ~\\text{with} ~w_t \\sim ~\\text{N}(0,25) \\\\ \\Updownarrow \\\\ x_t = w_t + 5 w_{t-1} ~\\text{with} ~w_t \\sim ~\\text{N}(0,1) \\] 5.4 SARIMA models Write out the following SARIMA model equations and describe the theoretical ACF function. (For example, The theoretical ACF tails of slowly at lags \\(\\ldots\\)). You can use simulations (astsa::sarima.sim) to arrive at your answer (show at least one empirical ACF per model). ARIMA\\((0,0,1)(0,0,1)_{12}\\) ARIMA\\((1,0,0)(1,0,0)_{12}\\) 5.5 Residuals check In Check the residuals, you can see the details for the Box-Pierce test. This is a very common way to check that the residuals process resembles white noise. Write a function that takes a time series vector as input and returns the Ljung-Box Q-Statistic for \\(k=2\\), along with the p-value for the hypothesis test. Then, check your function’s output using Box.test(x, lag = 2). "],["volatility-models.html", "Set 6 Volatility Models 6.1 ARCH(1) 6.2 GARCH(1,1) 6.3 fGarch::garchFit 6.4 Lab 5", " Set 6 Volatility Models 6.1 ARCH(1) So far, we have focused our attention on models for stationary time series data. In these models, the conditional mean changes over time while the conditional variance stays constant. For example, for the AR(1) model, \\[\\begin{align} E[x_t | x_{t-1}] &amp;= \\phi_1 x_{t-1} \\\\ Var[x_t | x_{t-1}] &amp;= \\sigma^2_w. \\end{align}\\] What if the conditional variance also changes over time? That is a good case to use an ARCH model. In an ARCH model, used often in financial applications, our response variable is typically the return at time \\(t\\): \\[\\begin{align} r_t = \\frac{x_t - x_{t-1}}{x_{t-1}} \\end{align}\\] Or, alternatively, the response may be \\(r_t = \\nabla log(x_t)\\). the ARCH(1) model is: \\[\\begin{align} r_t &amp;= \\sigma_t w_t \\\\ \\sigma^2_t &amp;= \\omega + \\alpha_1 r_{t-1}^2, \\end{align}\\] where \\(w_t \\sim N(0,1)\\). 6.1.1 Statistical Properties We can show that the ARCH(1) model has the following properties: \\[\\begin{align} E(r_t) &amp;= 0 \\\\ Var(r_t | r_{t-1}) &amp;= \\omega + \\alpha_1 r_{t-1}^2. \\end{align}\\] So, \\(r_{t}|r_{t-1} \\sim N(0,\\omega + \\alpha_1 r_{t-1}^2)\\). The conditional variance changes depending on the prior period return. This captures the characteristic that big return (loss) days tend to be followed by bit return (loss) days. Furthermore, we can subtract the second equation from the square of the first to obtain, \\[\\begin{align} r_t^2 &amp;= \\omega + \\alpha_1 r_{t-1}^2 + v_t. \\\\ \\end{align}\\] In this equation, \\(v_t = \\sigma_t^2 (w_t^2-1)\\). Since \\(w_t^2\\) is the square of a N(0,1) random variable \\(w_t^2-1\\) is a shifted (mean 0) \\(\\chi^2_1\\) random variable. What model does this look like? Using this model for \\(r_t^2\\), we can explore the unconditional variance of \\(r_t\\) since \\(Var(r_t)= E(r_t^2) = \\omega/(1-\\alpha_1)\\). And, furthermore, \\(E(r_t r_{t-h}) = 0\\) for all \\(h&gt;1\\). What model does \\(r_t\\) follow then? 6.1.2 Simulation/ Model Fit Let’s simulate data from: \\[\\begin{align} r_t &amp;= \\sigma_t w_t \\\\ \\sigma^2_t &amp;= 0.1 + 0.4 r_{t-1}^2, \\end{align}\\] And fit the model using fGarch::garchFit. 6.2 GARCH(1,1) An important extension of ARCH is the generalized ARCH (GARCH). The GARCH(1,1) can be written: \\[\\begin{align} r_t &amp;= \\sigma_t w_t \\\\ \\sigma^2_t &amp;= \\omega + \\alpha_1 r_{t-1}^2 + \\beta_1 \\sigma^2_{t-1}, \\end{align}\\] \\(\\alpha_1\\) is the ARCH effect. This parameter represents the short-term reaction of volatility to new shocks. The other parameter, \\(\\beta_1\\), captures the persistence of volatility. When this parameter is close to 1, volatility is more persistent. In this case, the squared returns follows a non-Gaussian ARMA(1,1) model: \\[\\begin{align} r_t^2 &amp;= \\omega + (\\alpha_1+\\beta_1)r_{t-1}^2 + v_t - \\beta_1 v_{t-1} \\\\ \\end{align}\\] 6.3 fGarch::garchFit # bitcoin data btc = read.csv(&quot;https://tinyurl.com/3epw5n4z&quot;) %&gt;% mutate(timestamp = ymd(timestamp)) y = diff(log(btc$btc)) # ARCH(1) m1 = fGarch::garchFit(data = y, formula = ~ garch(1,0)) # 4076 summary(m1) predict(m1, n.ahead = 10, plot = T) # Garch(1,1) m2 = fGarch::garchFit(data = y, formula = ~ garch(1,1)) # 4076 summary(m2) predict(m2, n.ahead = 10, plot = T) # ARMA(0,1) + Garch(1,1) m3 = fGarch::garchFit(data = y, formula = ~ arma(0,1) + garch(1,1)) # 4217 summary(m3) predict(m3, n.ahead = 10, plot = T, crit.val = 1) plot(m3, which = 5 ) 6.4 Lab 5 Fit the ARCH(1), GARCH(1,1), and GARCH(1,1) + ARMA(0,1) to the log of the difference of the historical bitcoin data. Report the log likelihood for each model. Plot the standardized residuals for each model (3 plots). Which one looks the best? Generate predictions for each model for the next 14 days (2/5 - 2/18) and compare with the actual data: recent = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/admn510_data/refs/heads/main/btc_recent.csv&quot;) For each of the 3 models, report the: mean squared prediction error (point estimate versus actual BTC price) Proportion of confidence intervals that contain the true BTC price using a critical value of 1. # hint! predict(model, n.ahead = 14, crit_val = 1) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
