[["index.html", "DS809 Set 1 Course Info", " DS809 David Reynolds 2024-02-27 Set 1 Course Info The course is designed to introduce techniques needed in the estimation/analysis of temporal data (time series) in various business disciplines. The first half of the course focuses on traditional stationary time series models. Some examples of business application areas include demand forecasting, financial asset return modeling, stochastic volatility modeling of financial indexes and securities, mortgage default risk assessment, call center arrival modeling, online webpage click-rate modeling, and market share modeling. The second half of the course focuses on state space modelling approaches to time series data and Bayesian techniques for time series data. "],["exploratory-analysis-of-time-series-data.html", "Set 2 Exploratory Analysis of Time Series Data 2.1 Time Series Data 2.2 Time Series EDA 2.3 Multiple Time Series 2.4 Autocorrelation 2.5 Lab 1", " Set 2 Exploratory Analysis of Time Series Data 2.1 Time Series Data A time series is an ordered sequence of observations, where the ordering is through time. Time series data creates unique problems for statistical modeling and inference. Traditional inference assumes that observations (data) are independent and identically distributed. Adjacent data points in time series data are not necessarily independent (uncorrelated). Most time series models aim to exploit such dependence. For instance, yesterday’s demand of a product may tell us something about today’s demand of a product. There are several different ways to represent time series data in R. We will use the tidyverse family of packages extensively in this class. This package includes the lubridate package, which includes functions to work with date-times. Two of the most common ways to represent time series data are using data frames in which one of the variables is a time object (such as POSIXct or Date) or using a time series object. These two representations are shown below with simulated trading data for a single 8-hour trading day. set.seed(1) # option 1: represent time series data within a data frame hr = seq(mdy_hm(&quot;12-11-2023 09:30&quot;), mdy_hm(&quot;12-11-2023 16:30&quot;), &#39;hour&#39;) # 8 hours pr = rnorm(8) # generate fake trading data trading_dat = data.frame(hr, pr) # option 2: represent time series data using a time series object trading_ts = ts(data = trading_dat$pr, start = 1, frequency = 8) 2.2 Time Series EDA The first thing to do in any data analysis is exploratory data analysis (EDA). Graphs enable many features of the data to be visualized, including patterns, unusual observations, changes over time, and relationships between variables. The features that are seen in plots of the data can then be incorporated into statistical models. R has several systems for making graphs. We will primarily use ggplot2, which is among the set of tidyverse packages and is one of the most versatile systems for plotting. We will use a data set from Kayak to motivate our analysis. conversions = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/conversions.csv&quot;) knitr::kable(head(conversions)) datestamp country_code marketing_channel conversions 2014-11-01 be Display Ads 1174 2014-11-01 be KAYAK Deals Email 57 2014-11-01 be Search Engine Ads 1122 2014-11-01 be Search Engine Results 976 2014-11-01 fi Display Ads 12 2014-11-01 fi Search Engine Results 2 This dataset contains information on the total number of daily conversions by country and marketing channel. Let us focus our analysis on the US and fist visualize the number of conversions by day. This plot contains a lot of useful information. To gain insight into how conversions depend on marketing channel, we can use facets. Facets are subplots that display a time series for each marketing channel. Display ads and search engine ads are the dominant marketing channels. Both have a regular pattern that is likely a function of the day of week, with a higher number of conversions during weekdays as compared with weekends. We can explore this feature by aggregating over each weekday and visualizing how the distribution of conversions changes by day. Clearly, there are significant changes in the mean level of conversions across the week. This is a form of seasonality. It may be useful to see what the data look like when this weekday effect is removed. To do so, we could visualize the residuals from the following linear regression model: \\[\\begin{align} \\hat{\\text{conversions}} = \\hat{\\beta}_0 + \\sum_{j=2}^7 \\bigg( \\hat{\\beta}_j \\times 1(\\text{weekday = j}) \\bigg), \\end{align}\\] where \\(j\\) indexes the day of week. The residuals from this model consist of each observation minus the mean for that particular weekday. This allows us to more clearly see the trend across the date range, removing the effect of the weekly pattern. 2.3 Multiple Time Series Often we will want to develop insight into the relationship between several variables. To illustrate, we will use quarterly data on GDP per capita and the Case Shiller Home Price Index (both from the FRED database). fred = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/fred_dat.csv&quot;)[,-1] fred$date = ymd(fred$date) It looks like these two time series track pretty closely to one another. We could fit a linear regression to this data in order to estimate the expected change in the Case Shiller Index for a unit ($1) change in GDP/ capital term estimate std.error statistic p.value (Intercept) -556.2290146 33.8244315 -16.44459 0 gdp 0.0126208 0.0005663 22.28669 0 Further, we could also examine the residuals to gain insight into what is missing from this model. The model severely underestimates the house index starting during the pandemic. There is a clear pattern to these residuals. Is this a problem? 2.4 Autocorrelation One of the assumptions of the linear regression model is that the errors are independent and identically distributed. That is, for the model, \\[\\begin{align} y = X \\beta + \\epsilon, \\end{align}\\] The error vector, \\(\\epsilon \\sim N(0, \\sigma^2)\\). This implies that there is no correlation structure to the residuals. One way to check that this is true is to check for the absence of correlation in the observed residuals. To review this concept, we’ll start with a definition for covariance. For two vectors of data, \\(x\\) and \\(y\\), the covariance between the two is, \\[\\begin{align} \\text{cov}(x,y) &amp;= \\frac{ \\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{n-1} \\end{align}\\] Correlation is a dimensionless measure of the linear association between two variables. It is defined as the covariance scaled by the standard deviations. That is, \\[\\begin{align} \\text{cor}(x,y) &amp;= \\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} \\\\ &amp;= \\frac{ \\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{ \\sqrt{ \\sum_i (x_i - \\bar{x})^2 \\sum_i (y_i - \\bar{y})^2 }} \\end{align}\\] Let’s compute this quantity on some simulated data. set.seed(1) n = 5 x = rnorm(n) y = rnorm(n) sum( (x - mean(x)) * (y - mean(y))) / ( (n-1) * sd(x) * sd(y) ) cor(x, y) For time series data, there is a closely related concept called autocorrelation. Given a time series, \\(y_t\\), where \\(t=1,\\ldots,T\\), autocorrelation is the correlation between \\(y_t\\) and its lagged value, \\(y_{t-k}\\). That is, autocorrelation is the correlation of a time series with a delayed copy of itself, as a function of delay. Just as correlation is a function of covariance, autocorrelation is a function of autocovariance. The (sample) autocovariance, \\(\\hat{\\gamma}\\) for a time series \\(y\\) at lag \\(k\\) is: \\[\\begin{align} \\hat{\\gamma}_k &amp;= \\text{cov}(y_t, y_{t-k}) \\\\ &amp;= \\frac{1}{T} \\sum_{t = k + 1}^{T} (y_t - \\bar{y})(y_{t-k} - \\bar{y}) \\end{align}\\] The (sample) autocorrelation function for lag \\(k\\), \\(\\hat{\\rho}_k\\), is simply the lag \\(k\\) autocovariance, \\(\\hat{\\gamma_k}\\) , scaled by the standard deviations. \\[\\begin{align} \\hat{\\rho}_k &amp;= \\frac{ \\hat{\\gamma_k} }{\\hat{\\sigma}_{y_t} \\hat{\\sigma}_{y_{t-k}}} \\\\ &amp;= \\frac{ \\hat{\\gamma_k} }{\\hat{\\gamma_0}}. \\end{align}\\] The second line follows from the linear regression assumption of constant variance. Here is a simple example of computing the lag 1 autocorrelation. a = c(1,2,3,4,5) a1 = c(1,2,3,4) a2 = c(2,3,4,5) # lag 1 autocorrelation sum( (a1 - mean(a)) * (a2 - mean(a))) / (sum( (a - mean(a))^2 ) ) # by hand (acf(a)) 2.5 Lab 1 Starting from the code chunk in Time Series Data, extend the simulated training data to a full week (5 days, eight hours each day). Using the data frame representation, plot(trading_dat$hr, trading_dat$pr). Using the time series data, plot(trading_ts). What are the differences between these two plots? Include both plots in your submission. set.seed(1) # option 1: represent time series data within a data frame hr = seq(mdy_hm(&quot;12-11-2023 09:30&quot;), mdy_hm(&quot;12-15-2023 16:30&quot;), &#39;hour&#39;) # 8 hours pr = rnorm(length(hr)) # generate fake trading data trading_dat = data.frame(hr, pr) %&gt;% filter(hour(hr) &gt; 9, hour(hr)&lt;17) # option 2: represent time series data using a time series object trading_ts = ts(data = trading_dat$pr, start = 1, frequency = 8) par(mfrow = c(1,2)) plot(x = trading_dat$hr, trading_dat$pr, type = &quot;l&quot;) plot(trading_ts) We can further hone in on the underlying trend of the residuals at the end of Time Series EDA by computing and plotting a moving average. For a time series \\(y_t\\), \\(t = 1, \\ldots, T\\), a moving average of order \\(m\\) can be written, \\[\\begin{align} \\hat{y_t} = \\frac{1}{m} \\sum_{j=-k}^{k} y_{t+j}, \\end{align}\\] where \\(m=2k+1\\). The concept behind this technique is that observations that are close in time are likely to be close in value. Compute a moving average of order \\(m=7\\) for the residual time series and plot it along with the residuals in a single plot. n = length(mod_df$resids) ma_resids = array(NA, dim = n) # initialize residuals vector for(i in 4:(n-3)) { ma_resids[i] = mean(mod_df$resids[(i-3):(i+3)]) } data.frame(date = mod_df$datestamp, resids = mod_df$resids, ma = ma_resids) %&gt;% drop_na() %&gt;% pivot_longer(-date) %&gt;% ggplot(aes(date, value)) + geom_line(aes(color = name)) + theme_minimal() + ggtitle(&quot;Plot of Residuals and their order 7 moving average&quot;) Join the Kayak visits data (see below) to the conversions data from Time Series EDA. Use the inner_join function with the argument, by = c(“datestamp”, “country_code”, “marketing_channel”) and then filter to only US observations. Make a plot of user_visits and conversions by day. Standardize them if it makes sense. Then, fit a linear regression model with conversions as the response variable and user_visits as the explanatory variable. What is the estimated line equation? Finally, make a plot of the residuals from this model. visits = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/visits.csv&quot;) visits$datestamp = ymd(visits$datestamp) df = inner_join(visits, conversions, by = c(&quot;datestamp&quot;, &quot;country_code&quot;, &quot;marketing_channel&quot;)) %&gt;% filter(country_code == &quot;us&quot;) %&gt;% drop_na() %&gt;% group_by(datestamp) %&gt;% summarise(visits = sum(user_visits), conv = sum(conversions)) conv_lm = lm(conv ~ visits, data = df) # conversions = 2752.13 + .31 * visits df %&gt;% mutate(std_visits = (visits - mean(visits))/sd(visits), std_conv = (conv - mean(conv))/sd(conv)) %&gt;% select(datestamp, std_visits, std_conv) %&gt;% pivot_longer(-datestamp) %&gt;% ggplot(aes(datestamp, value)) + geom_line(aes(color = name)) + theme_minimal() + ggtitle(&quot;Standardized Visits and Conversions&quot;) Wrtie a function in R that takes two arguments: a time series (\\(y\\)) formatted as a vector and an integer (\\(k\\)) specifying a lag. The output for this function is the lag \\(k\\) autocorrelation for \\(y\\), using the formula in Autocorrelation. Compare the output of your function to the output from (acf(y)). An extra challenge is to allow \\(k\\) to be a vector of lags, in which case your function should return a vector of autocorrelation values. y = arima.sim(n = 1e5, model = list(ar = c(.1,.2))) acf_fun = function(y, k) { return(cor( window(y, end = length(y)-k ), window(y,start = k+1))) } acf_fun(y,2) ## [1] 0.2054031 (acf(y, plot = F))$acf[3] ## [1] 0.2054009 The partial autocorrelation function, \\(\\phi_k\\), measures the correlation between a time series \\(y_t\\) and a lagged copy \\(y_{t-k}\\), with the linear dependence of \\(\\{ y_{t-1}, y_{t-2}, \\ldots,y_{t-k-1} \\}\\) removed. When \\(k=1\\), \\(\\hat{\\phi}_k = \\hat{\\rho}_k\\). When \\(k&gt;1\\), \\[\\begin{align} \\hat{\\phi}_k = \\text{cor}(y_1 - \\hat{y_1}|\\{ y_2, \\ldots, y_{k-1} \\} , y_k - \\hat{y_k}|\\{ y_2, \\ldots, y_{k-1} \\} ), \\end{align}\\] where \\(\\hat{y_1}|\\{ y_2, \\ldots, y_{k-1} \\}\\) is the predicted \\(y_1\\) using the linear regression where \\(\\{ y_2, \\ldots, y_{k-1} \\}\\) are explanatory variables. Compute the lag 1 and 2 partial autocorrelations for the following simulated time series. Show your code and validate your answers using the pacf function. The window function may be useful to extract subsets of the time series vector. set.seed(1) ysim = arima.sim(n=1000, list(ar=c(.5))) (pacf(ysim, plot = F)) ## ## Partial autocorrelations of series &#39;ysim&#39;, by lag ## ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.444 -0.030 -0.027 -0.026 0.011 -0.048 0.068 0.004 0.043 0.003 -0.008 ## 12 13 14 15 16 17 18 19 20 21 22 ## -0.033 0.036 0.009 0.057 -0.067 -0.030 -0.062 0.051 -0.028 -0.019 -0.043 ## 23 24 25 26 27 28 29 30 ## 0.018 -0.041 0.014 -0.014 -0.003 0.024 -0.014 0.031 # lag 1 - same as the lag 1 acf y1 = as.numeric( window(ysim,1,999)) y2 = as.numeric( window(ysim,2,1000) ) lag1 = (sum( (y1-mean(ysim)) * ( y2 - mean(ysim) )) )/ sum( (ysim - mean(ysim))^2) # lag 2 y1 = as.numeric( window(ysim, 1, 998)) y2 = as.numeric( window(ysim, 2, 999)) y3 = as.numeric( window(ysim, 3, 1000)) y1t = residuals( lm(y1 ~ y2)) y3t = residuals( lm(y3 ~ y2)) cor(y1t, y3t) # manual lag 2 pacf ## [1] -0.03019979 (pacf(ysim, plot = F))$acf[2] # function lag 2 pacf ## [1] -0.03049528 "],["smoothing-decomposition-noise.html", "Set 3 Smoothing, Decomposition, Noise 3.1 Exponential Smoothing 3.2 Decomposition 3.3 Statistical Models 3.4 Stationarity 3.5 Lab 2", " Set 3 Smoothing, Decomposition, Noise 3.1 Exponential Smoothing An alternate way to smooth a time series, generally appropriate for a time series with no clear seasonality or trend, is with exponential smoothing. Like the moving average, this method averages over recent observations but differs in that it assigns relatively more weight to observations that are relatively closer. This idea can be expressed as, \\[\\begin{align} \\hat{y}_{t+1 | t} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t | t-1}, \\end{align}\\] where \\(\\hat{y}_{t+1 | t}\\) can be interpreted as the smoothed value of \\(y_{t+1}\\) given data up to time \\(t\\). So, for the first few smoothed values, we have: \\[\\begin{align} \\hat{y}_{1|0} &amp;= l_0 \\\\ \\hat{y}_{2|1} &amp;= \\alpha y_1 + (1-\\alpha) l_0 \\\\ \\hat{y}_{3|2} &amp;= \\alpha y_2 + (1-\\alpha) \\hat{y}_{2|1} \\\\ &amp;= \\alpha y_2 + (1-\\alpha) \\alpha y_1 + (1-\\alpha)^2 l_0 \\end{align}\\] Since we don’t have data prior to \\(y_1\\), we denote \\(\\hat{y}_{1 | 0} = l_0\\). Therefore, this model depends on two parameters, \\((l_0, \\alpha)\\). If we continue with the sequence above, each predicted value \\(\\hat{y_t}\\) can be expressed, \\[\\begin{align} \\hat{y}_{t+1 | t} = (1-\\alpha) ^ t l_0 + \\sum_{j=0}^{t-1} \\alpha (1-\\alpha) ^ j y_{t-j}. \\\\ \\end{align}\\] Let’s take a look at how this method depends on the parameters. We will use Albanian export data from tsibbledata, which you can grab here: alb = global_economy %&gt;% filter(Country == &quot;Albania&quot;, Year &gt; 1990) y = alb$Exports In the plot above, of Albanian exports between 1991 and 2017, we display two smoothed time series (one for \\(\\alpha = 0.5\\) and one where \\(\\alpha = 0.9\\)). For any \\(\\alpha\\) between 0 and 1, the weights attached to the observations decrease exponentially as we go back in time, hence the name “exponential smoothing”. If \\(\\alpha\\) is small (i.e., close to 0), more weight is given to observations from the more distant past. If \\(\\alpha\\) is large (i.e., close to 1), more weight is given to the more recent observations. 3.1.1 Optimization The \\((\\alpha, l_0)\\) parameters can be estimated by minimizing the SSE: \\[\\begin{align} \\text{SSE} = \\sum_{i=i}^T \\bigg(y_t - \\hat{y}_{t | t-1} \\bigg)^2. \\end{align}\\] This is a non-linear optimization problem that you will solve in Lab 2! 3.2 Decomposition Time series data can exhibit a variety of patterns, and it is often helpful to split a time series into several components, each representing an underlying pattern category. In this section, we will aim to decompose a time series into three parts: a trend component (\\(T\\)), a seasonality component (\\(S\\)), and a random component (\\(R\\)). That is, for each observation \\(Y_t\\), we want to break it down into three parts: \\(Y_T = T_t + S_t + R_t\\). To illustrate, we will use a dataset on monthly retail employment in the US. Let’s first take a look at this data. # us_employment data frame from the &#39;fpp3&#39; package us_employment$date = mdy ( str_c( month(us_employment$Month), &quot;-1-&quot;, year(us_employment$Month)) ) retail = us_employment %&gt;% filter(Title == &quot;Retail Trade&quot;, year(date) &gt; 2002) To decompose this time series, we will follow this basic algorithm: First we will use a moving average of order \\(m=12\\) to get the trend, \\(T\\). Then, we will estimate the seasonal effects, \\(S\\),by fitting a linear regression model to the de-trendended series (\\(y-T\\)) in which the month is the explanatory variable. The remainder is the random component, \\(y-T-S\\). The trend captures the majority of the change that is observed in this time series, while the relative scale of the monthly seasonality and the random variation is small. 3.3 Statistical Models Thus far, we have explored time series data to better understand their properties. These exploration methods can also be used to generate forecasts for future values. However, they are not able to quantify the uncertainty inherent in those forecasts, nor do they model the dependency structure inherent in the time series data. We will begin with a very simple model that does both of these things. 3.3.1 Random Walk Let us consider a simple model to describe time series data, \\(y_t = y_{t-1}+e_t\\), where \\(e_t \\sim N(0, \\sigma^2)\\) and all elements of the error vector are mutually independent. Let’s derive some important properties of this model: What is the mean, \\(E(y_t)\\)? What is the variance, \\(Var(y_t)\\)? What is the covariance between successive observations, \\(\\text{cov}(y_t, y_{t-1})\\)? What is the correlation between successive observations, \\(\\text{cor}(y_t, y_{t-1})\\)? Which properties depend on time? 3.3.2 White noise Now let us define a new time series, \\(z_t = y_t - y_{t-1}\\). Define the same properties as 1-5 above. These two simple models are important in finance. If a time series follows a random walk, then its first difference is white noise. Let’s see if this is the case with GOOG. goog = getSymbols(&#39;GOOG&#39;, from=&#39;2020-12-22&#39;, to=&#39;2023-12-22&#39;,auto.assign = FALSE) googdf = data.frame(ymd(index(goog)), goog$GOOG.Close) names(googdf) = c(&quot;date&quot;, &quot;price&quot;) ggplot(googdf, aes(x = date, y = price)) + geom_line() + theme_minimal() + ggtitle(&quot;GOOG Closing Price&quot;) It seems plausible that this is a white noise series. How can we build further evidence of this. One way would be to examine the autocorrelation function. What are we looking for? This is helpful but visual evidence alone is fairly weak. We could alternatively use a hypothesis test (Box-Pierce test) in which: \\[\\begin{align} H_0 &amp;: \\rho_1 = \\rho_2 = \\ldots = \\rho_k = 0 \\\\ H_A &amp;: \\text{at least one autocorrelation is different from 0} \\end{align}\\] The test statistic (the Ljung-Box Q-Statistic) for this test: \\[\\begin{align} Q = n(n+2) \\sum_{i=1}^k \\frac{\\hat{\\rho}_i^2}{(n-k)} \\end{align}\\] follows a chi-squared distribution with \\(k\\) degrees of freedom under the null hypothesis. 3.4 Stationarity Stationarity is a convenient assumption that allows us to describe the statistical properties of a time series. A time series is said to be stationary if there is: Constant mean. \\(E(X_t) = \\mu\\). Constant variance. \\(Var(X_t) = \\sigma^2\\). Constant Autocorrelation. \\(Cor(X_t, X_{t-h}) = \\rho_h\\). 3.5 Lab 2 Change the es function we developed in Exponential Smoothing to return the SSE, rather than yt, the smoothed time series. Now, use the optim function to find the values of \\((\\alpha, l_0)\\) that minimize SSE. Plot the exponentially smoothed time series of the Albanian export data using the optimized values for \\((\\alpha, l_0)\\). What are these values? y = alb$Exports es_ss = function(par) { yt = es(par, y) return( sum( (yt - y)^2)) } optim(par = c(0,0), es_ss, lower = c(0,0), upper = c(1,10), method = &quot;L-BFGS-B&quot;)$par ## [1] 1.000000 7.484819 # alpha = 1; l0 = 7.484819 # how does this compare with ets (an implementation of exponential smoothing from the forecast pkg) ets(y, model = &quot;ANN&quot;) ## ETS(A,N,N) ## ## Call: ## ets(y = y, model = &quot;ANN&quot;) ## ## Smoothing parameters: ## alpha = 0.9999 ## ## Initial states: ## l = 7.4818 ## ## sigma: 2.4075 ## ## AIC AICc BIC ## 140.3544 141.3979 144.2419 Choose a different type of employee from the us_employment dataset and, first, plot the time series. Now, decompose the time series into trend, seasonal, and random components. Follow the algorithm in Decomposition. Plot these and comment on your observations. type = &quot;Government: Federal&quot; gov = filter(us_employment, Title == &quot;Government: Federal&quot;, year(date) &gt;= 1980) gov$trend = ma(gov$Employed, order = 12) gov = gov %&gt;% drop_na() gov$detrend = gov$Employed - gov$trend gov$season = predict(lm(detrend ~ factor(month(date)), gov)) gov$noise = gov$Employed - gov$trend - gov$season gov_plot = data.frame(gov) %&gt;% select(date, Employed, trend, season, noise) %&gt;% pivot_longer(-date) ggplot(gov_plot, aes(date, value)) + geom_line() + facet_wrap(~name, scales = &quot;free&quot;) + theme_minimal() Simulate a Random Walk with 100 time points (\\(y_t\\), \\(T= 1,\\ldots,100\\)). Repeat this process 50 times. Choose your own \\(\\sigma^2\\) and use the same value for each of the 50 iterations. Plot all 50 time series on the same plot. On the title, report the \\(\\sigma^2\\) value you used. Second, make a histogram of each of the end points (i.e., the 100th observation of each of the 50 time series). How does this histogram correspond with the theoretical properties of \\(Y_{100}\\) (i.e., the mean, variance, and shape of the distribution)? rw = cumsum(rnorm(100)) last_el = rw[100] par(mfrow = c(1,2)) plot(1:100, rw, type = &quot;l&quot;, ylim = c(-50,50), main = &quot;Random Walks; Sigma = 1&quot;) for (j in 2:50) { rw = cumsum(rnorm(100)) last_el[j] = rw[100] lines(1:100, rw, type = &quot;l&quot;, col = rainbow(50)[j]) } hist(last_el, breaks = 10, main = &quot;Histogram of 100th observations&quot;) Write a function that takes a time series vector as input and returns the Ljung-Box Q-Statistic for \\(k=2\\), along with the p-value for the hypothesis test in White Noise. y = arima.sim(100, model = list(ar=c(.6), ma = c(.1))) # input time series example boxtest = function(input_ts) { n = length(input_ts) rho_hat = acf(input_ts, plot = F)$acf[2:3] # rho_hat for k=1,2 Q = n * (n+2) * (rho_hat[1]^2/(n-1) + rho_hat[2]^2/(n-2)) pval = pchisq(Q, df = 2, lower.tail = F) return(list(Q=Q, pval = pval)) } boxtest(y) ## $Q ## [1] 56.90976 ## ## $pval ## [1] 4.387343e-13 Box.test(y, lag = 2, type = &quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: y ## X-squared = 56.91, df = 2, p-value = 4.388e-13 Using the quantmod package, choose your favorite stock and see if the differenced version of its closing prices over the past year can be described as white noise. Include plots and the result of the Box test. See GOOG example above. "],["arima-models.html", "Set 4 ARIMA Models 4.1 AR Models 4.2 MA Models 4.3 ARMA Models 4.4 ARIMA Models 4.5 Lab 3 4.6 Lab 3.5", " Set 4 ARIMA Models 4.1 AR Models An autoregressive model of order p, or AR(p), is defined as \\[ x_t = \\phi_0 + \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + \\dots + \\phi_p x_{t-p} + w_t \\] where we assume \\(w_t\\) is white noise. That is, \\(w_t \\sim N(0,\\sigma^2_w)\\). \\(\\phi_p \\neq 0\\) for an order-p process 4.1.1 AR(1) Model Let’s start by figuring out some properties of the simplest AR model, the AR(1) model: \\[ x_t = \\phi_0 + \\phi_1 x_{t-1} + w_t \\] We start by assuming that \\(x_t\\) is a stationary time series. Under this assumption, we can show: \\[\\begin{align} E(x_t) &amp;= \\frac{\\phi_0}{1-\\phi_1} \\\\ Var(x_t) &amp;= \\frac{\\sigma^2_w}{1-\\phi_1^2} \\\\ \\rho(h) &amp;= \\phi_1^h \\end{align}\\] For this to work, \\(|\\phi_1| &lt; 1\\). Now let’s simulate some data from an AR1 model and compare theoretical and observed quantities. set.seed(1) phi_1 = 0.5 sigsq_w = 1 x = arima.sim(n = 10000, model = list(ar=c(phi_1)), sd = sigsq_w) quantity theory empirical mean 0.000000 -0.0142169 variance 1.333333 1.3889404 ACF, k=2 0.250000 0.2611053 4.1.2 AR Stationarity In the case of the AR(1) model, it was apparent that the condition we needed to ensure a stationary model was the condition that \\(|\\phi_1| &lt; 1\\). For the general AR(p) model, though, the condition is more complicated. Before defining the condition, we define the backshift operator, \\(\\mathbf{B}\\): \\[\\mathbf{B}x_t = x_{t-1}\\]. Using this operator, we can re-write the AR(p) model as, \\[ x_t = \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + \\dots + \\phi_p x_{t-p} + w_t \\\\ \\Downarrow \\\\ \\begin{align} x_t - \\phi_1 x_{t-1} - \\phi_2 x_{t-2} - \\dots - \\phi_p x_{t-p} &amp;= w_t \\\\ (1 - \\phi_1 \\mathbf{B} - \\phi_2 \\mathbf{B}^2 - \\dots - \\phi_p \\mathbf{B}^p) x_t &amp;= w_t \\\\ \\phi_p (\\mathbf{B}^p) x_t &amp;= w_t \\\\ \\end{align} \\] We will refer to \\(\\phi_p (\\mathbf{B}^p)\\) as the characteristic equation. To be stationary, all roots of the characteristic equation must exceed 1 in absolute value. To make this more concrete, let’s go through some examples. \\(x_t = 0.5 x_{t-1} + w_t\\) \\(x_t = -0.2 x_{t-1} + 0.4 x_{t-2} + w_t\\) \\(x_t = x_{t-1} + w_t\\) 4.2 MA Models A moving average model of order q, or MA(q), is defined as \\[ x_t = \\theta_0 + w_t + \\theta_1 w_{t-1} + \\theta_2 w_{t-2} + \\dots + \\theta_q w_{t-q} \\] where \\(w_t\\) is white noise Each of the \\(x_t\\) is a sum of the most recent error terms Thus, all MA processes are stationary because they are finite sums of stationary WN processes. 4.2.1 MA(1) Model Let’s start by figuring out some properties of the simplest MA model, the MA(1) model: \\[ x_t = \\theta_0 + \\theta_1 w_{t-1} + w_t \\] We start by assuming that \\(x_t\\) is a stationary time series. Under this assumption, we can show: \\[\\begin{align} E(x_t) &amp;= \\theta_0 \\\\ Var(x_t) &amp;= \\sigma^2_w(1+\\theta_1^2) \\\\ \\rho(h) &amp;= \\frac{\\theta_1}{1+\\theta_1^2} \\text{ for } h=1 \\text{ and 0 otherwise. } \\end{align}\\] Now let’s simulate some data from an MA1 model and compare theoretical and observed quantities. set.seed(1) theta_1 = 0.5 sigsq_w = 1 z = arima.sim(n = 10000, model = list(ma=c(theta_1)), sd = sigsq_w) quantity theory empirical mean 0.00 -0.0098233 variance 1.25 1.2937915 ACF, k=2 0.40 0.4124200 4.3 ARMA Models An autoregressive moving average, or ARMA(p,q), model is written as \\[ x_t = \\phi_1 x_{t-1} + \\dots + \\phi_p x_{t-p} + w_t + \\theta_1 w_{t-1} + \\dots + \\theta_q w_{t-q} \\] 4.3.1 ACF for ARMA(p,q) models 4.3.2 PACF for ARMA(p,q) models 4.4 ARIMA Models Our data is not always stationary. If the data do not appear stationary, differencing can help. This leads to the class of autoregressive integrated moving average (ARIMA) models. ARIMA models are indexed with orders (p,d,q) where d indicates the order of differencing. \\(\\{x_t\\}\\) follows an ARIMA(p,d,q) process if \\((1-\\mathbf{B})^d x_t\\) is an ARMA(p,q) process. For example, if we look at Japan exports over the time period from 1960 to 2016, we see a clear evolution in the mean of the time series, indicating that the time series is not stationary. je = global_economy %&gt;% filter(Country == &quot;Japan&quot;, Year&lt; 2017) %&gt;% mutate(d = c(NA, diff(Exports))) As we saw in week one, if we instead look at the year over year changes in exports, we see something that more closely resembles a stationary time series. 4.4.1 Model Selection/ Fitting The general sequence of steps involved in fitting an ARIMA model to a given time series are: Evaluate whether the time series is stationary If not, make it stationary - select the differencing level (d) Select the AR level (p) and the MA level (q) that optimize the AIC Steps two and three are automated with the function forecast::auto.arima function. For instance, m0 = auto.arima(je$Exports) summary(m0) ## Series: je$Exports ## ARIMA(0,1,0) ## ## sigma^2 = 1.628: log likelihood = -93.1 ## AIC=188.21 AICc=188.28 BIC=190.24 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.0948585 1.264655 0.883942 0.2186117 7.269751 0.9826653 ## ACF1 ## Training set -0.04317139 4.4.2 Model Checking 4.4.2.1 Check the residuals Residuals = difference between the observations (data), \\(y\\), and expected (fitted) values, \\(\\hat{y}\\). Thus, the i’th residual is: \\(y_i-\\hat{y}_i\\). In R, we can obtain the vector of residuals using the residuals function, as shown below. 4.4.2.2 residuals() function in R The residuals() function will return the residuals for fitted models. ## Time Series: ## Start = 1 ## End = 57 ## Frequency = 1 ## [1] 0.01072294 -1.44501952 0.15303390 -0.39309669 0.45625211 1.02356255 ## [7] 0.06053730 -0.92437932 0.45807416 0.44426518 -0.19594755 0.86880508 ## [13] -1.08192227 -0.52480724 3.41625812 -0.76990990 0.72818035 -0.44252813 ## [19] -1.89419993 0.42955074 2.03742971 0.99396401 -0.20034080 -0.62338421 ## [25] 1.09590411 -0.50007152 -2.98081554 -0.97493052 -0.36290393 0.53728879 ## [31] 0.10797014 -0.43368768 -0.09399316 -0.61422391 -0.07420507 -0.03185334 ## [37] 0.50727239 1.06978245 -0.02437143 -0.57272023 0.67272500 -0.39538586 ## [43] 0.78912188 0.62107244 1.33273884 1.04040881 1.86046117 1.62041428 ## [49] -0.06927885 -4.90312357 2.51584283 -0.11176077 -0.38024365 1.37064557 ## [55] 1.62490343 0.04862657 -1.46977556 To check the fit of our model, we want to check that the residuals are white noise. 4.4.3 Forecasting 4.4.3.1 Point Estimates The basic idea of forecasting with an ARIMA model is to estimate the parameters and forecast forward. For example, let’s say we want to forecast with a ARIMA(2,1,0) model with drift: \\[z_t = \\mu + \\beta_1 z_{t-1} + \\beta_2 z_{t-2} + e_t\\] where \\(z_t = x_t - x_{t-1}\\), the first difference. Arima() would write this model: \\[(z_t-m) = \\beta_1 (z_{t-1}-m) + \\beta_2 (z_{t-2}-m) + e_t\\] The relationship between \\(\\mu\\) and \\(m\\) is \\(\\mu = m(1 - \\beta_1 - \\beta_2)\\). Let’s estimate the \\(\\beta\\)’s for this model from Japan export. ## ar1 ar2 drift ## -0.05580519 -0.18850080 0.10736838 ## drift ## 0.1335991 So we can forecast with this model: \\[z_t = 0.1335991 -0.05580519 z_{t-1} - 0.18850080 z_{t-2} + e_t\\] To obtain the \\(T+1\\) forecast value: zt_1 = 16.119153 - 17.588928 zt_2 = 17.588928 - 17.540302 16.119153 + (0.1335991 - 0.05580519 * zt_1 - 0.18850080 * zt_2) ## [1] 16.32561 4.4.3.2 Standard Errors To obtain the standard errors of the model forecast, it is helpful to reformulate the ARIMA model as an infinite order MA model. Any ARIMA model can be written as an infinite order MA model: \\[\\begin{align} x_t - \\mu &amp;= w_t + \\psi_1 w_{t-1} + \\psi_2 w_{t-1} + \\ldots + \\psi_k w_{t-k} + \\ldots \\\\ &amp;= \\sum_{j=0}^\\infty \\psi_j w_{t-j} \\text{ where } \\psi_0=1 \\end{align}\\] For example, the AR(1) model: \\[\\begin{align} y_t &amp;= \\phi y_{t-1} + w_t \\\\ &amp;= \\phi ( \\phi y_{t-1} + w_{t-1} ) + w_t \\\\ &amp;= \\phi^2 y_{t-1} + \\phi w_{t-1} + w_t \\\\ &amp;= \\sum_{j=0}^{\\infty} \\phi^j w_{t-j} \\end{align}\\] The standard deviation of the forecast error at time \\(T+h\\) is, \\[\\begin{align} \\sqrt{ \\hat{\\sigma}^2_w \\sum_{j=0}^{h-1} \\psi_j^2 } \\end{align}\\] With the assumption of normally distributed errors, a 95% prediction interval for \\(x_{T+h}\\) is, \\[\\begin{align} \\hat{x}_{T+h} \\pm 1.96 \\sqrt{ \\hat{\\sigma}^2_w \\sum_{j=0}^{h-1} \\psi_j^2 } \\end{align}\\] For example, the one step ahead forecast for the Japan Export model has a 95% prediction interval: c(16.32561 - qnorm(.975) * sqrt(1.645), 16.32561 + qnorm(.975) * sqrt(1.645)) ## [1] 13.81181 18.83941 You can also use the forecast function to obtain point estimates and prediction intervals. For example, fr = forecast(fit, h = 5) plot(fr) 4.5 Lab 3 Fill in the question marks in the table below. Additionally, for each box in the table, provide empirical evidence of the table entry using simulated data. For example, simulate data (you can use arima.sim or write your own code) that follows a AR(p) process to show that the ACF tails off slowly. Model ACF PACF AR(p) Tails off slowly Cuts off after lag ? MA(q) Cuts off after lag ? Tails off slowly ARMA(p,q) Tails off slowly Tails off slowly Consider fpp3::aus_airpassengers, the total number of passengers (in millions) from Australian air carriers for the period 1970-2011. Use forecast::auto.arima() to find an appropriate ARIMA model. What model was selected? Write the model in terms of the backshift operator. Check that the residuals look like white noise. Plot forecasts for the next 10 periods. Plot forecasts from an ARIMA(0,1,0) model with drift and compare these to the automatically selected model. Choose an employment type from fpp3::us_employment, the total employment in different industries in the United States. Are the data stationary? If not, find an appropriate transformation which yields stationary data. Examine ACF and PACF plots of the transformed data (if this was necessary to attain stationarity) to identify a couple of ARIMA models that might be useful in describing the time series. Which of your models is the best according to their AIC values? Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better. Forecast the next 3 years of data. Get the latest figures from https://fred.stlouisfed.org/categories/11 to check the accuracy of your forecasts. Eventually, the prediction intervals are so wide that the forecasts are not particularly useful. How many years of forecasts do you think are sufficiently accurate to be usable? 4.6 Lab 3.5 Another way that a time series may violate stationarity is with seasonality. For example, the Leisure and Hospitality sector of the economy spikes in the summer and dips in the winter months. We will look at employment (in thousands) in this sector since 2010: leisure = fpp3::us_employment %&gt;% filter(Title == &quot;Leisure and Hospitality&quot;, year(Month) &gt;= 2010) %&gt;% select(Month, Employed) Make a plot of this time series and comment on the trend and the seasonality. Find appropriate transformation(s) to make the time series stationary. What were the transformations? Provide any necessary plots to validate stationarity. Use auto.arima to fit an ARIMA model to the data. You can use either the original time series or the transformed time series. In either case, you may find it helpful to coerce your input time series into a time series object with frequency 12. This will ensure that the function explores seasonal parameters and transformations in the fitting process. What model was selected? Use forecast to forecast the next twelve months of data. Plot the forecast along with the orginal time series. Does the model capture the trend and seasonality? "],["prophet-lab.html", "Set 5 Prophet Lab", " Set 5 Prophet Lab Before doing this lab, read the paper on Canvas (“Forecasting at Scale,” in Other Resources within Modules) that describes the method implemented by the prophet package. Install and load the prophet package. And, skim the online tutorial so you know roughly where to look for further guidance if you get stuck anywhere. In this lab you will model daily US conversions generated across Kayak’s marketing channels from November 2014 through August 2015. Read in the conversions data (see code below). Manipulate the data frame so that it is compatible with prophet. That is, make it a dataframe with columns ds and y, containing the date (starting with 2014-11-01) and response variable (total US conversions across all marketing channels by day, starting with 18,669) respectively. See https://facebook.github.io/prophet/docs/quick_start.html#r-api for more details. In your written response, just show the first 6 rows of your dataframe, named df. conversions = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/conversions.csv&quot;) Fit the prophet model, using pmodel = prophet(df). Plot the model predictions along with the observed values. This can be achieved with the generic plot function, by passing in the model and the forecast dataframe, plot(pmodel, forecast), where forecast = predict(pmodel). Plot the components of the model fit using the function prophet_plot_components(model, forecast). What is the average squared difference between the observed values of \\(y_t\\) (i.e., us conversions on day t) versus the predicted values, \\(\\hat{y}_t\\)? The predicted values can be found in the forecast dataframe, as defined in question 2. That is, calculate: \\[\\begin{align} \\text{MSE} = \\frac{1}{T} \\sum_{t=1}^T (y_t - \\hat{y}_t)^2. \\end{align}\\] Now, include a regressor which is the total number of site visits per day in the US. This data is below (but will need to be processed). Details on adding regressors to a prophet model are here: https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#additional-regressors. What is the MSE for the updated model? visits = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/visits.csv&quot;) Compare the widths of the prediction intervals for the initial model, pmodel, and the updated model with the visits covariate. The prediction interval width is the difference between yhat_upper and yhat_lower, both of which are in the forecast dataframe as defined in question 2. Coerce the response, y, from your df into a time series object with frequency = 7 (since there is a clear weekly seasonality). Using auto.arima, fit a seasonal ARIMA model to this series. Write out the fitted model. Provide a summary comparison of the three models: pmodel, pmodel + covariates, and the seasonal ARIMA fit in question 7. You can use MSE or more visual methods to compare the fits of these three models. What are remaining questions you have about the prophet model? How about with ARIMA models? Answer this individually. There are no wrong answers. "],["covariates.html", "Set 6 Covariates 6.1 Linear Regression 6.2 dynlm Linear Model 6.3 lm with ARIMA Errors 6.4 State Space Model 6.5 Lab 5", " Set 6 Covariates For this chapter, we will look at modelling the amount of quotes generated by an insurance company using TV advertising spend as a predictor. The dataset is contained in fpp3::insurance. This chapter requires the following packages: fpp3 (for the insurance dataset), forecast (for auto.arima), dynlm (dynamic linear model), bsts (Bayesian Structural Time series). insurance %&gt;% pivot_longer(-Month) %&gt;% ggplot(aes(x = Month, y = value)) + geom_line(aes(color = name)) + theme_minimal() + ggtitle(&quot;Advertising verus Quotes&quot;) 6.1 Linear Regression We begin by modeling \\(y=\\) Quotes as a linear function of \\(x=\\) TV Spend. That is, we have the following model: \\[\\begin{align} y_t = \\beta_0 + \\beta_1 x_t + e_t, \\end{align}\\] where \\(e_1, \\ldots, e_t\\) are mutually independent and each \\(e_t \\sim \\text{N}(0,\\sigma^2)\\). # option one - simple linear regression linmod = lm(Quotes ~ TVadverts, data = insurance) linmod %&gt;% tidy %&gt;% kable() term estimate std.error statistic p.value (Intercept) -0.2401883 0.9008432 -0.2666261 0.7911986 TVadverts 1.6934377 0.1088208 15.5617088 0.0000000 plot(residuals(linmod), type = &quot;l&quot;) acf(residuals(linmod)) When we do regressions using time series variables, it is common for the errors (residuals) to have a time series structure. This violates the usual assumption of independent errors made in ordinary least squares regression. The consequence is that the estimates of coefficients and their standard errors will be wrong if the time series structure of the errors is ignored. 6.2 dynlm Linear Model Another way of fitting a linear model to time series data is using the package dynlm. This package, through its namesake function dynlm, fits the same model as above but is noteworthy for its helpful set of functions related to time. Before showing those, let us fit the same model as above using dynlm. # option two - time series regression: LM with very useful time functions y = ts(insurance$Quotes, frequency = 4) x1 = ts(insurance$TVadverts, frequency = 4) md = dynlm(y ~ x1 ) summary(md) %&gt;% tidy() %&gt;% kable() term estimate std.error statistic p.value (Intercept) -0.2401883 0.9008432 -0.2666261 0.7911986 x1 1.6934377 0.1088208 15.5617088 0.0000000 The results from this model are identcal to the simple linear regression. The utility of this package comes from the ease with which you can add lagged terms, trends and seasonal factors: md2 = dynlm(y ~ x1 + L(x1,1) + trend(y) + season(y)) 6.3 lm with ARIMA Errors See this site for additional information. It is possible to adjust estimated regression coefficients and standard errors when the errors have an ARIMA structure. The purpose is to adjust “ordinary” regression estimates for the fact that the residuals have an ARIMA structure. In this case, the model is written as, \\[\\begin{align} y_t &amp;= \\beta_0 + \\beta_1 x_t + \\eta_t \\\\ \\eta_t &amp;= \\phi_1 \\eta_{t-1} + \\dots + \\phi_p \\eta_{t-p} + w_t + \\theta_1 w_{t-1} + \\dots + \\theta_q w_{t-q}, \\end{align}\\] where \\(w_t \\sim N(0, \\sigma^2)\\) are white noise. This model can be fit within auto.arima or Arima. # option three - time series with ARIMA errors linmod2 = auto.arima(insurance$Quotes, xreg = as.matrix(insurance$TVadverts)) summary(linmod2) ## Series: insurance$Quotes ## Regression with ARIMA(0,1,0) errors ## ## Coefficients: ## xreg ## 1.3485 ## s.e. 0.0666 ## ## sigma^2 = 0.3301: log likelihood = -33.22 ## AIC=70.44 AICc=70.77 BIC=73.77 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set -0.01299736 0.5599955 0.4354811 -0.09980506 3.271638 0.2767685 ## ACF1 ## Training set 0.1664547 checkresiduals(m0) ## ## Ljung-Box test ## ## data: Residuals from ARIMA(0,1,0) ## Q* = 5.0746, df = 10, p-value = 0.8861 ## ## Model df: 0. Total lags used: 10 6.4 State Space Model 6.4.1 Framework An alternate framework for time series modeling is the state space model. This model contains a lot of depth and flexibility. For additional model details, see this resource. These models are based on a decomposition of the series into a number of components, each of which may be accompanied by error terms (and thus, uncertainty). The simplest model is the local level model. In this model, \\[\\begin{align} y_t &amp;= \\mu_t + \\epsilon_t \\\\ \\mu_t &amp;= \\mu_{t-1} + \\tau_t. \\end{align}\\] The idea of this model is that the observations \\(y\\) consist of noisy measurements (observation error) of an underlying random walk. What is the estimate for \\(\\mu_t\\) when \\(\\text{Var}(\\tau_t) = 0\\)? How about when \\(\\text{Var}(\\epsilon_t) = 0\\)? How does this relate to simple exponential smoothing? ss1 = StructTS(y, type = &quot;level&quot;) plot(forecast(ss1, h = 5)) 6.4.2 Specific Problem For the more general state space model we will use the package bsts. For our problem, we will introduce the covariate into the measurement equation. Our model is, \\[\\begin{align} y_t &amp;= \\mu_t + \\beta x_t + \\epsilon_t \\\\ \\mu_t &amp;= \\mu_{t-1} + \\eta_t. \\end{align}\\] We will use the package bsts to fit this model. The first thing to do when specifying a bsts package is the specify the contents of the latent state vector \\(\\mu_t\\) #library(bsts) ss &lt;- AddLocalLevel(list(), y) model1 &lt;- bsts(y ~ x1, state.specification = ss, niter = 1000) ## =-=-=-=-= Iteration 0 Tue Feb 27 15:30:16 2024 =-=-=-=-= ## =-=-=-=-= Iteration 100 Tue Feb 27 15:30:16 2024 =-=-=-=-= ## =-=-=-=-= Iteration 200 Tue Feb 27 15:30:16 2024 =-=-=-=-= ## =-=-=-=-= Iteration 300 Tue Feb 27 15:30:16 2024 =-=-=-=-= ## =-=-=-=-= Iteration 400 Tue Feb 27 15:30:16 2024 =-=-=-=-= ## =-=-=-=-= Iteration 500 Tue Feb 27 15:30:16 2024 =-=-=-=-= ## =-=-=-=-= Iteration 600 Tue Feb 27 15:30:16 2024 =-=-=-=-= ## =-=-=-=-= Iteration 700 Tue Feb 27 15:30:16 2024 =-=-=-=-= ## =-=-=-=-= Iteration 800 Tue Feb 27 15:30:16 2024 =-=-=-=-= ## =-=-=-=-= Iteration 900 Tue Feb 27 15:30:16 2024 =-=-=-=-= plot(model1, &quot;components&quot;) fore = predict(model1, horizon = 4, newdata = rep(mean(x1),5)) plot(fore) 6.5 Lab 5 Suppose you are a commodities trader in Australia and you would like to model and forecast the demand for cattle (i.e., Cattle (excl. calves)) using the fpp3::aus_livestock dataset, using data from January 2017 through December 2018. Knowing the market for meat production in Australia, you plan to use the demand for bulls (i.e., Bulls, bullocks and steers) as a covariate in your models to make your forecasts more precise. First, make a plot of these two time series (the total production per month across all States) starting in January 2017. Do they exhibit similar characteristics? What is the correlation between the two time series? Fit a simple linear regression with cattle as the response variable and bulls as the predictor. What is the estimated model equation (i.e., the fit line equation)? Plot the residuals over time and the acf of the residuals. Do these plots indicate that any of the linear regression assumptions may be violated? Using dynlm, include a linear trend term in the linear regression from above (using trend), and include the first lag of bulls. Are the coefficients for these terms significant? Using auto.arima, fit a linear regression with ARIMA errors using the same model setup as above (cattle as the response variable and bulls as the predictor). What is the model fit? Examine the residuals. Do they appear to be white noise? Using bsts, fit a state space model with cattle as the response variable and bulls as the predictor (i.e., same model as Specific Problem). What is the estimated regression coefficient? How does this compare to the coefficient in the models fit above (i.e., question 2 and 4)? Make a plot of the model components. Of the three different model types with covariates (i.e., the linear regression fit in #2, the linear regression with AIRMA errors fit in #3, and the state space model fit in #5), which has the best performance? Make a table that shows the MSE for each model. For the second two model types, \\(\\hat{y}_t\\) values can be obtained with the predict function where you specify the covariate values using the observed values. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
