[["index.html", "DS809 Set 1 Course Info", " DS809 David Reynolds 2024-12-27 Set 1 Course Info The course is designed to introduce techniques needed in the estimation/analysis of temporal data (time series) in various business disciplines. The first half of the course focuses on traditional stationary time series models. Some examples of business application areas include demand forecasting, financial asset return modeling, stochastic volatility modeling of financial indexes and securities, mortgage default risk assessment, call center arrival modeling, online webpage click-rate modeling, and market share modeling. The second half of the course focuses on state space modelling approaches to time series data and Bayesian techniques for time series data. "],["exploratory-analysis-of-time-series-data.html", "Set 2 Exploratory Analysis of Time Series Data 2.1 Time Series Data 2.2 Time Series EDA 2.3 Classical Regression 2.4 Noise Processes 2.5 Measures of Dependence 2.6 Stationarity 2.7 Estimation 2.8 Lab 1", " Set 2 Exploratory Analysis of Time Series Data 2.1 Time Series Data A time series is an ordered sequence of observations, where the ordering is through time. Time series data creates unique problems for statistical modeling and inference. Traditional inference assumes that observations (data) are independent and identically distributed. Adjacent data points in time series data are not necessarily independent (uncorrelated). Most time series models aim to exploit such dependence. For instance, yesterday’s demand of a product may tell us something about today’s demand of a product. There are several different ways to represent time series data in R. We will use the tidyverse family of packages extensively in this class. This package includes the lubridate package, which includes functions to work with date-times. Two of the most common ways to represent time series data are using data frames in which one of the variables is a time object (such as POSIXct or Date) or using a time series object. 2.2 Time Series EDA The first thing to do in any data analysis is exploratory data analysis (EDA). Graphs enable many features of the data to be visualized, including patterns, unusual observations, changes over time, and relationships between variables. The features that are seen in plots of the data can then be incorporated into statistical models. R has several systems for making graphs. We will primarily use ggplot2, which is among the set of tidyverse packages and is one of the most versatile systems for plotting. We will use a data set from Kayak to motivate our analysis. conversions = read.csv(&quot;data/conversions.csv&quot;) #read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/conversions.csv&quot;) knitr::kable(head(conversions)) datestamp country_code marketing_channel conversions 2014-11-01 be Display Ads 1174 2014-11-01 be KAYAK Deals Email 57 2014-11-01 be Search Engine Ads 1122 2014-11-01 be Search Engine Results 976 2014-11-01 fi Display Ads 12 2014-11-01 fi Search Engine Results 2 This dataset contains information on the total number of daily conversions by country and marketing channel. Let us focus our analysis on the US and first visualize the number of conversions by day. This plot contains a lot of useful information. To gain insight into how conversions depend on marketing channel, we can use facets. Facets are subplots that display a time series for each marketing channel. Display ads and search engine ads are the dominant marketing channels. Both have a regular pattern that is likely a function of the day of week, with a higher number of conversions during weekdays as compared with weekends. We can explore this feature by aggregating over each weekday and visualizing how the distribution of conversions changes by day. Clearly, there are significant changes in the mean level of conversions across the week. This is a form of seasonality. It may be useful to see what the data look like when this weekday effect is removed. To do so, we could visualize the residuals from the following linear regression model: \\[\\begin{align} \\hat{\\text{conversions}} = \\hat{\\beta}_0 + \\sum_{j=2}^7 \\bigg( \\hat{\\beta}_j \\times 1(\\text{weekday = j}) \\bigg), \\end{align}\\] where \\(j\\) indexes the day of week. The residuals from this model consist of each observation minus the mean for that particular weekday. This allows us to more clearly see the trend across the date range, removing the effect of the weekly pattern. 2.3 Classical Regression Given that regression is already in our tool kit, we will use it as our first method to model a time series. Let us assume that we have some output or dependent time series, say, \\(x_t\\) , for \\(t = 1, \\ldots,n\\), that is being influenced by a collection of possible inputs, say, \\(z_{t1}, z_{t2}, \\ldots, z_{tq}\\), where we regard the inputs as fixed and known. We will look at monthly Australian electricity production (the fpp2::elec dataframe). The data is shown below. Let us fit two models to this data, one where we use time as the independent variable. This estimates a linear trend. In the other, we will try to capture some of the obvious seasonality in the data. \\[\\begin{align} y_t &amp;= \\beta_0 + \\beta_1 t + w_t \\\\ y_t &amp;= \\beta_0 + \\beta_1 t + \\beta_2 \\times ({2\\pi \\cos(t)}) + w_t \\end{align}\\] The fits from these two models are shown below. Clearly, model 2 offers a superior fit as compared with model 1. In fact, this model explains about 95% of the variation in electricity production. To check the validity of the model, let us examine the residuals across the range of time, \\(t\\). One of the assumptions of the linear regression model is that the errors are independent and identically distributed. That is, for the model, \\[\\begin{align} y = X \\beta + \\epsilon, \\end{align}\\] The error vector, \\(\\epsilon \\sim N(0, \\sigma^2)\\), consists of independent and identically distributed random variables. This implies that there is no correlation structure to the residuals. One way to check that this is true is to check for the absence of correlation in the observed residuals. What does the figure above indicate about the validity of this assumption for model 2? 2.4 Noise Processes What if we re-arrange the model we fit above into a deterministic part and a random part: \\[\\begin{align} y_t - x_t \\beta = w_t \\end{align}\\] What can we say about the right hand side? Since it is a random variable that is indexed by time, we can consider this our first and simplest time series model. 2.4.1 White noise Let \\(w_t\\) be a random variable indexed by time, \\(t \\in [1,T]\\). The following properties characterize white noise: \\[\\begin{align} E(w_t) &amp;= 0 \\\\ Var(w_t) &amp;= \\sigma^2 \\\\ Cov(w_t, w_s) &amp;= 0 \\text{ } \\forall t,s \\end{align}\\] Note that Gaussian white noise is a special case where \\(w_t \\sim N(o, \\sigma^2)\\). White noise can come in different flavors. In the right hand plot below, \\(w_t = 2e_t - 1\\), where \\(e_t \\sim \\text{Bernoulli(.5)}\\). In the left plot, \\(w_t \\sim N(0,2)\\). 2.4.2 Random Walk Let us consider another simple model to describe time series data, \\(y_t = y_{t-1}+w_t\\), where \\(w_t \\sim N(0, \\sigma^2)\\) and all elements of the error vector are mutually independent. Let’s derive some important properties of this model: What is the mean, \\(E(y_t)\\)? What is the variance, \\(Var(y_t)\\)? 2.4.3 AR Process A slightly more general model is the AR model, in which \\(x_t\\) is a linear combination of its past \\(p\\) values plus gaussian white noise, \\(w_t\\). That is, \\[\\begin{align} x_t = \\sum_{i=1}^p \\phi_i x_{t-i} + w_t. \\end{align}\\] 2.4.4 Exercise Consider the AR(1) process with a drift: \\[\\begin{align} x_t &amp;= a + \\phi x_t + w_t \\end{align}\\] Write \\(x_t\\) as a function of past white noise values and find \\(E(x_t)\\) and \\(Var(x_t)\\). 2.5 Measures of Dependence 2.5.1 Autocovariance In all but the simplest models, there is dependence between adjacent values \\(x_s\\) and \\(x_t\\). This can be assessed using the notions of covariance and correlation. The autocovariance function is defined as the second moment product: \\[\\begin{align} \\gamma(s,t) &amp;= \\text{cov}(x_s, x_t) \\\\ &amp;= E((x_s - \\mu_s) (x_t - \\mu_t)) \\end{align}\\] The autocovariance measures the linear dependence between two points on the same series observed at different times. Very smooth series exhibit autocovariance functions that stay large even when the \\(t\\) and \\(s\\) are far apart, whereas choppy series tend to have autocovariance functions that are nearly zero for large separations. 2.5.2 Autocorrelation The autocorrelation function (ACF) is defined as, \\[\\begin{align} \\rho(s,t) &amp;= \\frac{\\gamma(s,t)}{\\sqrt{\\gamma(s,s) \\gamma(t,t)}} \\end{align}\\] The ACF measures the linear predictability of the series at time \\(t\\), say \\(x_t\\) , using only the value \\(x_s\\). 2.5.3 Exercise Compute the autocorrelation of: The white noise process. The random walk \\(x_t = a + \\phi x_{t-1} + w_t\\) 2.6 Stationarity A (weakly) stationary time series \\(x_t\\) is a process such that, Constant mean. \\(E(X_t) = \\mu\\). The autocovariance function, \\(\\gamma(s,t)\\) depends on \\(s\\) and \\(t\\) only through their difference, \\(|s-t|\\). Therefore, we can write the autocovariance function of a stationary time series as, \\[\\begin{align} \\gamma(h) &amp;= \\text{cov}(x_{t+h}, x_{t}) \\\\ &amp;= E((x_{t+h} - \\mu) (x_{t} - \\mu)) \\end{align}\\] And, similarly, the autocorrelation function of a stationary time series can be written, \\[\\begin{align} \\rho(h) &amp;= \\frac{\\gamma(t+h,t)}{\\sqrt{\\gamma(t+h,t+h) \\gamma(t,t)}} \\\\ &amp;= \\frac{\\gamma(h)}{\\gamma(0)} \\end{align}\\] 2.7 Estimation Although the theoretical autocorrelation and cross-correlation functions are useful for describing the properties of certain hypothesized models, most of the analyses must be performed using sampled data. 2.7.1 Mean if a time series is stationary, the mean function is constant so that we can estimate it by the sample mean, \\[\\begin{align} \\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^T x_t \\end{align}\\] 2.7.2 Autocorrelation Autocorrelation is the correlation of a time series with a delayed copy of itself, as a function of delay. The sample autocovariance, \\(\\hat{\\gamma}\\), for a time series \\(x\\) at lag \\(k\\) is: \\[\\begin{align} \\hat{\\gamma}_k &amp;= \\text{cov}(x_t, x_{t-k}) \\\\ &amp;= \\frac{1}{T} \\sum_{t = k + 1}^{T} (x_t - \\bar{x})(x_{t-k} - \\bar{x}) \\end{align}\\] The sample autocorrelation function for lag \\(k\\), \\(\\hat{\\rho}_k\\), is simply the lag \\(k\\) autocovariance, \\(\\hat{\\gamma_k}\\), scaled by the standard deviation. \\[\\begin{align} \\hat{\\rho}_k &amp;= \\frac{ \\hat{\\gamma_k} }{\\hat{\\sigma}_{y_t} \\hat{\\sigma}_{y_{t-k}}} \\\\ &amp;= \\frac{ \\hat{\\gamma_k} }{\\hat{\\gamma_0}}. \\end{align}\\] 2.8 Lab 1 cross correlation/ decomposition/ Often we will want to develop insight into the relationship between several variables. To illustrate, we will use quarterly data on GDP per capita and the Case Shiller Home Price Index (both from the FRED database). fred = read.csv(&quot;data/fred_dat.csv&quot;) #read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/fred_dat.csv&quot;)[,-1] fred$date = ymd(fred$date) We can further hone in on the underlying trend of the residuals at the end of Time Series EDA by computing and plotting a moving average. For a time series \\(y_t\\), \\(t = 1, \\ldots, T\\), a moving average of order \\(m\\) can be written, \\[\\begin{align} \\hat{y_t} = \\frac{1}{m} \\sum_{j=-k}^{k} y_{t+j}, \\end{align}\\] where \\(m=2k+1\\). The concept behind this technique is that observations that are close in time are likely to be close in value. Compute a moving average of order \\(m=7\\) for the residual time series and plot it along with the residuals in a single plot. n = length(mod_df$resids) ma_resids = array(NA, dim = n) # initialize residuals vector for(i in 4:(n-3)) { ma_resids[i] = mean(mod_df$resids[(i-3):(i+3)]) } data.frame(date = mod_df$datestamp, resids = mod_df$resids, ma = ma_resids) %&gt;% drop_na() %&gt;% pivot_longer(-date) %&gt;% ggplot(aes(date, value)) + geom_line(aes(color = name)) + theme_minimal() + ggtitle(&quot;Plot of Residuals and their order 7 moving average&quot;) Join the Kayak visits data (see below) to the conversions data from Time Series EDA. Use the inner_join function with the argument, by = c(“datestamp”, “country_code”, “marketing_channel”) and then filter to only US observations. Make a plot of user_visits and conversions by day. Standardize them if it makes sense. Then, fit a linear regression model with conversions as the response variable and user_visits as the explanatory variable. What is the estimated line equation? Finally, make a plot of the residuals from this model. visits = read.csv(&quot;https://raw.githubusercontent.com/dbreynol/DS809/main/data/visits.csv&quot;) visits$datestamp = ymd(visits$datestamp) df = inner_join(visits, conversions, by = c(&quot;datestamp&quot;, &quot;country_code&quot;, &quot;marketing_channel&quot;)) %&gt;% filter(country_code == &quot;us&quot;) %&gt;% drop_na() %&gt;% group_by(datestamp) %&gt;% summarise(visits = sum(user_visits), conv = sum(conversions)) conv_lm = lm(conv ~ visits, data = df) # conversions = 2752.13 + .31 * visits df %&gt;% mutate(std_visits = (visits - mean(visits))/sd(visits), std_conv = (conv - mean(conv))/sd(conv)) %&gt;% select(datestamp, std_visits, std_conv) %&gt;% pivot_longer(-datestamp) %&gt;% ggplot(aes(datestamp, value)) + geom_line(aes(color = name)) + theme_minimal() + ggtitle(&quot;Standardized Visits and Conversions&quot;) Wrtie a function in R that takes two arguments: a time series (\\(y\\)) formatted as a vector and an integer (\\(k\\)) specifying a lag. The output for this function is the lag \\(k\\) autocorrelation for \\(y\\), using the formula in Autocorrelation. Compare the output of your function to the output from (acf(y)). An extra challenge is to allow \\(k\\) to be a vector of lags, in which case your function should return a vector of autocorrelation values. y = arima.sim(n = 1e5, model = list(ar = c(.1,.2))) acf_fun = function(y, k) { return(cor( window(y, end = length(y)-k ), window(y,start = k+1))) } acf_fun(y,2) ## [1] 0.2160751 (acf(y, plot = F))$acf[3] ## [1] 0.2160726 The partial autocorrelation function, \\(\\phi_k\\), measures the correlation between a time series \\(y_t\\) and a lagged copy \\(y_{t-k}\\), with the linear dependence of \\(\\{ y_{t-1}, y_{t-2}, \\ldots,y_{t-k-1} \\}\\) removed. When \\(k=1\\), \\(\\hat{\\phi}_k = \\hat{\\rho}_k\\). When \\(k&gt;1\\), \\[\\begin{align} \\hat{\\phi}_k = \\text{cor}(y_1 - \\hat{y_1}|\\{ y_2, \\ldots, y_{k-1} \\} , y_k - \\hat{y_k}|\\{ y_2, \\ldots, y_{k-1} \\} ), \\end{align}\\] where \\(\\hat{y_1}|\\{ y_2, \\ldots, y_{k-1} \\}\\) is the predicted \\(y_1\\) using the linear regression where \\(\\{ y_2, \\ldots, y_{k-1} \\}\\) are explanatory variables. Compute the lag 1 and 2 partial autocorrelations for the following simulated time series. Show your code and validate your answers using the pacf function. The window function may be useful to extract subsets of the time series vector. set.seed(1) ysim = arima.sim(n=1000, list(ar=c(.5))) (pacf(ysim, plot = F)) ## ## Partial autocorrelations of series &#39;ysim&#39;, by lag ## ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.444 -0.030 -0.027 -0.026 0.011 -0.048 0.068 0.004 0.043 0.003 -0.008 ## 12 13 14 15 16 17 18 19 20 21 22 ## -0.033 0.036 0.009 0.057 -0.067 -0.030 -0.062 0.051 -0.028 -0.019 -0.043 ## 23 24 25 26 27 28 29 30 ## 0.018 -0.041 0.014 -0.014 -0.003 0.024 -0.014 0.031 # lag 1 - same as the lag 1 acf y1 = as.numeric( window(ysim,1,999)) y2 = as.numeric( window(ysim,2,1000) ) lag1 = (sum( (y1-mean(ysim)) * ( y2 - mean(ysim) )) )/ sum( (ysim - mean(ysim))^2) # lag 2 y1 = as.numeric( window(ysim, 1, 998)) y2 = as.numeric( window(ysim, 2, 999)) y3 = as.numeric( window(ysim, 3, 1000)) y1t = residuals( lm(y1 ~ y2)) y3t = residuals( lm(y3 ~ y2)) cor(y1t, y3t) # manual lag 2 pacf ## [1] -0.03019979 (pacf(ysim, plot = F))$acf[2] # function lag 2 pacf ## [1] -0.03049528 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
