--- 
title: "Artifex - Fall 2023"
author: "David Reynolds"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
# output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: openscapes/series
description: "(Partial) course materials for Artifex, Spring 2023."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(quantmod)
library(broom) # for the 'tidy' function
```

# Topics in Time Series

## Autocorrelation

A central concept in time series analysis is autocorrelation. This is the correlation between $y_t$ and its lagged value. For a lag of $h$, this is the correlation between $y_t$ and $y_{t-h}$. Let's first review correlation. For two vectors of data, $x$ and $y$, the correlation between the two is,

\begin{align}
\text{cor}(x,y) &= \frac{\text{cov}(x,y)}{\sigma_x \sigma_y} \\
                &= \frac{ \sum_i (x_i - \bar{x})(y_i - \bar{y})}{ \sqrt{ \sum_i (x_i - \bar{x})^2 \sum_i (y_i - \bar{y})^2 }}
\end{align}

```{r echo=TRUE, eval=FALSE}
set.seed(1)
n = 5
x = rnorm(n)
y = rnorm(n)

sum( (x - mean(x)) * (y - mean(y))) / ( (n-1) * sd(x) * sd(y) )
cor(x, y)
```

The autocorrelation applies this concept to a single time series. Autocorrelation is the correlation of a time series with a delayed copy of itself as a function of delay. The autocorrelation for a time series $y$ at lag $k$ is:

\begin{equation}
r_k = \frac{ \sum_{t = k + 1}^{T} (y_t - \bar{y})(y_{t-k} - \bar{y}) }{\sum_{t=1}^T (y_t - \bar{y})^2 }
\end{equation}

Here is a simple example of computing a lag 1 autocorrelation.

```{r echo=TRUE, eval=FALSE}
a = c(1,2,3,4,5)

a1 = c(1,2,3,4)
a2 = c(2,3,4,5)

# lag 1 autocorrelation
sum( (a1 - mean(a)) * (a2 - mean(a))) / (sum( (a - mean(a))^2 ) )  # by hand
(acf(a))
```

## Assignment 3

1. Write a function in R that takes a vector and a lag value, $k$, as arguments and outputs that autocorrelation of the vector with the lag $k$ copy of itself.

2. Find an economic variable of interest and compute the lag 1 through lag 10 acf of the variable. Do the same thing after applying the <code>diff</code> function to your data. Compare your function's output with that of <code>acf</code>. Describe the results of this analysis. What does it tell you, if anything, about the variable you chose.

```{r echo=FALSE, eval=FALSE}
set.seed(1)
n = 10
y = rnorm(n)

s1 = y[1:(n-1)]
s2 = y[2:n]
sum( (s1 - mean(y)) * (s2 - mean(y)) ) / sum( (y-mean(y))^2 )
(acf(y))

```


# Linear Regression Revisited

Let's investigate the relationship between home prices and mortgage rates. To start, let's use data from the <code>quantmod</code> package and look at the relationship between these two variables from 2000 to 2004.


```{r echo=TRUE, eval=FALSE}
# https://fred.stlouisfed.org/
pr = getSymbols('MORTGAGE30US',src='FRED', warnings = F) # 30-Year Fixed Rate Mortgage Average
home = getSymbols('CSUSHPINSA', src = 'FRED') # S&P/Case-Shiller U.S. National Home Price Index


# clean up dataset
mort = tidy(MORTGAGE30US) %>% # tidy is from the broom pkg - coerces xts object to a data frame
  mutate(date = ymd(index)) %>% # coerce the index to be a date
  filter(year(date) >= 2000, year(date) <= 2004) %>% # date range
  mutate(mo_year = ymd ( str_c (year(date), "-", month(date), "-", 01 )) ) %>% # make a new date so we can group by month/year 
  group_by(mo_year) %>% 
  summarise(val = mean(value)) %>% 
  filter(mo_year < "2023-08-01")


home_pr = tidy(CSUSHPINSA) %>% 
  mutate(date = ymd(index)) %>% 
  filter(year(date) >= 2000, year(date) <= 2004)

combined_dat = data.frame(mort = mort$val, pr = home_pr$value, date = home_pr$date)

ggplot(combined_dat, aes( mort, pr, color = as.factor( year(date) ) ) ) + 
  geom_point() + 
  theme_minimal() + 
  ggtitle("Case Shiller Index (y) and Avg. Mortgage Rates (x)") + 
  xlab("30-Year Fixed Rate Mortgage Average") + 
  ylab("S&P/Case-Shiller U.S. National Home Price Index")
```

How would you describe this relationship? Based on the relationship, do you think a simple linear regression model is appropriate for these data?

## Assignment 2

1. Fit a linear regression model to these two variables and write out the model equation.

2. Make a histogram of the residuals from this model and comment on how they correspond with the assumption of normal residuals.

3. Plot the residuals over time. What do you see?

4. Based on the residuals over time (plot from question 3), is there any additional variable you could add to the model to improve fit? If so, add this variable as a covariate to the linear regression and compare the fit of the augmented model with the one you fit in question 1.

5. Currently, the 30-Year Fixed Rate Mortgage Average is around 8%. What does the model predict for the Case-Shiller Index? The current value of the Case-Shiller Index is 305. What is the residual?

6. Now, using a scatterplot, check the relationship between these two variables for a more recent time frame (such as the last 10 years). Do you see the same relationship? Color code observations by year. 

```{r}
#long_dat = combined_dat %>% pivot_longer(-date)

#ggplot(long_dat, aes(x = date, y = value)) + geom_line() + facet_wrap(~ name)
```



# Kayak Data Analysis

We are going to do some analysis on a dataset sent from Kayak. The prompts and data are from their data science recruiting process. To access the data, use the following code:

```{r echo=TRUE, eval=FALSE}
visits = read.csv("https://raw.githubusercontent.com/dbreynol/DS809/main/data/visits.csv")
conversions = read.csv("https://raw.githubusercontent.com/dbreynol/DS809/main/data/conversions.csv")
channels = read.csv("https://raw.githubusercontent.com/dbreynol/DS809/main/data/channel_descriptions.csv")
```

## Prompt 

One of our team's goals is to accurately evaluate how successful our marketing campaigns are and to predict how successful future campaigns will be. In this assignment, we want you to explore how to evaluate one notion of success: conversion rate. Depending on the context, a conversion can mean different things, including a flight or hotel booking, a click on a specific part of our site, or an account sign up. Conversion rate in the first case could therefore be defined as completed flight bookings per user visit.
You were given some datasets that contain a sanitized version of some of our logging data. Explore the datasets and work on the questions below. One of the CSVs contains data on daily user visits broken down by user location and marketing channel, and another contains data on conversions also broken down by user location and channel. You also have been given a file with a brief description of the channels.

1. Do you see any interesting patterns in the data? For example, are there any seasonality trends in user visits or conversion rates (conversions/visits)? Do these seasonality trends differ between countries or channel? Can you think of any potential explanations for any of the patterns/trends/differences you saw?

2. Which channels have the best and worst conversion rates? What reasons can you think of to explain the differences? Do you see any major conversion differences for the same channel in different countries?

3. Predict next month's (September 2015) aggregate conversion rates for each channel and country combination? How good are your predictions?


```{r echo=F}
visits = read.csv("data/visits.csv")
conversions = read.csv("data/conversions.csv")
channels = read.csv("data/channel_descriptions.csv")
```

Last time, we discussed a few ways to estimate the slope coefficient in a linear regression model. This yields a point estimate. The other key ingredient for inference is to determine how much uncertainty there is in our estimate. For the case of $\hat{\beta_1}$, we want to know how much this estimate varies from sample to sample for the specified sample size. That is, we want to know the variance of the estimate:

\begin{align}
Var(\hat{\beta_1}).
\end{align}

To find what this is, let's just try to figure out the variance directly, using the analytic formula for $\hat{\beta_1}$ in simple linear regression:

\begin{align}
Var \bigg( \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2 }  \bigg).
\end{align}

To carry out this calculation, recall that $\text{var}(cx) = c^2 \text{var}(x)$. Also, recall the model assumption of independence across observations. After carrying out this calculation, we can show that,

\begin{equation}
Var(\hat{\beta}) = \frac{\hat{\sigma}^2}{(n-1) \hat{var}(x)}
\end{equation}

How does variation in $\hat{\beta}$ respond to changes in sample size? How about to variation in the predictor?

Before moving on, let's write some code to make sure our calculations correspond with those generated by <code>lm</code>.

In this case, we are able to obtain not only the variation of the estimate but also the distribution of the estimate as well as the test statistic,

\begin{align}
T = \frac{\hat{\beta_1} - \beta}{\sqrt{\text{var}\beta}} \sim t_{n-2}
\end{align}

This allows us to compute confidence intervals. For instance, if we want to compute a (1-$\alpha$) confidence interval, note that: 

\begin{align}
P(t_{\alpha/2} < T < t_{1-\alpha/2}) = (1-\alpha)
\end{align}

