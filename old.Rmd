# Topics in Time Series

## Autocorrelation

A central concept in time series analysis is autocorrelation. This is the correlation between $y_t$ and its lagged value. For a lag of $h$, this is the correlation between $y_t$ and $y_{t-h}$. Let's first review correlation. For two vectors of data, $x$ and $y$, the correlation between the two is,

\begin{align}
\text{cor}(x,y) &= \frac{\text{cov}(x,y)}{\sigma_x \sigma_y} \\
                &= \frac{ \sum_i (x_i - \bar{x})(y_i - \bar{y})}{ \sqrt{ \sum_i (x_i - \bar{x})^2 \sum_i (y_i - \bar{y})^2 }}
\end{align}

```{r echo=TRUE, eval=FALSE}
set.seed(1)
n = 5
x = rnorm(n)
y = rnorm(n)

sum( (x - mean(x)) * (y - mean(y))) / ( (n-1) * sd(x) * sd(y) )
cor(x, y)
```

The autocorrelation applies this concept to a single time series. Autocorrelation is the correlation of a time series with a delayed copy of itself as a function of delay. The autocorrelation for a time series $y$ at lag $k$ is:

\begin{equation}
r_k = \frac{ \sum_{t = k + 1}^{T} (y_t - \bar{y})(y_{t-k} - \bar{y}) }{\sum_{t=1}^T (y_t - \bar{y})^2 }
\end{equation}

Here is a simple example of computing a lag 1 autocorrelation.

```{r echo=TRUE, eval=FALSE}
a = c(1,2,3,4,5)

a1 = c(1,2,3,4)
a2 = c(2,3,4,5)

# lag 1 autocorrelation
sum( (a1 - mean(a)) * (a2 - mean(a))) / (sum( (a - mean(a))^2 ) )  # by hand
(acf(a))


```

## Assignment 3

1. Write a function in R that takes a vector and a lag value, $k$, as arguments and outputs that autocorrelation of the vector with the lag $k$ copy of itself.

2. Compute the lag 1 through 14 acf for the Kayak visits data (group by date to get total visits per day). What do you notice? Verify the results of your function using <code>acf</code>.

3. Find an economic variable of interest and compute the lag 1 through lag 10 acf of the variable. Do the same thing after applying the <code>diff</code> function to your data. Describe the results of this analysis. What does it tell you, if anything, about the variable you chose?

```{r echo=FALSE, eval=FALSE}
set.seed(1)
n = 10
y = rnorm(n)

s1 = y[1:(n-1)]
s2 = y[2:n]
sum( (s1 - mean(y)) * (s2 - mean(y)) ) / sum( (y-mean(y))^2 )
(acf(y))

```

## Change Points 

### CUSUM

Consider the following time series $x_t$, for $t = 1, \ldots, T$, where $T=100$. Suppose you are an analyst at an industrial company and this is a data stream from some instrument that you expect to generate random noise around 0. If there is a shift in the mean of this process, that signals trouble. Looking at this data, do you think there is any shift?

```{r echo=TRUE}
set.seed(1)
x1 = rnorm(75, 0, 0.5)
x2 = rnorm(25, 0.25, 0.5)
xt = c(x1, x2)
plot(xt, type = "l")
```

It is hard to say. One method to detect change points is CUSUM. This method works by incorporating all of the information in the sequence in the form of a cumulative sum. To implement this method, first standardize the data. We can call the new time series $y$ where each element, $y_i$, can is a standardized version of the corresponding $x_i$:

\begin{align}
y_i = \frac{x_i - \mu_0}{\sigma}
\end{align}

Then, define two vectors, $C^+$ and $C^-$ whose first element is 0. That is, $C_1^+ = C_1^- = 0$.  For $i \in (2,\ldots,T)$,

\begin{align}
C_i^+ &= \text{max}(0, y_i - K + C_{i-1}^+) \\
C_i^- &= \text{min}(0, y_i + K + C_{i-1}^-)
\end{align}

The parameter $K$ is the shift in means that you hope to detect, $\delta  \sigma$, measured in standard deviations. For example, if you want to detect a shift of a half standard deviation, use $K=0.5$. This is implemented in the <code>qcc</code> package within the <code>cusum</code> function, where the parameter $\delta$ is controlled by <code>se.shift</code>.


```{r}
mu_0 = 0
y = (xt - mu_0)/sd(xt)

shift_detect = 1

K = (shift_detect * sd(xt) -mu_0) # half of the mu_1 -  mu_0

st_hi = c(0)
st_lo = c(0)

for(i in 2:length(xt)) {
  st_hi[i] = max(0, y[i] - K + st_hi[i-1])
  st_lo[i] = min(0, y[i] + K + st_lo[i-1])
}

ob = cusum(xt, se.shift = shift_detect, center = mu_0,
           add.stats = T)

y1 = rnorm(50, 25, 1)
y2 = rnorm(25, 27, 1)
y = c(y1, y2)
#cusum(y)

st_hi = c(0)
st_lo = c(0)

z = (y - mean(y)) / sd(y)

K = (shift_detect * sd(y) - mu_0) 

for(i in 2:length(y)) {
  st_hi[i] = max(0, z[i] - K + st_hi[i-1])
  st_lo[i] = min(0, z[i] + K + st_lo[i-1])
}

#plot(st_hi, cusum(y)$pos)



# cusum fcn

cusum_us = function(time_series, mu_0) {
  # first, standardize
  y = (time_series - mu_0)/sd(time_series)
  
  # initialize hi and lo vectors
  st_hi = c(0)
  st_lo = c(0)
  
  #K = (shift_detect * sd(y) - mu_0) 
  k1 = 0.5
  
  
  for(i in 2:length(y)) {
    st_hi[i] = max(0, y[i] - k1 + st_hi[i-1])
    st_lo[i] = min(0, y[i] + k1 + st_lo[i-1])
  }
  
  return(list(lo = st_lo, hi = st_hi))

}

y1 = rnorm(50, 25, 2)
y2 = rnorm(25, 27, 2)
y = c(y1, y2)

#plot ( cusum_us(y, mu_0 = mean(y))$hi, cusum(y)$pos)
#plot ( cusum_us(y, mu_0 = mean(y))$lo, cusum(y)$neg)

#df = data.frame(ind = rep(1:length(y), 2), type = c(rep("us", length(y)), rep("cusum", length(y))), vals = c(cusum_us(y, mu_0 = mean(y))$lo, cusum(y)$neg) )

#df2 = data.frame(ind = rep(1:length(y), 2), type = c(rep("us", length(y)), rep("cusum", length(y))), vals = c(cusum_us(y, mu_0 = mean(y))$hi, cusum(y)$pos) )

#ggplot(df, aes(x = ind, y = vals, color = type)) + geom_line()
#ggplot(df2, aes(x = ind, y = vals, color = type)) + geom_line()
#cusum(y)
```

## Assignment 4

1. Write a function to generate vectors $C^+$ and $C^-$ for a given time series and $\delta = 1$.

2. Use the function on simulated data. How does it perform? Compare with output from <code>cusum</code>. Show the plotted data to another team and ask them for an eyeball estimate of the changepoint. Who won: (wo)man or machine?

## Moving Average

Another way to detect whether changes are occurring in a process is with a simple moving average. For a time series $y_t$, $t = 1, \ldots, T$, a moving average of order $m$ can be written,

\begin{align}
\hat{y_t} = \frac{1}{m} \sum_{j=-k}^{k} y_{t+j},
\end{align}

where $m=2k+1$. The concept behind this technique is that observations that are close in time are likely to be close in value. 

Below, I have simulated a noisy time series with a strong trend component and plotted the data long with the order 5 moving average and the order 10 moving average.

```{r echo = T, warning=FALSE}
set.seed(1)
xt = 0.5 * 1:100 + rnorm(100, 0, 5)
#plot(xt, type = "l", main = "Simulated Data")

df = data.frame(t = 1:100,
                data = xt,
                ma_5 = forecast::ma(xt, 3),
                ma_10 = forecast::ma(xt,11)) %>% 
  pivot_longer(-t)

ggplot(df, aes(x = t, y = value, color = name)) + 
  geom_line() + 
  theme_minimal() + 
  ggtitle("Higher order MA leads to a smoother series")

```



## Assignment 5

1. Write your own moving average function that takes a vector (your time series) and a scalar (order of the moving average, $m$) as arguments and outputs an order $m$ moving average of the vector input. Make sure that your output has the same length as the vector input, with NA values where appropriate.

2. For $\alpha = 0.3172$ and $l_0 = 1.1199$, write a function that computes a vector of predictions (i.e., $\hat{y}_t$ for all $t$) using exponential smoothing. Use the <code>xt</code> vector from the code block above (using a seed value of 1) and compare your results with the vector generated below. Hint: you might find it helpful to use the <code>sapply</code> function for the summation (but don't let this restrict your creativity).

```{r echo=TRUE}
fitted_vals = forecast::ets(xt, model = "ANN")$fitted
```


```{r eval=FALSE}
library(forecast)

# compute moving average on xt
t = 1:100
xt2 = t * 1.1 + rnorm(100, 1, 1)

l0 = 1
preds = c(l0)
alpha = 0.5
upperT = length(xt)
#l0 = 1
for(j in 2:length(xt)) {
  #t = j
  preds[j] = sum ( sapply(0:(j-1), function(x) (alpha * (1-alpha)^j * xt[j-x]) ))  + (1-alpha)^(j-1)* l0 
}

gen_preds = function(ts_data, alpha, l0) {
  preds = c(l0)
  #alpha = 0.5
  upperT = length(ts_data)
  #l0 = 1
  for(j in 2:length(ts_data)) {
    #t = j
    preds[j] = sum ( sapply(1:(j-1), function(x) (alpha * (1-alpha)^(x-1) * ts_data[j-x]) ))  + (1-alpha)^(j-1)* l0 
  }
  return(preds)
}

opt_fn = function(par) {
  sum( (gen_preds(xt2, par[1], par[2]) - xt2)^ 2)
}

optim(par = c(0,0), fn = opt_fn)

head( gen_preds(xt2, 0.9, 1) ) 
.9 * xt2[1] + .1 * 1

library(forecast)
m0 = ets(xt2, model = "ANN")
```


# Linear Regression Revisited

Let's consider a scenario in which we have information on a response variable, $y$, and $p$ predictor variables organized in a design matrix $X$. We want to model the conditional mean, $E(Y | X)$ using a linear combination of predictor variables. So, our model is:

\begin{align}
y = X \beta + \epsilon
\end{align}

For now, let us assume that there is a linear relationship between $y$ and $X$ and that the errors (the elements of $\epsilon$) are independent with mean of 0 and variance of $\sigma^2$.

## Estimation

To find the estimate for $\beta$, we can use the criteria of least squares and find the $\beta$ that minimizes the sum of the squared errors. That is, our $\hat{\beta}$ is the vector that minimizes,

\begin{align}
(y - X \beta) ^ T (y - X \beta)
\end{align}

In order to minimize this function, we need some basic results for matrix derivatives. 

\begin{align}
\frac{\partial}{\partial \beta} (A \beta) &= A \\
\frac{\partial}{\partial \beta} (\beta^T M \beta) &= 2 M \beta
\end{align}




## Example

Let's investigate the relationship between home prices and mortgage rates. To start, let's use data from the <code>quantmod</code> package and look at the relationship between these two variables from 2000 to 2004.


```{r echo=TRUE, eval=FALSE}
# https://fred.stlouisfed.org/
pr = getSymbols('MORTGAGE30US',src='FRED', warnings = F) # 30-Year Fixed Rate Mortgage Average
home = getSymbols('CSUSHPINSA', src = 'FRED') # S&P/Case-Shiller U.S. National Home Price Index
gdp = getSymbols('A939RX0Q048SBEA', src = 'FRED')

# clean up dataset
mort = tidy(MORTGAGE30US) %>% # tidy is from the broom pkg - coerces xts object to a data frame
  mutate(date = ymd(index)) %>% # coerce the index to be a date
  filter(year(date) >= 2010, year(date) <= 2023) %>% # date range
  mutate(mo_year = ymd ( str_c (year(date), "-", month(date), "-", 01 )) ) %>% # make a new date so we can group by month/year 
  group_by(mo_year) %>% 
  summarise(val = mean(value)) %>% 
  filter(mo_year < "2023-08-01")


home_pr = tidy(CSUSHPINSA) %>% 
  mutate(date = ymd(index)) %>% 
  filter(year(date) >= 2010, year(date) <= 2021)

gd = tidy(A939RX0Q048SBEA)
plot(gd$index, gd$value)

comb = inner_join(gd, home_pr, by = c("index"))
with(comb, plot(value.x, value.y))

m0 = lm(value.y ~ value.x, data = comb)
ggplot(comb, aes(x = value.x, y = value.y, color = as.factor(year(index)))) + geom_point()
ggplot(comb, aes(x = index, y = residuals(m0))) + geom_point()


combined_dat = data.frame(mort = mort$val, pr = home_pr$value, date = home_pr$date)

ggplot(combined_dat, aes( mort, pr, color = as.factor( year(date) ) ) ) + 
  geom_point() + 
  theme_minimal() + 
  ggtitle("Case Shiller Index (y) and Avg. Mortgage Rates (x)") + 
  xlab("30-Year Fixed Rate Mortgage Average") + 
  ylab("S&P/Case-Shiller U.S. National Home Price Index")
```

How would you describe this relationship? Based on the relationship, do you think a simple linear regression model is appropriate for these data?

## Assignment 2

1. Fit a linear regression model to these two variables and write out the model equation.

2. Make a histogram of the residuals from this model and comment on how they correspond with the assumption of normal residuals.

3. Plot the residuals over time. What do you see?

4. Based on the residuals over time (plot from question 3), is there any additional variable you could add to the model to improve fit? If so, add this variable as a covariate to the linear regression and compare the fit of the augmented model with the one you fit in question 1.

5. Currently, the 30-Year Fixed Rate Mortgage Average is around 8%. What does the model predict for the Case-Shiller Index? The current value of the Case-Shiller Index is 305. What is the residual?

6. Now, using a scatterplot, check the relationship between these two variables for a more recent time frame (such as the last 10 years). Do you see the same relationship? Color code observations by year. 

```{r}
#long_dat = combined_dat %>% pivot_longer(-date)

#ggplot(long_dat, aes(x = date, y = value)) + geom_line() + facet_wrap(~ name)
```



# Kayak Data Analysis

We are going to do some analysis on a dataset sent from Kayak. The prompts and data are from their data science recruiting process. To access the data, use the following code:

```{r echo=TRUE, eval=FALSE}
visits = read.csv("https://raw.githubusercontent.com/dbreynol/DS809/main/data/visits.csv")
conversions = read.csv("https://raw.githubusercontent.com/dbreynol/DS809/main/data/conversions.csv")
channels = read.csv("https://raw.githubusercontent.com/dbreynol/DS809/main/data/channel_descriptions.csv")
```

## Prompt 

One of our team's goals is to accurately evaluate how successful our marketing campaigns are and to predict how successful future campaigns will be. In this assignment, we want you to explore how to evaluate one notion of success: conversion rate. Depending on the context, a conversion can mean different things, including a flight or hotel booking, a click on a specific part of our site, or an account sign up. Conversion rate in the first case could therefore be defined as completed flight bookings per user visit.
You were given some datasets that contain a sanitized version of some of our logging data. Explore the datasets and work on the questions below. One of the CSVs contains data on daily user visits broken down by user location and marketing channel, and another contains data on conversions also broken down by user location and channel. You also have been given a file with a brief description of the channels.

1. Do you see any interesting patterns in the data? For example, are there any seasonality trends in user visits or conversion rates (conversions/visits)? Do these seasonality trends differ between countries or channel? Can you think of any potential explanations for any of the patterns/trends/differences you saw?

2. Which channels have the best and worst conversion rates? What reasons can you think of to explain the differences? Do you see any major conversion differences for the same channel in different countries?

3. Predict next month's (September 2015) aggregate conversion rates for each channel and country combination? How good are your predictions?


```{r echo=F}
visits = read.csv("data/visits.csv")
conversions = read.csv("data/conversions.csv")
channels = read.csv("data/channel_descriptions.csv")
```

Last time, we discussed a few ways to estimate the slope coefficient in a linear regression model. This yields a point estimate. The other key ingredient for inference is to determine how much uncertainty there is in our estimate. For the case of $\hat{\beta_1}$, we want to know how much this estimate varies from sample to sample for the specified sample size. That is, we want to know the variance of the estimate:

\begin{align}
Var(\hat{\beta_1}).
\end{align}

To find what this is, let's just try to figure out the variance directly, using the analytic formula for $\hat{\beta_1}$ in simple linear regression:

\begin{align}
Var \bigg( \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2 }  \bigg).
\end{align}

To carry out this calculation, recall that $\text{var}(cx) = c^2 \text{var}(x)$. Also, recall the model assumption of independence across observations. After carrying out this calculation, we can show that,

\begin{equation}
Var(\hat{\beta}) = \frac{\hat{\sigma}^2}{(n-1) \hat{var}(x)}
\end{equation}

How does variation in $\hat{\beta}$ respond to changes in sample size? How about to variation in the predictor?

Before moving on, let's write some code to make sure our calculations correspond with those generated by <code>lm</code>.

In this case, we are able to obtain not only the variation of the estimate but also the distribution of the estimate as well as the test statistic,

\begin{align}
T = \frac{\hat{\beta_1} - \beta}{\sqrt{\text{var}\beta}} \sim t_{n-2}
\end{align}

This allows us to compute confidence intervals. For instance, if we want to compute a (1-$\alpha$) confidence interval, note that: 

\begin{align}
P(t_{\alpha/2} < T < t_{1-\alpha/2}) = (1-\alpha)
\end{align}