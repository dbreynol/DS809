# Covariates



```{r}
#library(forecast)
#library(dynlm)

insurance %>% pivot_longer(-Month) %>% 
  ggplot(aes(x = Month, y = value)) + 
  geom_line(aes(color = name)) + 
  theme_minimal() + ggtitle("Advertising verus Quotes")
```

## Linear Regression

We begin by modeling $y=$ Quotes as a *linear function* of $x=$ TV Spend. That is, we have the following model:

\begin{align}
y_t = \beta_0 + \beta_1 x_t + e_t,
\end{align}

where $e_1, \ldots, e_t$ are mutually independent and each $e_t \sim \text{N}(0,\sigma^2)$.

```{r}
# option one - simple linear regression
linmod = lm(Quotes ~ TVadverts, data = insurance)
linmod %>% tidy %>% kable()
plot(residuals(linmod), type = "l")
```

When we do regressions using time series variables, it is common for the errors (residuals) to have a time series structure. This violates the usual assumption of independent errors made in ordinary least squares regression. The consequence is that the estimates of coefficients and their standard errors will be wrong if the time series structure of the errors is ignored.

## `dynlm` Linear Model

Another way of fitting a linear model to time series data is using the package `dynlm`. This package, through its namesake function `dynlm`, fits the same model as above but is noteworthy for its helpful set of functions related to time. Before showing those, let us fit the same model as above using `dynlm`.

```{r echo=TRUE}
# option two - time series regression: LM with very useful time functions
y = ts(insurance$Quotes, frequency = 4)
x1 = ts(insurance$TVadverts, frequency = 4)
md = dynlm(y ~ x1 )
summary(md) %>% tidy() %>% kable()
```

The results from this model are identcal to the simple linear regression. The utility of this package comes from the ease with which you can add lagged terms, trends and seasonal factors:

```{r echo=TRUE}
md2 = dynlm(y ~ x1 + L(x1,1) + trend(y) + season(y))
```

## `lm` with ARIMA Errors

It is possible to adjust estimated regression coefficients and standard errors when the errors have an ARIMA structure. The purpose is to adjust "ordinary" regression estimates for the fact that the residuals have an ARIMA structure.

In this case, the model is written as,

\begin{align}
y_t &= \beta_0 + \beta_1 x_t + \eta_t \\
\eta_t &= \phi_1 \eta_{t-1} + \dots + \phi_p \eta_{t-p} + w_t + \theta_1 w_{t-1} + \dots + \theta_q w_{t-q},
\end{align}

where $w_t \sim N(0, \sigma^2)$ are white noise. This model can be fit within `auto.arima` or `Arima`.

```{r echo=TRUE}
# option three - time series with ARIMA errors
linmod2 = auto.arima(insurance$Quotes, xreg = as.matrix(insurance$TVadverts))
summary(linmod2)
checkresiduals(m0)
```

## State Space Model

### Framework

An alternate framework for time series modeling is the state space model. These models are based on a decomposition of the series into a number of components, each of which may be accompanied by error terms (and thus, uncertainty). The simplest model is the *local level model*. In this model,

\begin{align}
y_t &= \mu_t + \epsilon_t \\
\mu_t &= \mu_{t-1} + \tau_t.
\end{align}

The idea of this model is that the observations $y$ consist of noisy measurements (observation error) of an underlying random walk. 

What is the estimate for $\mu_t$ when $\tau_t = 0$? How about when $\epsilon_t = 0$? How does this relate to simple exponential smoothing?

```{r echo=TRUE}
ss1 = StructTS(y, type = "level")
plot(forecast(ss1, h = 5))
```

### Specific Problem

For our problem, we will introduce the covariate into the measurement equation. Our model is,

\begin{align}
y_t &= \mu_t + \beta x_t + \epsilon_t \\
\mu_t = \mu_{t-1} + \eta_t.
\end{align}

We will use the package `bsts` to fit this model. The first thing to do when specifying a `bsts` package is the specify the contents of the latent state vector $\mu_t$

```{r message=FALSE}
#library(bsts)
ss <- AddLocalLevel(list(), y)
model1 <- bsts(y ~ x1,
               state.specification = ss,
               niter = 1000)

plot(model1, "components")
fore = predict(model1, horizon = 4, newdata = rep(mean(x1),5))
plot(fore)
```

