# Exploratory Analysis of Time Series Data

## Time Series Data

* A time series is an ordered sequence of observations, where the ordering is through time.

* Time series data creates unique problems for statistical modeling and inference.
  + Traditional inference assumes that observations (data) are independent and identically distributed. Adjacent data points in time series data are not necessarily independent (uncorrelated).
  + Most time series models aim to exploit such dependence. For instance, yesterday’s demand of a product may tell us something about today’s
demand of a product.

* There are several different ways to represent time series data in R. 

* We will use the <code>tidyverse</code> family of packages extensively in this class. This package includes the <code>lubridate</code> package, which includes functions to work with date-times.

* Two of the most common ways to represent time series data are using data frames in which one of the variables is a time object (such as POSIXct or Date) or using a time series object. 

## Time Series EDA

The first thing to do in any data analysis is exploratory data analysis (EDA). Graphs enable many features of the data to be visualized, including patterns, unusual observations, changes over time, and relationships between variables. The features that are seen in plots of the data can then be incorporated into statistical models.

R has several systems for making graphs. We will primarily use ggplot2, which is among the set of tidyverse packages and is one of the most versatile systems for plotting. We will use a data set from Kayak to motivate our analysis.

```{r echo=TRUE}
conversions = read.csv("https://raw.githubusercontent.com/dbreynol/DS809/main/data/conversions.csv")
knitr::kable(head(conversions))
```

This dataset contains information on the total number of daily conversions by country and marketing channel. Let us focus our analysis on the US and fist visualize the number of conversions by day. 

```{r}
conversions$datestamp = ymd(conversions$datestamp)

# overall trend

conversions %>% 
  filter(country_code == "us") %>% 
  group_by(datestamp, country_code) %>% 
  summarise(tot_conv = sum(conversions), .groups = "drop") %>% 
  ggplot(aes(x = datestamp, y = tot_conv)) +
  geom_line() + 
  theme_minimal() + 
  ggtitle(" US Conversions by Day") + 
  xlab("Date") + 
  ylab("Total Conversions") + 
  scale_x_date(NULL, date_breaks = "1 month", date_labels = "%b")


```

This plot contains a lot of useful information. To gain insight into how conversions depend on marketing channel, we can use facets. Facets are subplots that display a time series for each marketing channel.


```{r}
conversions %>% 
  filter(country_code == "us") %>% 
  ggplot(aes(x = datestamp, y = conversions)) + 
  geom_line() + 
  facet_wrap(~ marketing_channel) + theme_minimal()
```

Display ads and search engine ads are the dominant marketing channels. Both have a regular pattern that is likely a function of the day of week, with a higher number of conversions during weekdays as compared with weekends. We can explore this feature by aggregating over each weekday and visualizing how the distribution of conversions changes by day.

```{r}
conversions %>% 
  filter(country_code == "us") %>% 
  drop_na() %>% 
  group_by(datestamp) %>% summarise(conversions = sum(conversions)) %>% 
  mutate(day = wday(datestamp, label = T)) %>% 
  ggplot(aes(x = as.factor(day), y = conversions)) + 
  geom_boxplot() + 
  stat_summary(fun = "mean", geom = "point", color = "red") +
  theme_minimal() + 
  ggtitle("Conversion Distribution by Day") + 
  xlab("Day of Week") + 
  ylab("Conversions")

```

Clearly, there are significant changes in the mean level of conversions across the week. This is a form of seasonality. It may be useful to see what the data look like when this weekday effect is removed. To do so, we could visualize the residuals from the following linear regression model:

\begin{align}
\hat{\text{conversions}} = \hat{\beta}_0 + \sum_{j=2}^7 \bigg( \hat{\beta}_j \times 1(\text{weekday = j}) \bigg),
\end{align}

where $j$ indexes the day of week. The residuals from this model consist of each observation minus the mean for that particular weekday. 

```{r}
# EDA via models

mod_df = conversions %>% 
  filter(country_code == "us") %>% 
  drop_na() %>% 
  group_by(datestamp, country_code) %>% 
  summarise(tot_conv = sum(conversions), .groups = "drop") %>% 
  mutate(wday = factor(wday(datestamp, label= T), ordered = F))

mod = lm(tot_conv ~ wday, data = mod_df)

mod_df$resids = residuals(mod)

mod_df %>% 
  ggplot(aes(x = datestamp, y = resids)) + 
  geom_line() + 
  theme_minimal() + 
  ggtitle("US Conversions excluding Weekly Seasonality")
```

This allows us to more clearly see the trend across the date range, removing the effect of the weekly pattern. 

## Classical Regression 

Given that regression is already in your tool kit, we will use it to model a time series. Let us assume that we have some output or dependent time series, say, $x_t$ , for $t = 1, \ldots,n$, that is being influenced by a collection of possible inputs, say, $z_{t1}, z_{t2}, \ldots, z_{tq}$, where we regard the inputs as fixed and known. 

We will look at monthly Australian electricity production (the <code>fpp2::elec</code> dataframe). The data is shown below.

```{r echo=F}
aelec <- window(elec, start=1980)
plot(x = time(aelec), y = aelec, type = "l", main = "Australian Electricity Production")
```

Let us fit two models to this data, one where we use time as the independent variable. This estimates a linear trend. In the other, we will try to capture some of the obvious seasonality in the data.

\begin{align}
y_t &= \beta_0 + \beta_1 t + w_t \\ 
y_t &= \beta_0 + \beta_1 t + \beta_2 \times ({2\pi \cos(t)}) + w_t
\end{align}

The fits from these two models are shown below. 

```{r}
m0 = lm(aelec ~ time(aelec))
m1 = lm(aelec ~ time(aelec) + cos(2 * pi * time(aelec)))
plot(x = time(aelec), y = aelec, type = "l", main = "Australian Electricity Production")
lines(x = c(time(aelec)), y = predict(m0), col = "red")
lines(x = c(time(aelec)), y = predict(m1), col = "blue")
```

Clearly, model 2 offers a superior fit as compared with model 1. In fact, this model explains about 95\% of the variation in electricity production. To check the validity of the model, let us examine the residuals across the range of time, $t$.

```{r}
plot(x = c(time(aelec)), y = residuals(m1), main = "Model 2 Residuals")
```

One of the assumptions of the linear regression model is that the errors are independent and identically distributed. That is, for the model,

\begin{align}
y = X \beta + \epsilon,
\end{align}

The error vector, $\epsilon \sim N(0, \sigma^2)$. This implies that there is no correlation structure to the residuals. One way to check that this is true is to check for the absence of correlation in the observed residuals.

## Noise Processes


Thus far, we have explored time series data to better understand their properties. These exploration methods can also be used to generate forecasts for future values. However, they are not able to quantify the uncertainty inherent in those forecasts, nor do they model the dependency structure inherent in the time series data. We will begin with a very simple model that does both of these things.


### White noise

Now let us define a new time series, $z_t = y_t - y_{t-1}$.

Define the same properties as 1-5 above. These two simple models are important in finance. If a time series follows a random walk, then its first difference is white noise. Let's see if this is the case with GOOG. 

```{r echo=TRUE, eval=FALSE}
goog = getSymbols('GOOG', from='2020-12-22', to='2023-12-22',auto.assign = FALSE)
googdf = data.frame(ymd(index(goog)), goog$GOOG.Close)
names(googdf) = c("date", "price")

ggplot(googdf, aes(x = date, y = price)) + 
  geom_line() + 
  theme_minimal() + 
  ggtitle("GOOG Closing Price")
```

### Random Walk

Let us consider a simple model to describe time series data,

$y_t = y_{t-1}+e_t$,

where $e_t \sim N(0, \sigma^2)$ and all elements of the error vector are mutually independent.

Let's derive some important properties of this model:

1. What is the mean, $E(y_t)$?
2. What is the variance, $Var(y_t)$?
3. What is the covariance between successive observations, $\text{cov}(y_t, y_{t-1})$?
4. What is the correlation between successive observations, $\text{cor}(y_t, y_{t-1})$?
5. Which properties depend on time?

```{r echo=FALSE, eval=FALSE}
# simulate from each model type in class
t = 100
acf_rw = function(k) {sqrt( (t-k)/ t )}
curve(acf_rw, from = 1, to = 9)
```

### AR Process

## Autocorrelation

compute autocorrelation of ar process

To review this concept, we'll start with a definition for covariance. For two vectors of data, $x$ and $y$, the covariance between the two is,

\begin{align}
\text{cov}(x,y) &= \frac{ \sum_i (x_i - \bar{x})(y_i - \bar{y})}{n-1}
\end{align}

Correlation is a dimensionless measure of the linear association between two variables. It is defined as the covariance scaled by the standard deviations. That is,

\begin{align}
\text{cor}(x,y) &= \frac{\text{cov}(x,y)}{\sigma_x \sigma_y} \\
                &= \frac{ \sum_i (x_i - \bar{x})(y_i - \bar{y})}{ \sqrt{ \sum_i (x_i - \bar{x})^2 \sum_i (y_i - \bar{y})^2 }}
\end{align}

Let's compute this quantity on some simulated data.

```{r echo=TRUE, eval=FALSE}
set.seed(1)
n = 5
x = rnorm(n)
y = rnorm(n)

sum( (x - mean(x)) * (y - mean(y))) / ( (n-1) * sd(x) * sd(y) )
cor(x, y)
```

For time series data, there is a closely related concept called autocorrelation. Given a time series, $y_t$, where $t=1,\ldots,T$, autocorrelation is the correlation between $y_t$ and its lagged value, $y_{t-k}$. That is, autocorrelation is the correlation of a time series with a delayed copy of itself, as a function of delay. Just as correlation is a function of covariance, autocorrelation is a function of autocovariance. The (sample) autocovariance, $\hat{\gamma}$ for a time series $y$ at lag $k$ is:

\begin{align}
\hat{\gamma}_k &= \text{cov}(y_t, y_{t-k}) \\
&=  \frac{1}{T} \sum_{t = k + 1}^{T} (y_t - \bar{y})(y_{t-k} - \bar{y})
\end{align}

The (sample) autocorrelation function for lag $k$, $\hat{\rho}_k$, is simply the lag $k$ autocovariance, $\hat{\gamma_k}$ , scaled by the standard deviations.

\begin{align}
\hat{\rho}_k &= \frac{ \hat{\gamma_k} }{\hat{\sigma}_{y_t} \hat{\sigma}_{y_{t-k}}} \\
&= \frac{ \hat{\gamma_k} }{\hat{\gamma_0}}.
\end{align}

The second line follows from the linear regression assumption of constant variance. Here is a simple example of computing the lag 1 autocorrelation.

```{r echo=TRUE, eval=FALSE}
a = c(1,2,3,4,5)

a1 = c(1,2,3,4)
a2 = c(2,3,4,5)

# lag 1 autocorrelation
sum( (a1 - mean(a)) * (a2 - mean(a))) / (sum( (a - mean(a))^2 ) )  # by hand
(acf(a))
```



## Stationarity 

Stationarity is a convenient assumption that allows us to describe the statistical properties of a time series. A time series is said to be stationary if there is:

1. Constant mean. $E(X_t) = \mu$.
2. Constant variance. $Var(X_t) = \sigma^2$.
3. Constant Autocorrelation. $Cor(X_t, X_{t-h}) = \rho_h$.

compute mean/ variance under stationarity

## Multiple Time Series

Often we will want to develop insight into the relationship between several variables. To illustrate, we will use quarterly data on GDP per capita and the Case Shiller Home Price Index (both from the FRED database). 

```{r echo=TRUE}
fred = read.csv("https://raw.githubusercontent.com/dbreynol/DS809/main/data/fred_dat.csv")[,-1]
fred$date = ymd(fred$date)
```


```{r}
fred %>% 
  mutate(gdp_s = (gdp-mean(gdp))/sd(gdp), shiller_s = (shiller - mean(shiller))/sd(shiller))  %>%
  pivot_longer(-date) %>% 
  filter(name %in% c("gdp_s", "shiller_s")) %>% 
  ggplot(aes(x = date, y = value)) + 
  geom_line(aes(color = name)) + theme_minimal() + 
  ggtitle("Standardized Case Shiller Index versus GPD per Capita")

```

It looks like these two time series track pretty closely to one another. We could fit a linear regression to this data in order to estimate the expected change in the Case Shiller Index for a unit ($1) change in GDP/ capital

```{r eval=T}
m0 = lm(shiller ~ gdp, data = fred)
m0 %>% tidy() %>% kable()

#gdp = 65000
#predict(m0, newdata = data.frame(gdp), interval = "prediction")


#se = 16.22 * sqrt(1 + 1/55 + (gdp - mean(fred$gdp)) / sum( (fred$gdp - mean(fred$gdp))^2 ))
#coef(m0) %*% c(1, gdp) - 2 * se

```

Further, we could also examine the residuals to gain insight into what is missing from this model.

```{r}
fred$res = residuals(m0)
ggplot(fred, aes(x = date, y = res)) + geom_line() + theme_minimal() + 
  ggtitle("Residuals from Shiller ~ GDP/Capita linear regression")
```

The model severely underestimates the house index starting during the pandemic. There is a clear pattern to these residuals. Is this a problem?



## Lab 1
cross correlation/ decomposition/ 


2. We can further hone in on the underlying trend of the residuals at the end of [Time Series EDA] by computing and plotting a moving average. For a time series $y_t$, $t = 1, \ldots, T$, a moving average of order $m$ can be written,
\begin{align}
\hat{y_t} = \frac{1}{m} \sum_{j=-k}^{k} y_{t+j},
\end{align}
where $m=2k+1$. The concept behind this technique is that observations that are close in time are likely to be close in value. Compute a moving average of order $m=7$ for the residual time series and plot it along with the residuals in a single plot.

```{r echo=TRUE}
n = length(mod_df$resids)
ma_resids = array(NA, dim = n) # initialize residuals vector
for(i in 4:(n-3)) { 
  ma_resids[i] = mean(mod_df$resids[(i-3):(i+3)])
}

data.frame(date = mod_df$datestamp, resids = mod_df$resids, ma = ma_resids) %>% 
  drop_na() %>% 
  pivot_longer(-date) %>% 
  ggplot(aes(date, value)) + geom_line(aes(color = name)) + 
  theme_minimal() + 
  ggtitle("Plot of Residuals and their order 7 moving average")
```



3. Join the Kayak visits data (see below) to the conversions data from [Time Series EDA]. Use the <code>inner_join</code> function with the argument, <code>by = c("datestamp", "country_code", "marketing_channel")</code> and then filter to only US observations. Make a plot of <code>user_visits</code> and <code>conversions</code> by day. Standardize them if it makes sense. Then, fit a linear regression model with <code>conversions</code> as the response variable and <code>user_visits</code> as the explanatory variable. What is the estimated line equation? Finally, make a plot of the residuals from this model.

```{r echo=TRUE}
visits = read.csv("https://raw.githubusercontent.com/dbreynol/DS809/main/data/visits.csv")
visits$datestamp = ymd(visits$datestamp)

df = inner_join(visits, conversions, by = c("datestamp", "country_code", "marketing_channel")) %>% 
  filter(country_code == "us") %>% 
  drop_na() %>% 
  group_by(datestamp) %>% 
  summarise(visits = sum(user_visits), conv = sum(conversions))

conv_lm = lm(conv ~ visits, data = df) # conversions = 2752.13 + .31 * visits

df %>% mutate(std_visits = (visits - mean(visits))/sd(visits), std_conv = (conv - mean(conv))/sd(conv)) %>% 
  select(datestamp, std_visits, std_conv) %>% 
  pivot_longer(-datestamp) %>% 
  ggplot(aes(datestamp, value)) + 
  geom_line(aes(color = name)) + 
  theme_minimal() + 
  ggtitle("Standardized Visits and Conversions")
```

4. Wrtie a function in R that takes two arguments: a time series ($y$) formatted as a vector and an integer ($k$) specifying a lag. The output for this function is the lag $k$ autocorrelation for $y$, using the formula in [Autocorrelation]. Compare the output of your function to the output from <code>(acf(y))</code>. An extra challenge is to allow $k$ to be a vector of lags, in which case your function should return a vector of autocorrelation values.

```{r echo=TRUE}

y = arima.sim(n = 1e5, model = list(ar = c(.1,.2)))

acf_fun = function(y, k) {
  return(cor( window(y, end = length(y)-k ), window(y,start = k+1)))
}

acf_fun(y,2)
(acf(y, plot = F))$acf[3]
```


5. The partial autocorrelation function, $\phi_k$, measures the correlation between a time series $y_t$ and a lagged copy $y_{t-k}$, with the linear dependence of $\{ y_{t-1}, y_{t-2}, \ldots,y_{t-k-1} \}$ removed. When $k=1$, $\hat{\phi}_k = \hat{\rho}_k$. When $k>1$,
\begin{align}
\hat{\phi}_k = \text{cor}(y_1 - \hat{y_1}|\{ y_2, \ldots, y_{k-1} \} , y_k - \hat{y_k}|\{ y_2, \ldots, y_{k-1} \} ),
\end{align}
where $\hat{y_1}|\{ y_2, \ldots, y_{k-1} \}$ is the predicted $y_1$ using the linear regression where $\{ y_2, \ldots, y_{k-1} \}$ are explanatory variables.
Compute the lag 1 and 2 partial autocorrelations for the following simulated time series. Show your code and validate your answers using the <code>pacf</code> function. The <code>window</code> function may be useful to extract subsets of the time series vector.


```{r echo=T, eval=T}
set.seed(1)
ysim = arima.sim(n=1000, list(ar=c(.5)))
(pacf(ysim, plot = F))
```

```{r echo=T, eval=T}
# lag 1 - same as the lag 1 acf
y1 = as.numeric( window(ysim,1,999))
y2 = as.numeric( window(ysim,2,1000) )
lag1 = (sum( (y1-mean(ysim)) * ( y2 - mean(ysim) )) )/ sum( (ysim - mean(ysim))^2)

# lag 2
y1 = as.numeric( window(ysim, 1, 998))
y2 = as.numeric( window(ysim, 2, 999))
y3 = as.numeric( window(ysim, 3, 1000))
y1t = residuals( lm(y1 ~ y2))
y3t = residuals( lm(y3 ~ y2))

cor(y1t, y3t) # manual lag 2 pacf

(pacf(ysim, plot = F))$acf[2] # function lag 2 pacf

```




