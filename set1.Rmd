# Exploratory Analysis of Time Series Data

## Time Series Data

* A time series is an ordered sequence of observations, where the ordering is through time.

* Time series data creates unique problems for statistical modeling and inference.
  + Traditional inference assumes that observations (data) are independent and identically distributed. Adjacent data points in time series data are not necessarily independent (uncorrelated).
  + Most time series models aim to exploit such dependence. For instance, yesterday’s demand of a product may tell us something about today’s
demand of a product.

* There are several different ways to represent time series data in R. 

* We will use the <code>tidyverse</code> family of packages extensively in this class. This package includes the <code>lubridate</code> package, which includes functions to work with date-times.

* Two of the most common ways to represent time series data are using data frames in which one of the variables is a time object (such as POSIXct or Date) or using a time series object. 

## Time Series EDA

The first thing to do in any data analysis is exploratory data analysis (EDA). Graphs enable many features of the data to be visualized, including patterns, unusual observations, changes over time, and relationships between variables. The features that are seen in plots of the data can then be incorporated into statistical models.

R has several systems for making graphs. We will primarily use ggplot2, which is among the set of tidyverse packages and is one of the most versatile systems for plotting. We will use a data set from Kayak to motivate our analysis.

```{r echo=TRUE}
conversions = read.csv("data/conversions.csv")
  #read.csv("https://raw.githubusercontent.com/dbreynol/DS809/main/data/conversions.csv")
knitr::kable(head(conversions))
```

This dataset contains information on the total number of daily conversions by country and marketing channel. Let us focus our analysis on the US and first visualize the number of conversions by day. 

```{r fig.height= 4}
conversions$datestamp = ymd(conversions$datestamp)

# overall trend

conversions %>% 
  filter(country_code == "us") %>% 
  group_by(datestamp, country_code) %>% 
  summarise(tot_conv = sum(conversions), .groups = "drop") %>% 
  ggplot(aes(x = datestamp, y = tot_conv)) +
  geom_line() + 
  theme_minimal() + 
  ggtitle(" US Conversions by Day") + 
  xlab("Date") + 
  ylab("Total Conversions") + 
  scale_x_date(NULL, date_breaks = "1 month", date_labels = "%b")


```

This plot contains a lot of useful information. To gain insight into how conversions depend on marketing channel, we can use facets. Facets are subplots that display a time series for each marketing channel.


```{r fig.height= 4}
conversions %>% 
  filter(country_code == "us") %>% 
  ggplot(aes(x = datestamp, y = conversions)) + 
  geom_line() + 
  facet_wrap(~ marketing_channel) + theme_minimal()
```

Display ads and search engine ads are the dominant marketing channels. Both have a regular pattern that is likely a function of the day of week, with a higher number of conversions during weekdays as compared with weekends. We can explore this feature by aggregating over each weekday and visualizing how the distribution of conversions changes by day.

```{r fig.height= 4}
conversions %>% 
  filter(country_code == "us") %>% 
  drop_na() %>% 
  group_by(datestamp) %>% summarise(conversions = sum(conversions)) %>% 
  mutate(day = wday(datestamp, label = T)) %>% 
  ggplot(aes(x = as.factor(day), y = conversions)) + 
  geom_boxplot() + 
  stat_summary(fun = "mean", geom = "point", color = "red") +
  theme_minimal() + 
  ggtitle("Conversion Distribution by Day") + 
  xlab("Day of Week") + 
  ylab("Conversions")

```

Clearly, there are significant changes in the mean level of conversions across the week. This is a form of seasonality. It may be useful to see what the data look like when this weekday effect is removed. To do so, we could visualize the residuals from the following linear regression model:

\begin{align}
\hat{\text{conversions}} = \hat{\beta}_0 + \sum_{j=1}^6 \bigg( \hat{\beta}_j \times \mathbb{1}(\text{weekday = j}) \bigg),
\end{align}


where $j$ indexes the day of week. The indicator function in this case acts as a logical switch where,

\begin{align}
\mathbb{1}(\text{weekday} = j) =
\begin{cases}
1 & \text{if } \text{weekday} = j \\
0 & \text{otherwise.}
\end{cases}
\end{align}


The residuals from this model consist of each observation minus the mean for that particular weekday. 

```{r fig.height= 4}
# EDA via models

mod_df = conversions %>% 
  filter(country_code == "us") %>% 
  drop_na() %>% 
  group_by(datestamp, country_code) %>% 
  summarise(tot_conv = sum(conversions), .groups = "drop") %>% 
  mutate(wday = factor(wday(datestamp, label= T), ordered = F))

mod = lm(tot_conv ~ wday, data = mod_df)

mod_df$resids = residuals(mod)

mod_df %>% 
  ggplot(aes(x = datestamp, y = resids)) + 
  geom_line() + 
  theme_minimal() + 
  ggtitle("US Conversions excluding Weekly Seasonality") + 
  ylab("Residuals")

```

This allows us to more clearly see the trend across the date range, removing the effect of the weekly pattern. 

## Noise Processes

Noise processes (white noise, random walk, AR, MA) are important in time-series work because they are the practical language for describing, diagnosing, and modeling uncertainty over time. These are the building blocks of nearly all time series models.

Time series data always mix:

- **Signal**: trends, seasonality, persistence, cycles

- **Noise**: random shocks, measurement error, unmodeled behavior

Noise offers a precise way of measuring:

- How much of today depends on yesterday (AR)

- How shocks propagate over time (MA)

- How quickly randomness dies out.



### White noise

White noise is a particularly important building block becuase when a time series is white noise, then there is no predictable structure. That is, a white noise time series will not yield to any amount of forecasting prowess.

Let $w_t$ be a random variable indexed by time, $t \in [1,T]$.

The following properties characterize white noise:

\begin{align}
\mathbb{E}(w_t) &= 0 \\
\operatorname{Var}(w_t) &= \sigma^2 \\
\operatorname{Cov}(w_t, w_s) &= 0 \text{  } \forall \text{ } t,s
\end{align}

Note that Gaussian white noise is a special case where $w_t \sim N(o, \sigma^2)$. White noise can come in different flavors. In the right hand plot below, $w_t = 2e_t - 1$, where $e_t \sim \text{Bernoulli(.5)}$. In the left plot, $w_t \sim N(0,2)$.

```{r fig.height= 4}
library(ggplot2)
library(gridExtra)

# Set seed and generate data
set.seed(2)
n = 100
wn1 = rnorm(n, sd = 2)
wn2 = ifelse((rbinom(n, 1, 1/2) == 1), 2, 0) - 1

# Create data frames for plotting
df1 = data.frame(index = 1:n, value = wn1)
df2 = data.frame(index = 1:n, value = wn2)

# Create Gaussian White Noise plot
p1 = ggplot(df1, aes(x = index, y = value)) +
  geom_point() +
  labs(x = "Index", y = expression(italic(x[t])), 
       title = "Gaussian White Noise") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Create Bernoulli White Noise plot
p2 = ggplot(df2, aes(x = index, y = value)) +
  geom_point() +
  labs(x = "Index", y = expression(italic(y[t])), 
       title = "Bernoulli White Noise") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Arrange plots side by side
grid.arrange(p1, p2, ncol = 2)
```

### Random Walk

Let us consider another simple model to describe time series data,

$y_t = y_{t-1}+w_t$,

where $w_t \sim N(0, \sigma^2)$ and all elements of the error vector are mutually independent. This is the random walk model.

The random walk is fundamental because it is the simplest model where the past fully determines the present level, yet provides no information about the future—making it the natural benchmark (or, null model) in time-series analysis. When a time series follows a random walk, one can estimate the distribution of changes from one time step to the next but not the level of the time series beyond one time step.

Let's derive some important properties of this model:

1. What is the mean, $\mathbb{E}(y_t)$?
2. What is the variance, $\operatorname{Var}(y_t)$?

```{r fig.height=4}
# Generate random walks data
set.seed(1)  # Remove this line if you want reproducible results
n_walks <- 50
n_steps <- 100

# Create a data frame to store all random walks
rw_data <- data.frame()
last_el <- numeric(n_walks)

for (j in 1:n_walks) {
  rw <- cumsum(rnorm(n_steps))
  last_el[j] <- rw[n_steps]
  
  # Add this walk to the data frame
  rw_data <- rbind(rw_data, data.frame(
    t = 1:n_steps,
    value = rw,
    walk_id = j
  ))
}

# Create the ggplot
ggplot(rw_data, aes(x = t, y = value, group = walk_id, color = walk_id)) +
  geom_line() +
  scale_color_gradientn(colors = rainbow(n_walks)) +
  coord_cartesian(ylim = c(-30, 30)) +
  labs(
    title = "Random Walks; Sigma = 1",
    x = "t",
    y = NULL
  ) +
  theme_classic() +
  theme(legend.position = "none")  # Hide legend since we have 50 walks
```

### AR Process


A slightly more general model is the autoregressive (AR) model, in which $y_t$ is a linear combination of its past $p$ values plus gaussian white noise, $w_t$. That is,

\begin{align}
y_t = \sum_{i=1}^p \phi_i y_{t-i} + w_t.
\end{align}

This model quantifies how much of today depends on yesterday (and the day before, etc.).

### Exercise

Consider the AR(1) process with a drift: 

\begin{align}
y_t &= a + \phi y_{t-1} + w_t
\end{align}

Write $y_t$ as a function of past white noise values and find $\mathbb{E}(y_t)$ and $\operatorname{Var}(y_t)$.

## Measures of Dependence

### Autocovariance

In all but the simplest models, there is dependence between adjacent values $x_s$ and $x_t$. This can be assessed using the notions of covariance and correlation. The autocovariance function is defined as:

\begin{align}
\gamma(s,t) &= \operatorname{Cov}(x_s, x_t) \\
&= \mathbb{E}((x_s - \mu_s) (x_t - \mu_t))
\end{align}

The autocovariance measures the linear dependence between two points on the same series observed at different times. Very smooth series exhibit autocovariance functions that stay large even when the $t$ and $s$ are far apart, whereas choppy series tend to have autocovariance functions that are nearly zero for large separations.

### Autocorrelation

The autocorrelation function (ACF) is defined as,

\begin{align}
\rho(s,t) &= \frac{\gamma(s,t)}{\sqrt{\gamma(s,s) \gamma(t,t)}}
\end{align}

The ACF measures the linear predictability of the series at time $t$, say $x_t$ , using only the value $x_s$. 

### Exercise

Compute the autocorrelation of:

1. The white noise process
2. The random walk
3. $x_t = a + \phi x_{t-1} + w_t$


## Stationarity 

A (weakly) stationary time series $x_t$ is a process such that,

1. The mean is constant: $\mathbb{E}(X_t) = \mu$ for all $t$.
2. The autocovariance function, $\gamma(s,t)$ depends on $s$ and $t$ only through their difference, $|s-t|$.

Therefore, we can write the autocovariance function of a stationary time series as,

\begin{align}
\gamma(h) &= \operatorname{Cov}(x_{t+h}, x_{t}) \\
&= \mathbb{E}((x_{t+h} - \mu) (x_{t} - \mu))
\end{align}

And, similarly, the autocorrelation function of a stationary time series can be written,

\begin{align}
\rho(h) &= \frac{\gamma(t+h,t)}{\sqrt{\gamma(t+h,t+h) \gamma(t,t)}} \\
&= \frac{\gamma(h)}{\gamma(0)}
\end{align}


## Estimation

Although the theoretical mean, autocovariance, and autocorrelation functions are useful for describing the properties of certain hypothesized models, we also need to estimate these quantities using sampled data.

### Mean

If a time series is stationary, the mean function is constant so that we can estimate it by the sample mean,

\begin{align}
\hat{\mu} = \frac{1}{T} \sum_{t=1}^T x_t 
\end{align}

**What is the variance of the sample mean in a stationary time series context?**

### Autocovariance

The sample autocovariance, $\hat{\gamma}$, for a time series $x$ at lag $k$ is:

\begin{align}
\hat{\gamma}_k &= \hat{\operatorname{Cov}}(x_t, x_{t-k}) \\
&=  \frac{1}{T} \sum_{t = k + 1}^{T} (x_t - \bar{x})(x_{t-k} - \bar{x})
\end{align}

### Autocorrelation

The sample autocorrelation function for lag $k$, $\hat{\rho}_k$, is simply the lag $k$ autocovariance, $\hat{\gamma_k}$, scaled by the sample variance.

\begin{align}
\hat{\rho}_k &= \frac{ \hat{\gamma}_k }{\hat{\sigma}_{y_t} \hat{\sigma}_{y_{t-k}}} \\
&= \frac{ \hat{\gamma}_k }{\hat{\gamma}_0}.
\end{align}


## Lab 1

1. Join the Kayak visits data (see below) to the conversions data from [Time Series EDA]. Use the <code>inner_join</code> function with the argument, <code>by = c("datestamp", "country_code", "marketing_channel")</code> and then filter to only US observations. Make a plot of <code>user_visits</code> and <code>conversions</code> by day. Standardize them if it makes sense. Then, fit a linear regression model with <code>conversions</code> as the response variable and <code>user_visits</code> as the explanatory variable. What is the estimated line equation? Finally, make a plot of the residuals from this model. Do the residuals validate standard linear regression model assumptions?

```{r echo=TRUE, eval=FALSE}
visits = read.csv("https://raw.githubusercontent.com/dbreynol/DS809/main/data/visits.csv")
visits$datestamp = ymd(visits$datestamp)
```




```{r eval=FALSE, echo=F}

df = inner_join(visits, conversions, by = c("datestamp", "country_code", "marketing_channel")) %>% 
  filter(country_code == "us") %>% 
  drop_na() %>% 
  group_by(datestamp) %>% 
  summarise(visits = sum(user_visits), conv = sum(conversions))

conv_lm = lm(conv ~ visits, data = df) # conversions = 2752.13 + .31 * visits

df %>% mutate(std_visits = (visits - mean(visits))/sd(visits), std_conv = (conv - mean(conv))/sd(conv)) %>% 
  select(datestamp, std_visits, std_conv) %>% 
  pivot_longer(-datestamp) %>% 
  ggplot(aes(datestamp, value)) + 
  geom_line(aes(color = name)) + 
  theme_minimal() + 
  ggtitle("Standardized Visits and Conversions")
```


2. Smoothing a time series offers a way of visualizing the underlying trend of a process, which can often be obscured by seasonal as well as random fluctuations. One relatively simple method of smoothing a time series is by computing a moving average. For a time series $y_t$, $t = 1, \ldots, T$, a moving average of order $m$ can be written,
\begin{align}
\hat{y_t} = \frac{1}{m} \sum_{j=-k}^{k} y_{t+j},
\end{align}
where $m=2k+1$. The concept behind this technique is that observations that are close in time are likely to be close in value. Compute a moving average of order $m=13$ for the <code>fma::elec</code> dataframe used in [Classical Regression]. Plot the smoothed series along with the raw data in a single plot. 

```{r echo=F, eval = F}
k = 6
elec_T = stats::filter(fma::elec, filter = rep(1/13, 13), sides = 2)
```


3. Time series data can exhibit a variety of patterns, and it is often helpful to decompose a time series into components, each representing an underlying pattern category. In this question, you will decompose a time series into three parts: a trend component ($T$), a seasonality component ($S$), and a random component ($R$). That is, for each observation $Y_t$, we want to break it down into three parts:

$$Y_T = T_t + S_t + R_t$$
You will again use the <code>fma::elec</code> dataframe and proceed in 3 steps:

* *Step 1*: Compute the trend component using an order 13 moving average (good news: you did exactly this in the last question, so use the output from the prior question as the trend component, $\hat{T}$).

* *Step 2*: Compute the detrended series: $Y-\hat{T}$


* *Step 3*: To estimate the seasonal component for each month, simply average the detrended values for each month. For example, the seasonal component for March is the average of all the detrended March values in the data. Then adjust the seasonal component values to ensure that they add to zero. The seasonal component is obtained by stringing together these monthly values, and then replicating the sequence for each year of data. This gives $\hat{S}$.

* *Step 4*: The remainder component is calculated by subtracting the estimated seasonal and trend-cycle components. That is $\hat{R} = Y - \hat{T} - \hat{S}$.

Provide your code that is clearly commented for each step as well as a plot that shows each of the three component series in three separate plots (hint: use <code>par(mfrow = c(1,3))</code> to make three plots in a row).

```{r echo=F, eval = F}
detrend = fma::elec - elec_T
monthly_means = tapply(detrend, cycle(detrend), function(x) mean(x, na.rm = T))
elec_S = ts ( rep(monthly_means, length.out = length(fma::elec)), start = c(1956,1), frequency = 12)
elec_R = ts ( detrend - elec_S, start = c(1956,1), frequency = 12)

par(mfrow = c(1,3))
plot(elec_T, main = "Trend")
plot(elec_S, ylim = c(-1000,1000), main = "Seasonality")
plot(elec_R, ylim = c(-1000,1000), main = "Random")
# tseries::na.remove(ts) to look at acf plots for time series with na values

```

4. See chapter 1 of TSA (page 30) for a definition of cross correlation. This is a measure of the linear dependence of one time series on another. In other words, this is a multivariate version of the autocorrelation. Compute the cross correlation using your own defined function between GDP per capita and the Case Shiller Home Price Index (both from the FRED database), using the following data: 

```{r echo=TRUE}
fred = read.csv("https://raw.githubusercontent.com/dbreynol/DS809/main/data/fred_dat.csv")[,-1]
fred$date = ymd(fred$date)
```

Provide your code that computes the ccf for lag 0 and 1:10 in both _directions_ (i.e., one where GDP/capita leads and the other in which the Case Shiller leads). Validate your calculations using the <code>ccf</code> function.

From the text, we aim to compute:
\begin{align}
\hat{\gamma}_{xy}(h) = n^{-1} \sum_{t=1}^{n-h} (x_{t+h}-\bar{x})(y_t - \bar{y})
\end{align}

```{r echo=F, eval = F}
x = fred$gdp
y = fred$shiller

xbar = mean(x)
ybar = mean(y)
n = length(x)
den = sqrt( sum( (x - xbar) ^ 2 ) * sum( (y - ybar) ^2 ))

h = 2
# compute ccf for a given value of h

my_ccf = sapply( -10:10 , function(hi) {
  if(hi < 0) {
    x = lag(x, abs(hi))
  } else if(hi > 0) { 
    y = lag(y, hi) }
  sum( (x - xbar) * (y - ybar) , na.rm = T ) / den
  })
  
names(my_ccf) = -10:10
print(round(my_ccf, 3))
(ccf(x,y, type = "correlation", lag.max = 10))
```


5. Write a function in R that takes two arguments: a time series ($y$) vector and a vector ($k$) specifying a set of lags. The output for this function is the autocorrelation of $y$ for each lag in $k$, using the formula in [Autocorrelation]. Compare the output of your function to the output from <code>(acf(y))</code>.

```{r echo=F, eval=F}

# simulated data to test function
y = arima.sim(n = 10, model = list(ar = c(.1,.2)))

acf_fun = function(y, k) {
  
  mean_y = mean(y)
  gamma_0 = sum((y - mean_y) ^ 2)
  
  # compute autocovariance for each lag in k
  sapply(k, function(ki) {
    v1 = as.numeric(window(y, end = length(y) - ki))  # ends 'early'
    v2 = as.numeric(window(y, start = ki + 1))       # starts 'late'
    
    gamma_k = sum((v1 - mean_y) * (v2 - mean_y))
    
    return(gamma_k / gamma_0)
  })
}

print ( acf_fun(y,c(1:5)) )
print ( (acf(y, plot = F))$acf[2:6] )
```


6. Simulate data from the following model: $x_t = a + \phi x_{t-1} + w_t$, in which $w_t \sim N(0,\sigma^2)$. You can choose the value for parameters: $\phi, \sigma^2, a$. Report which values you chose and make a plot of the theoretical versus observed autocorrelation function for a simulation with length $T=100$ and a simulation with length $T=10,000$.





